{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_pamap2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM models for Human Activity Recognition**\n",
        "\n",
        "Experiements on [Pamap2](https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring) dataset using different combinations of \n",
        "*with/without* x *temporal and/or spatial attention* x *1 or 2 LSTM layer(s)*\n",
        "\n"
      ],
      "metadata": {
        "id": "-hUwKQBBY40r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oSzZmQw9QLlv",
        "outputId": "8efa9816-aae4-4480-a3fa-fc2f0b6be171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py \n",
        "!pip install -q tensorflow-addons\n",
        "!pip install keras\n",
        "!pip install pyts\n"
      ],
      "metadata": {
        "id": "UAFxJj4VR7Vf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eabdb3c7-1696-478d-8be3-2bfa28967eb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyts\n",
            "  Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from pyts) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyts) (3.1.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/Debarshi-Bhattacharya/Ensem_HAR/blob/9d7769f34258185c56feb7c34f6059e07469030f/Implementation_on_PAMAP2/datapreprocessing.ipynb\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import h5py\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "'''\n",
        "0: 'transient', 1: 'lying', 2: 'sitting', 3: 'standing', 4: 'walking', 5: 'running', 6: 'cycling', 7: 'Nordic_walking', 9: 'watching_TV', \n",
        "10: 'computer_work', 11: 'car driving', 12: 'ascending_stairs', 13: 'descending_stairs', 16: 'vacuum_cleaning', 17: 'ironing', \n",
        "18: 'folding_laundry', 19: 'house_cleaning', 20: 'playing_soccer', 24: 'rope_jumping'\n",
        "'''\n",
        "\n",
        "def read_files():\n",
        "    list_of_files = ['Protocol/subject101.dat',\n",
        "                     'Protocol/subject102.dat',\n",
        "                     'Protocol/subject103.dat',\n",
        "                     'Protocol/subject104.dat',\n",
        "                     'Protocol/subject105.dat',\n",
        "                     'Protocol/subject106.dat',\n",
        "                     'Protocol/subject107.dat',\n",
        "                     'Protocol/subject108.dat',\n",
        "                     'Protocol/subject109.dat']\n",
        "    \n",
        "    subjectID = [1,2,3,4,5,6,7,8,9]\n",
        "    \n",
        "    # there are 54 columns in the data files\n",
        "    colNames = [\"timestamp\", \"activityID\",\"heartrate\"] # 1, 2, 3\n",
        "    IMUhand = ['handTemperature', \n",
        "               'handAcc16_1', 'handAcc16_2', 'handAcc16_3', \n",
        "               'handAcc6_1', 'handAcc6_2', 'handAcc6_3', \n",
        "               'handGyro1', 'handGyro2', 'handGyro3', \n",
        "               'handMagne1', 'handMagne2', 'handMagne3',\n",
        "               'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4'] # 4-20\n",
        "    IMUchest = ['chestTemperature', \n",
        "               'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', \n",
        "               'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', \n",
        "               'chestGyro1', 'chestGyro2', 'chestGyro3', \n",
        "               'chestMagne1', 'chestMagne2', 'chestMagne3',\n",
        "               'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4'] # 21-37\n",
        "    IMUankle = ['ankleTemperature', \n",
        "               'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', \n",
        "               'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', \n",
        "               'ankleGyro1', 'ankleGyro2', 'ankleGyro3', \n",
        "               'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n",
        "               'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'] # 38-54\n",
        "    \n",
        "    columns = colNames + IMUhand + IMUchest + IMUankle\n",
        "    \n",
        "    dataCollection = pd.DataFrame()\n",
        "\n",
        "    for file in list_of_files:\n",
        "        print(file)\n",
        "        procData = pd.read_table(file, header=None, sep='\\s+')\n",
        "        procData.columns = columns\n",
        "        procData['subject_id'] = int(file[-5])\n",
        "        dataCollection = dataCollection.append(procData, ignore_index=True) \n",
        "        \n",
        "    dataCollection.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    return dataCollection\n",
        "\n",
        "data = read_files()\n",
        "data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "5TfLqWMp7WWi",
        "outputId": "33ee15e7-a713-49f3-d953-95850af06b29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Protocol/subject101.dat\n",
            "Protocol/subject102.dat\n",
            "Protocol/subject103.dat\n",
            "Protocol/subject104.dat\n",
            "Protocol/subject105.dat\n",
            "Protocol/subject106.dat\n",
            "Protocol/subject107.dat\n",
            "Protocol/subject108.dat\n",
            "Protocol/subject109.dat\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   timestamp  activityID  heartrate  handTemperature  handAcc16_1  \\\n",
              "0       8.38           0      104.0             30.0      2.37223   \n",
              "1       8.39           0        NaN             30.0      2.18837   \n",
              "2       8.40           0        NaN             30.0      2.37357   \n",
              "3       8.41           0        NaN             30.0      2.07473   \n",
              "4       8.42           0        NaN             30.0      2.22936   \n",
              "\n",
              "   handAcc16_2  handAcc16_3  handAcc6_1  handAcc6_2  handAcc6_3  ...  \\\n",
              "0      8.60074      3.51048     2.43954     8.76165     3.35465  ...   \n",
              "1      8.56560      3.66179     2.39494     8.55081     3.64207  ...   \n",
              "2      8.60107      3.54898     2.30514     8.53644     3.73280  ...   \n",
              "3      8.52853      3.66021     2.33528     8.53622     3.73277  ...   \n",
              "4      8.83122      3.70000     2.23055     8.59741     3.76295  ...   \n",
              "\n",
              "   ankleGyro2  ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  \\\n",
              "0    0.009250   -0.017580     -61.1888     -38.9599     -58.1438   \n",
              "1   -0.004638    0.000368     -59.8479     -38.8919     -58.5253   \n",
              "2    0.000148    0.022495     -60.7361     -39.4138     -58.3999   \n",
              "3   -0.020301    0.011275     -60.4091     -38.7635     -58.3956   \n",
              "4   -0.014303   -0.002823     -61.5199     -39.3879     -58.2694   \n",
              "\n",
              "   ankleOrientation1  ankleOrientation2  ankleOrientation3  ankleOrientation4  \\\n",
              "0                1.0                0.0                0.0                0.0   \n",
              "1                1.0                0.0                0.0                0.0   \n",
              "2                1.0                0.0                0.0                0.0   \n",
              "3                1.0                0.0                0.0                0.0   \n",
              "4                1.0                0.0                0.0                0.0   \n",
              "\n",
              "   subject_id  \n",
              "0           1  \n",
              "1           1  \n",
              "2           1  \n",
              "3           1  \n",
              "4           1  \n",
              "\n",
              "[5 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09192e17-e1bb-4c45-b60c-89cfff09dd8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>activityID</th>\n",
              "      <th>heartrate</th>\n",
              "      <th>handTemperature</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>ankleOrientation1</th>\n",
              "      <th>ankleOrientation2</th>\n",
              "      <th>ankleOrientation3</th>\n",
              "      <th>ankleOrientation4</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.38</td>\n",
              "      <td>0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.37223</td>\n",
              "      <td>8.60074</td>\n",
              "      <td>3.51048</td>\n",
              "      <td>2.43954</td>\n",
              "      <td>8.76165</td>\n",
              "      <td>3.35465</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>-0.017580</td>\n",
              "      <td>-61.1888</td>\n",
              "      <td>-38.9599</td>\n",
              "      <td>-58.1438</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.39</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.18837</td>\n",
              "      <td>8.56560</td>\n",
              "      <td>3.66179</td>\n",
              "      <td>2.39494</td>\n",
              "      <td>8.55081</td>\n",
              "      <td>3.64207</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004638</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>-59.8479</td>\n",
              "      <td>-38.8919</td>\n",
              "      <td>-58.5253</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.40</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.37357</td>\n",
              "      <td>8.60107</td>\n",
              "      <td>3.54898</td>\n",
              "      <td>2.30514</td>\n",
              "      <td>8.53644</td>\n",
              "      <td>3.73280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.022495</td>\n",
              "      <td>-60.7361</td>\n",
              "      <td>-39.4138</td>\n",
              "      <td>-58.3999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.41</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.07473</td>\n",
              "      <td>8.52853</td>\n",
              "      <td>3.66021</td>\n",
              "      <td>2.33528</td>\n",
              "      <td>8.53622</td>\n",
              "      <td>3.73277</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020301</td>\n",
              "      <td>0.011275</td>\n",
              "      <td>-60.4091</td>\n",
              "      <td>-38.7635</td>\n",
              "      <td>-58.3956</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.42</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.22936</td>\n",
              "      <td>8.83122</td>\n",
              "      <td>3.70000</td>\n",
              "      <td>2.23055</td>\n",
              "      <td>8.59741</td>\n",
              "      <td>3.76295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014303</td>\n",
              "      <td>-0.002823</td>\n",
              "      <td>-61.5199</td>\n",
              "      <td>-39.3879</td>\n",
              "      <td>-58.2694</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09192e17-e1bb-4c45-b60c-89cfff09dd8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09192e17-e1bb-4c45-b60c-89cfff09dd8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09192e17-e1bb-4c45-b60c-89cfff09dd8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dataCleaning(dataCollection):\n",
        "    dataCollection = dataCollection.drop(['timestamp', 'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4',\n",
        "                                         'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4',\n",
        "                                         'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'],\n",
        "                                         axis = 1)  # removal of orientation columns as they are not needed\n",
        "    dataCollection = dataCollection.drop(dataCollection[dataCollection.activityID == 0].index) # removal of any row of activity 0 as it is transient activity which it is not used\n",
        "    dataCollection = dataCollection.apply(pd.to_numeric, errors = 'coerce') # removal of non numeric data in cells\n",
        "    dataCollection = dataCollection.drop(['heartrate'], axis = 1)\n",
        "    dataCollection = dataCollection.dropna()\n",
        "\n",
        "    dataCollection = dataCollection.drop(['handTemperature', 'chestTemperature', 'ankleTemperature'],\n",
        "                                         axis = 1)  # removal of temperature columns as they are not needed - sumeyye\n",
        "    print(\"data cleaned!\")\n",
        "    return dataCollection\n",
        "\n",
        "cleaned_data = dataCleaning(data)\n",
        "print(cleaned_data['activityID'].value_counts())\n",
        "cleaned_data.head(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "AD-d8IZUcjo4",
        "outputId": "d8d45602-8c2f-499a-adca-ad4d3e08060e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data cleaned!\n",
            "17    237902\n",
            "4     229709\n",
            "1     192290\n",
            "3     188984\n",
            "2     184645\n",
            "7     184444\n",
            "16    174976\n",
            "6     163302\n",
            "12    117094\n",
            "13    104865\n",
            "5      95641\n",
            "24     47579\n",
            "Name: activityID, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1      2.21530      8.27915      5.58753     2.24689   \n",
              "2929           1      2.29196      7.67288      5.74467     2.27373   \n",
              "2930           1      2.29090      7.14240      5.82342     2.26966   \n",
              "2931           1      2.21800      7.14365      5.89930     2.22177   \n",
              "2932           1      2.30106      7.25857      6.09259     2.20720   \n",
              "2933           1      2.07165      7.25965      6.01218     2.19238   \n",
              "2934           1      2.41148      7.59780      5.93915     2.23988   \n",
              "2935           1      2.32815      7.63431      5.70686     2.31663   \n",
              "2936           1      2.25096      7.78598      5.62821     2.28637   \n",
              "2937           1      2.14107      7.52262      5.78141     2.31538   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928     8.55387     5.77143  -0.004750   0.037579  -0.011145  ...   \n",
              "2929     8.14592     5.78739  -0.171710   0.025479  -0.009538  ...   \n",
              "2930     7.66268     5.78846  -0.238241   0.011214   0.000831  ...   \n",
              "2931     7.25535     5.88000  -0.192912   0.019053   0.013374  ...   \n",
              "2932     7.24042     5.95555  -0.069961  -0.018328   0.004582  ...   \n",
              "2933     7.21038     6.01604   0.063895   0.007175   0.024701  ...   \n",
              "2934     7.46679     6.03053   0.190837   0.003116   0.038762  ...   \n",
              "2935     7.64745     6.01495   0.200328  -0.009266   0.068567  ...   \n",
              "2936     7.70801     5.93935   0.204098  -0.068256   0.050000  ...   \n",
              "2937     7.72276     5.78828   0.171291  -0.055411   0.021576  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "2933      9.60177     -1.75239     0.311276    0.003793   -0.026906   \n",
              "2934      9.67694     -1.76748     0.326060    0.036814   -0.032277   \n",
              "2935      9.61685     -1.76749     0.326380   -0.010352   -0.016621   \n",
              "2936      9.61686     -1.72212     0.326234    0.039346    0.020393   \n",
              "2937      9.63189     -1.70699     0.326105    0.029874   -0.010763   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "2933    0.004125     -61.3257     -36.9744     -57.7501           1  \n",
              "2934   -0.006866     -61.5520     -36.9632     -57.9957           1  \n",
              "2935    0.006548     -61.5738     -36.1724     -59.3487           1  \n",
              "2936   -0.011880     -61.7741     -37.1744     -58.1199           1  \n",
              "2937    0.005133     -60.7680     -37.4206     -58.8735           1  \n",
              "\n",
              "[10 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-995d8752-721e-4d1a-85cb-104d97ec6c08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>2.07165</td>\n",
              "      <td>7.25965</td>\n",
              "      <td>6.01218</td>\n",
              "      <td>2.19238</td>\n",
              "      <td>7.21038</td>\n",
              "      <td>6.01604</td>\n",
              "      <td>0.063895</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60177</td>\n",
              "      <td>-1.75239</td>\n",
              "      <td>0.311276</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-61.3257</td>\n",
              "      <td>-36.9744</td>\n",
              "      <td>-57.7501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>2.41148</td>\n",
              "      <td>7.59780</td>\n",
              "      <td>5.93915</td>\n",
              "      <td>2.23988</td>\n",
              "      <td>7.46679</td>\n",
              "      <td>6.03053</td>\n",
              "      <td>0.190837</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.038762</td>\n",
              "      <td>...</td>\n",
              "      <td>9.67694</td>\n",
              "      <td>-1.76748</td>\n",
              "      <td>0.326060</td>\n",
              "      <td>0.036814</td>\n",
              "      <td>-0.032277</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-61.5520</td>\n",
              "      <td>-36.9632</td>\n",
              "      <td>-57.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>2.32815</td>\n",
              "      <td>7.63431</td>\n",
              "      <td>5.70686</td>\n",
              "      <td>2.31663</td>\n",
              "      <td>7.64745</td>\n",
              "      <td>6.01495</td>\n",
              "      <td>0.200328</td>\n",
              "      <td>-0.009266</td>\n",
              "      <td>0.068567</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61685</td>\n",
              "      <td>-1.76749</td>\n",
              "      <td>0.326380</td>\n",
              "      <td>-0.010352</td>\n",
              "      <td>-0.016621</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>-61.5738</td>\n",
              "      <td>-36.1724</td>\n",
              "      <td>-59.3487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>2.25096</td>\n",
              "      <td>7.78598</td>\n",
              "      <td>5.62821</td>\n",
              "      <td>2.28637</td>\n",
              "      <td>7.70801</td>\n",
              "      <td>5.93935</td>\n",
              "      <td>0.204098</td>\n",
              "      <td>-0.068256</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61686</td>\n",
              "      <td>-1.72212</td>\n",
              "      <td>0.326234</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-61.7741</td>\n",
              "      <td>-37.1744</td>\n",
              "      <td>-58.1199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>2.14107</td>\n",
              "      <td>7.52262</td>\n",
              "      <td>5.78141</td>\n",
              "      <td>2.31538</td>\n",
              "      <td>7.72276</td>\n",
              "      <td>5.78828</td>\n",
              "      <td>0.171291</td>\n",
              "      <td>-0.055411</td>\n",
              "      <td>0.021576</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63189</td>\n",
              "      <td>-1.70699</td>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.029874</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>0.005133</td>\n",
              "      <td>-60.7680</td>\n",
              "      <td>-37.4206</td>\n",
              "      <td>-58.8735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-995d8752-721e-4d1a-85cb-104d97ec6c08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-995d8752-721e-4d1a-85cb-104d97ec6c08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-995d8752-721e-4d1a-85cb-104d97ec6c08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_label(dataCollection): \n",
        "    # Convert original labels {1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24} to new labels. \n",
        "    mapping = {24:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,12:8,13:9,16:10,17:11} # old activity Id to new activity Id \n",
        "    for i in [24,12,13,16,17]:\n",
        "        dataCollection.loc[dataCollection.activityID == i, 'activityID'] = mapping[i]\n",
        "\n",
        "    return dataCollection\n",
        "data_reset = reset_label(cleaned_data)  \n",
        "data_reset.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "nBYOFqYrcu2i",
        "outputId": "830f52df-c9e5-424b-c40b-df091b143e30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1      2.21530      8.27915      5.58753     2.24689   \n",
              "2929           1      2.29196      7.67288      5.74467     2.27373   \n",
              "2930           1      2.29090      7.14240      5.82342     2.26966   \n",
              "2931           1      2.21800      7.14365      5.89930     2.22177   \n",
              "2932           1      2.30106      7.25857      6.09259     2.20720   \n",
              "2933           1      2.07165      7.25965      6.01218     2.19238   \n",
              "2934           1      2.41148      7.59780      5.93915     2.23988   \n",
              "2935           1      2.32815      7.63431      5.70686     2.31663   \n",
              "2936           1      2.25096      7.78598      5.62821     2.28637   \n",
              "2937           1      2.14107      7.52262      5.78141     2.31538   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928     8.55387     5.77143  -0.004750   0.037579  -0.011145  ...   \n",
              "2929     8.14592     5.78739  -0.171710   0.025479  -0.009538  ...   \n",
              "2930     7.66268     5.78846  -0.238241   0.011214   0.000831  ...   \n",
              "2931     7.25535     5.88000  -0.192912   0.019053   0.013374  ...   \n",
              "2932     7.24042     5.95555  -0.069961  -0.018328   0.004582  ...   \n",
              "2933     7.21038     6.01604   0.063895   0.007175   0.024701  ...   \n",
              "2934     7.46679     6.03053   0.190837   0.003116   0.038762  ...   \n",
              "2935     7.64745     6.01495   0.200328  -0.009266   0.068567  ...   \n",
              "2936     7.70801     5.93935   0.204098  -0.068256   0.050000  ...   \n",
              "2937     7.72276     5.78828   0.171291  -0.055411   0.021576  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "2933      9.60177     -1.75239     0.311276    0.003793   -0.026906   \n",
              "2934      9.67694     -1.76748     0.326060    0.036814   -0.032277   \n",
              "2935      9.61685     -1.76749     0.326380   -0.010352   -0.016621   \n",
              "2936      9.61686     -1.72212     0.326234    0.039346    0.020393   \n",
              "2937      9.63189     -1.70699     0.326105    0.029874   -0.010763   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "2933    0.004125     -61.3257     -36.9744     -57.7501           1  \n",
              "2934   -0.006866     -61.5520     -36.9632     -57.9957           1  \n",
              "2935    0.006548     -61.5738     -36.1724     -59.3487           1  \n",
              "2936   -0.011880     -61.7741     -37.1744     -58.1199           1  \n",
              "2937    0.005133     -60.7680     -37.4206     -58.8735           1  \n",
              "\n",
              "[10 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8cbf13e-b086-455f-980c-d9c03df49ea8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>2.07165</td>\n",
              "      <td>7.25965</td>\n",
              "      <td>6.01218</td>\n",
              "      <td>2.19238</td>\n",
              "      <td>7.21038</td>\n",
              "      <td>6.01604</td>\n",
              "      <td>0.063895</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60177</td>\n",
              "      <td>-1.75239</td>\n",
              "      <td>0.311276</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-61.3257</td>\n",
              "      <td>-36.9744</td>\n",
              "      <td>-57.7501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>2.41148</td>\n",
              "      <td>7.59780</td>\n",
              "      <td>5.93915</td>\n",
              "      <td>2.23988</td>\n",
              "      <td>7.46679</td>\n",
              "      <td>6.03053</td>\n",
              "      <td>0.190837</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.038762</td>\n",
              "      <td>...</td>\n",
              "      <td>9.67694</td>\n",
              "      <td>-1.76748</td>\n",
              "      <td>0.326060</td>\n",
              "      <td>0.036814</td>\n",
              "      <td>-0.032277</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-61.5520</td>\n",
              "      <td>-36.9632</td>\n",
              "      <td>-57.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>2.32815</td>\n",
              "      <td>7.63431</td>\n",
              "      <td>5.70686</td>\n",
              "      <td>2.31663</td>\n",
              "      <td>7.64745</td>\n",
              "      <td>6.01495</td>\n",
              "      <td>0.200328</td>\n",
              "      <td>-0.009266</td>\n",
              "      <td>0.068567</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61685</td>\n",
              "      <td>-1.76749</td>\n",
              "      <td>0.326380</td>\n",
              "      <td>-0.010352</td>\n",
              "      <td>-0.016621</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>-61.5738</td>\n",
              "      <td>-36.1724</td>\n",
              "      <td>-59.3487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>2.25096</td>\n",
              "      <td>7.78598</td>\n",
              "      <td>5.62821</td>\n",
              "      <td>2.28637</td>\n",
              "      <td>7.70801</td>\n",
              "      <td>5.93935</td>\n",
              "      <td>0.204098</td>\n",
              "      <td>-0.068256</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61686</td>\n",
              "      <td>-1.72212</td>\n",
              "      <td>0.326234</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-61.7741</td>\n",
              "      <td>-37.1744</td>\n",
              "      <td>-58.1199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>2.14107</td>\n",
              "      <td>7.52262</td>\n",
              "      <td>5.78141</td>\n",
              "      <td>2.31538</td>\n",
              "      <td>7.72276</td>\n",
              "      <td>5.78828</td>\n",
              "      <td>0.171291</td>\n",
              "      <td>-0.055411</td>\n",
              "      <td>0.021576</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63189</td>\n",
              "      <td>-1.70699</td>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.029874</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>0.005133</td>\n",
              "      <td>-60.7680</td>\n",
              "      <td>-37.4206</td>\n",
              "      <td>-58.8735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8cbf13e-b086-455f-980c-d9c03df49ea8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8cbf13e-b086-455f-980c-d9c03df49ea8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8cbf13e-b086-455f-980c-d9c03df49ea8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=data_reset.drop(['activityID'],axis=1)\n",
        "y=data_reset['activityID']\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NpsSGkYYdKF5",
        "outputId": "3a13c76f-2714-4e96-8076-059af5456cee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  handAcc6_2  \\\n",
              "2928      2.21530      8.27915      5.58753     2.24689     8.55387   \n",
              "2929      2.29196      7.67288      5.74467     2.27373     8.14592   \n",
              "2930      2.29090      7.14240      5.82342     2.26966     7.66268   \n",
              "2931      2.21800      7.14365      5.89930     2.22177     7.25535   \n",
              "2932      2.30106      7.25857      6.09259     2.20720     7.24042   \n",
              "\n",
              "      handAcc6_3  handGyro1  handGyro2  handGyro3  handMagne1  ...  \\\n",
              "2928     5.77143  -0.004750   0.037579  -0.011145     8.93200  ...   \n",
              "2929     5.78739  -0.171710   0.025479  -0.009538     9.58300  ...   \n",
              "2930     5.78846  -0.238241   0.011214   0.000831     9.05516  ...   \n",
              "2931     5.88000  -0.192912   0.019053   0.013374     9.92698  ...   \n",
              "2932     5.95555  -0.069961  -0.018328   0.004582     9.15626  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "\n",
              "[5 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>handMagne1</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>8.93200</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>9.58300</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>9.05516</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>9.92698</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>9.15626</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_subID=X['subject_id']\n",
        "\n",
        "def scale(df): # minmax scale\n",
        "    features=df.columns[0:X.shape[1]]\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    df[features]=scaler.fit_transform(df[features])\n",
        "    return df\n",
        "\n",
        "data_scaled =scale(X)\n",
        "data_scaled.shape\n",
        "X_scaled=pd.concat([pd.DataFrame(y,columns = ['activityID']),pd.DataFrame(data_scaled)],axis=1)\n",
        "X_scaled=pd.concat([pd.DataFrame(X_scaled),pd.DataFrame(X_subID,columns = ['subject_id'])],axis=1)\n",
        "\n",
        "X_scaled.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "E9rs2zewb8Pi",
        "outputId": "c60b7b47-d498-4dc1-f618-3c5e69fe160d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1     0.417516    -0.133999    -0.174116    0.113009   \n",
              "2929           1     0.418253    -0.138662    -0.172903    0.113480   \n",
              "2930           1     0.418242    -0.142743    -0.172296    0.113408   \n",
              "2931           1     0.417542    -0.142733    -0.171710    0.112568   \n",
              "2932           1     0.418340    -0.141849    -0.170219    0.112313   \n",
              "2933           1     0.416137    -0.141841    -0.170839    0.112053   \n",
              "2934           1     0.419401    -0.139240    -0.171403    0.112886   \n",
              "2935           1     0.418600    -0.138959    -0.173195    0.114232   \n",
              "2936           1     0.417859    -0.137792    -0.173802    0.113701   \n",
              "2937           1     0.416803    -0.139818    -0.172620    0.114210   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928    0.134484    0.093285   0.031349  -0.125912  -0.003356  ...   \n",
              "2929    0.127909    0.093543   0.025227  -0.126503  -0.003244  ...   \n",
              "2930    0.120122    0.093560   0.022788  -0.127200  -0.002519  ...   \n",
              "2931    0.113557    0.095039   0.024450  -0.126817  -0.001641  ...   \n",
              "2932    0.113316    0.096259   0.028958  -0.128644  -0.002256  ...   \n",
              "2933    0.112832    0.097235   0.033865  -0.127398  -0.000850  ...   \n",
              "2934    0.116965    0.097469   0.038519  -0.127596   0.000134  ...   \n",
              "2935    0.119876    0.097218   0.038867  -0.128201   0.002218  ...   \n",
              "2936    0.120852    0.095997   0.039005  -0.131084   0.000920  ...   \n",
              "2937    0.121090    0.093558   0.037803  -0.130456  -0.001068  ...   \n",
              "\n",
              "      ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  ankleGyro3  \\\n",
              "2928    -0.029670     0.015502    0.186908    0.141361   -0.082024   \n",
              "2929    -0.029426     0.015259    0.187797    0.143168   -0.081745   \n",
              "2930    -0.029180     0.016977    0.185013    0.139803   -0.082458   \n",
              "2931    -0.029913     0.016243    0.185156    0.141920   -0.080374   \n",
              "2932    -0.029425     0.015991    0.186831    0.140026   -0.082553   \n",
              "2933    -0.029425     0.016241    0.186951    0.141412   -0.081868   \n",
              "2934    -0.029668     0.016480    0.188585    0.141073   -0.082588   \n",
              "2935    -0.029669     0.016486    0.186252    0.142060   -0.081710   \n",
              "2936    -0.028937     0.016483    0.188710    0.144395   -0.082916   \n",
              "2937    -0.028692     0.016481    0.188241    0.142430   -0.081802   \n",
              "\n",
              "      ankleMagne1  ankleMagne2  ankleMagne3  subject_id  subject_id  \n",
              "2928    -0.154691    -0.129512    -0.644683        -1.0           1  \n",
              "2929    -0.153053    -0.124827    -0.644651        -1.0           1  \n",
              "2930    -0.148886    -0.120213    -0.646624        -1.0           1  \n",
              "2931    -0.152093    -0.131573    -0.640759        -1.0           1  \n",
              "2932    -0.147852    -0.131743    -0.640798        -1.0           1  \n",
              "2933    -0.156337    -0.130467    -0.639719        -1.0           1  \n",
              "2934    -0.158048    -0.130370    -0.641687        -1.0           1  \n",
              "2935    -0.158213    -0.123558    -0.652528        -1.0           1  \n",
              "2936    -0.159728    -0.132190    -0.642682        -1.0           1  \n",
              "2937    -0.152118    -0.134311    -0.648720        -1.0           1  \n",
              "\n",
              "[10 rows x 39 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b815e042-a857-4dcd-adc2-d7441be0438a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417516</td>\n",
              "      <td>-0.133999</td>\n",
              "      <td>-0.174116</td>\n",
              "      <td>0.113009</td>\n",
              "      <td>0.134484</td>\n",
              "      <td>0.093285</td>\n",
              "      <td>0.031349</td>\n",
              "      <td>-0.125912</td>\n",
              "      <td>-0.003356</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029670</td>\n",
              "      <td>0.015502</td>\n",
              "      <td>0.186908</td>\n",
              "      <td>0.141361</td>\n",
              "      <td>-0.082024</td>\n",
              "      <td>-0.154691</td>\n",
              "      <td>-0.129512</td>\n",
              "      <td>-0.644683</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418253</td>\n",
              "      <td>-0.138662</td>\n",
              "      <td>-0.172903</td>\n",
              "      <td>0.113480</td>\n",
              "      <td>0.127909</td>\n",
              "      <td>0.093543</td>\n",
              "      <td>0.025227</td>\n",
              "      <td>-0.126503</td>\n",
              "      <td>-0.003244</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029426</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.187797</td>\n",
              "      <td>0.143168</td>\n",
              "      <td>-0.081745</td>\n",
              "      <td>-0.153053</td>\n",
              "      <td>-0.124827</td>\n",
              "      <td>-0.644651</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418242</td>\n",
              "      <td>-0.142743</td>\n",
              "      <td>-0.172296</td>\n",
              "      <td>0.113408</td>\n",
              "      <td>0.120122</td>\n",
              "      <td>0.093560</td>\n",
              "      <td>0.022788</td>\n",
              "      <td>-0.127200</td>\n",
              "      <td>-0.002519</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029180</td>\n",
              "      <td>0.016977</td>\n",
              "      <td>0.185013</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>-0.082458</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>-0.120213</td>\n",
              "      <td>-0.646624</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417542</td>\n",
              "      <td>-0.142733</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.112568</td>\n",
              "      <td>0.113557</td>\n",
              "      <td>0.095039</td>\n",
              "      <td>0.024450</td>\n",
              "      <td>-0.126817</td>\n",
              "      <td>-0.001641</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029913</td>\n",
              "      <td>0.016243</td>\n",
              "      <td>0.185156</td>\n",
              "      <td>0.141920</td>\n",
              "      <td>-0.080374</td>\n",
              "      <td>-0.152093</td>\n",
              "      <td>-0.131573</td>\n",
              "      <td>-0.640759</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418340</td>\n",
              "      <td>-0.141849</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>0.112313</td>\n",
              "      <td>0.113316</td>\n",
              "      <td>0.096259</td>\n",
              "      <td>0.028958</td>\n",
              "      <td>-0.128644</td>\n",
              "      <td>-0.002256</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029425</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>0.186831</td>\n",
              "      <td>0.140026</td>\n",
              "      <td>-0.082553</td>\n",
              "      <td>-0.147852</td>\n",
              "      <td>-0.131743</td>\n",
              "      <td>-0.640798</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>-0.141841</td>\n",
              "      <td>-0.170839</td>\n",
              "      <td>0.112053</td>\n",
              "      <td>0.112832</td>\n",
              "      <td>0.097235</td>\n",
              "      <td>0.033865</td>\n",
              "      <td>-0.127398</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029425</td>\n",
              "      <td>0.016241</td>\n",
              "      <td>0.186951</td>\n",
              "      <td>0.141412</td>\n",
              "      <td>-0.081868</td>\n",
              "      <td>-0.156337</td>\n",
              "      <td>-0.130467</td>\n",
              "      <td>-0.639719</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>0.419401</td>\n",
              "      <td>-0.139240</td>\n",
              "      <td>-0.171403</td>\n",
              "      <td>0.112886</td>\n",
              "      <td>0.116965</td>\n",
              "      <td>0.097469</td>\n",
              "      <td>0.038519</td>\n",
              "      <td>-0.127596</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029668</td>\n",
              "      <td>0.016480</td>\n",
              "      <td>0.188585</td>\n",
              "      <td>0.141073</td>\n",
              "      <td>-0.082588</td>\n",
              "      <td>-0.158048</td>\n",
              "      <td>-0.130370</td>\n",
              "      <td>-0.641687</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>-0.138959</td>\n",
              "      <td>-0.173195</td>\n",
              "      <td>0.114232</td>\n",
              "      <td>0.119876</td>\n",
              "      <td>0.097218</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>-0.128201</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029669</td>\n",
              "      <td>0.016486</td>\n",
              "      <td>0.186252</td>\n",
              "      <td>0.142060</td>\n",
              "      <td>-0.081710</td>\n",
              "      <td>-0.158213</td>\n",
              "      <td>-0.123558</td>\n",
              "      <td>-0.652528</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417859</td>\n",
              "      <td>-0.137792</td>\n",
              "      <td>-0.173802</td>\n",
              "      <td>0.113701</td>\n",
              "      <td>0.120852</td>\n",
              "      <td>0.095997</td>\n",
              "      <td>0.039005</td>\n",
              "      <td>-0.131084</td>\n",
              "      <td>0.000920</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028937</td>\n",
              "      <td>0.016483</td>\n",
              "      <td>0.188710</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.082916</td>\n",
              "      <td>-0.159728</td>\n",
              "      <td>-0.132190</td>\n",
              "      <td>-0.642682</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416803</td>\n",
              "      <td>-0.139818</td>\n",
              "      <td>-0.172620</td>\n",
              "      <td>0.114210</td>\n",
              "      <td>0.121090</td>\n",
              "      <td>0.093558</td>\n",
              "      <td>0.037803</td>\n",
              "      <td>-0.130456</td>\n",
              "      <td>-0.001068</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028692</td>\n",
              "      <td>0.016481</td>\n",
              "      <td>0.188241</td>\n",
              "      <td>0.142430</td>\n",
              "      <td>-0.081802</td>\n",
              "      <td>-0.152118</td>\n",
              "      <td>-0.134311</td>\n",
              "      <td>-0.648720</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 39 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b815e042-a857-4dcd-adc2-d7441be0438a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b815e042-a857-4dcd-adc2-d7441be0438a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b815e042-a857-4dcd-adc2-d7441be0438a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SLIDING_WINDOW_LENGTH = 100\n",
        "\n",
        "def segment_signal(data, window_size): # data is numpy array\n",
        "    n = len(data)\n",
        "    X, y = [], []\n",
        "    start, end = 0, 0\n",
        "    while start + window_size - 1 < n:\n",
        "        end = start + window_size-1\n",
        "        # if the frame contains the same activity and from the same object\n",
        "        X.append(data[start:(end+1),1:-1])\n",
        "        y.append(data[start][0])\n",
        "        start += window_size #without overlap (for 50% overlap use window_size//2)\n",
        "    print(np.asarray(X).shape, np.asarray(y).shape)\n",
        "    return {'inputs' : np.asarray(X), 'labels': np.asarray(y,dtype=int)}\n",
        "\n",
        "data_segmented=segment_signal(X_scaled.to_numpy(),SLIDING_WINDOW_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sJOSDb87g3KU",
        "outputId": "139ffe00-08b1-49da-97b4-dcd2a24f39c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37) (19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(data,file_name): # save the data in h5 format\n",
        "    f = h5py.File(file_name,'w')\n",
        "    for key in data:\n",
        "        f.create_dataset(key,data = data[key])       \n",
        "    f.close()   \n",
        "\n",
        "file_name = 'pamap_scaled_segmented_100.h5'\n",
        "\n",
        "save_data(data_segmented, file_name)\n",
        "print(\"File is saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f3dWmwoLhWEH",
        "outputId": "91c9ca52-e2a3-4d1a-e48f-eb2ea51ce885"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File is saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "path = \"pamap_scaled_segmented_100.h5\"\n",
        "\n",
        "f = h5py.File(path, 'r')\n",
        "\n",
        "data_x = np.array(f[\"inputs\"][:]) \n",
        "data_y = np.array(f[\"labels\"][:])\n",
        "\n",
        "print(data_x.shape)\n",
        "print(data_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y7xOGP679WKn",
        "outputId": "8d9b8d2e-b82f-4bbe-effe-208030760677"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37)\n",
            "(19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/53731141/cifar10-randomize-train-and-test-set\n",
        "def shuffle_train_data(X_train, Y_train): \n",
        "    \"\"\"called after each epoch\"\"\" \n",
        "    perm = np.random.permutation(len(Y_train)) \n",
        "    Xtr_shuf = X_train[perm] \n",
        "    Ytr_shuf = Y_train[perm] \n",
        "    return Xtr_shuf, Ytr_shuf \n",
        "X_shuffled, y_shuffled = shuffle_train_data(data_x, data_y) \n",
        "print(X_shuffled.shape) \n",
        "print(y_shuffled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G7Kk09rA3Ems",
        "outputId": "7673f5de-0b17-42b5-9cdf-88246c1cd6f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37)\n",
            "(19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/53731141/cifar10-randomize-train-and-test-set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.33, random_state=1234)\n",
        "# Check shape\n",
        "print(X_train.shape) \n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "67z8auk8inFT",
        "outputId": "10d47e02-02f2-43ef-b404-db99c1b0f36e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12873, 100, 37)\n",
            "(12873,)\n",
            "(6341, 100, 37)\n",
            "(6341,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM, Flatten, Input\n",
        "from keras import optimizers, losses, metrics, initializers\n",
        "\n",
        "from collections import Counter\n",
        "NUM_CLASSES = len(Counter(y_shuffled).keys()) # Hardcoded number of classes in the gesture recognition problem\n",
        "BATCH_SIZE = 50 # Batch Size\n",
        "NUM_UNITS_LSTM = 16 # Number of unit in the long short-term recurrent layers\n",
        "NB_SENSOR_CHANNELS = data_x.shape[2]\n",
        "SLIDING_WINDOW_LENGTH = data_x.shape[1]"
      ],
      "metadata": {
        "id": "7iIHarpB2u13"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model._name=\"Experiement1_1LSTM_without_Attention\"\n",
        "model.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "0NVDN3J_S9Fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "de073bfb-146f-43a0-b122-42d6bb7484ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement1_1LSTM_without_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,668\n",
            "Trainable params: 22,668\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 11s 13ms/step - loss: 1.4186 - sparse_categorical_accuracy: 0.5538 - val_loss: 1.1230 - val_sparse_categorical_accuracy: 0.6342\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9893 - sparse_categorical_accuracy: 0.7056 - val_loss: 0.8632 - val_sparse_categorical_accuracy: 0.7417\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.8170 - sparse_categorical_accuracy: 0.7614 - val_loss: 0.7470 - val_sparse_categorical_accuracy: 0.8000\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.7059 - sparse_categorical_accuracy: 0.7953 - val_loss: 0.7048 - val_sparse_categorical_accuracy: 0.7938\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.6686 - val_sparse_categorical_accuracy: 0.8190\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5822 - sparse_categorical_accuracy: 0.8344 - val_loss: 0.6048 - val_sparse_categorical_accuracy: 0.8350\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.5284 - sparse_categorical_accuracy: 0.8459 - val_loss: 0.5608 - val_sparse_categorical_accuracy: 0.8388\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.5013 - sparse_categorical_accuracy: 0.8546 - val_loss: 0.5580 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4767 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.5591 - val_sparse_categorical_accuracy: 0.8384\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4495 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.5095 - val_sparse_categorical_accuracy: 0.8551\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4420 - sparse_categorical_accuracy: 0.8746 - val_loss: 0.5107 - val_sparse_categorical_accuracy: 0.8501\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4212 - sparse_categorical_accuracy: 0.8770 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8614\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4066 - sparse_categorical_accuracy: 0.8836 - val_loss: 0.4674 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3881 - sparse_categorical_accuracy: 0.8865 - val_loss: 0.4859 - val_sparse_categorical_accuracy: 0.8652\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8908 - val_loss: 0.4393 - val_sparse_categorical_accuracy: 0.8847\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3719 - sparse_categorical_accuracy: 0.8921 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.8691\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3542 - sparse_categorical_accuracy: 0.8957 - val_loss: 0.4574 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3392 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.4553 - val_sparse_categorical_accuracy: 0.8757\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9010 - val_loss: 0.4194 - val_sparse_categorical_accuracy: 0.8800\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3308 - sparse_categorical_accuracy: 0.9040 - val_loss: 0.4056 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.9055 - val_loss: 0.4101 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3135 - sparse_categorical_accuracy: 0.9064 - val_loss: 0.3856 - val_sparse_categorical_accuracy: 0.8928\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3088 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4173 - val_sparse_categorical_accuracy: 0.8878\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3057 - sparse_categorical_accuracy: 0.9091 - val_loss: 0.4159 - val_sparse_categorical_accuracy: 0.8870\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2966 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.3756 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2948 - sparse_categorical_accuracy: 0.9116 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2876 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3782 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.9120 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2722 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.3747 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2731 - sparse_categorical_accuracy: 0.9180 - val_loss: 0.3801 - val_sparse_categorical_accuracy: 0.8936\n",
            "199/199 [==============================] - 1s 4ms/step - loss: 0.3725 - sparse_categorical_accuracy: 0.8959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37246766686439514, 0.8959154486656189]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ManzhuYu/Code-SpatioTemporalAttention-LSTM-main/blob/main/modelbase.py\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2._name=\"Experiement2_1LSTM_with_temporal_Attention\"\n",
        "model_2.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_2.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_2.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_2.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_2.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_2.summary()\n",
        "\n",
        "model_2.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_2.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9BzwM2gLTSR5",
        "outputId": "0fa58fad-6892-46be-b3b9-763ba75ce82b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement2_1LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,950,668\n",
            "Trainable params: 1,950,668\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 4s 13ms/step - loss: 1.3131 - sparse_categorical_accuracy: 0.5967 - val_loss: 1.0788 - val_sparse_categorical_accuracy: 0.6781\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9614 - sparse_categorical_accuracy: 0.7121 - val_loss: 0.8401 - val_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.8328 - sparse_categorical_accuracy: 0.7567 - val_loss: 0.7572 - val_sparse_categorical_accuracy: 0.7922\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.7555 - sparse_categorical_accuracy: 0.7802 - val_loss: 0.6876 - val_sparse_categorical_accuracy: 0.8035\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6970 - sparse_categorical_accuracy: 0.7979 - val_loss: 0.6230 - val_sparse_categorical_accuracy: 0.8346\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6389 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.8146 - val_sparse_categorical_accuracy: 0.7794\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.8212 - val_loss: 0.6371 - val_sparse_categorical_accuracy: 0.8082\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.5718 - sparse_categorical_accuracy: 0.8322 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.8381\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5464 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.6169 - val_sparse_categorical_accuracy: 0.8315\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.5203 - sparse_categorical_accuracy: 0.8496 - val_loss: 0.5373 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4984 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.4582 - val_sparse_categorical_accuracy: 0.8668\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4821 - sparse_categorical_accuracy: 0.8592 - val_loss: 0.5083 - val_sparse_categorical_accuracy: 0.8520\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4590 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.5593 - val_sparse_categorical_accuracy: 0.8252\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4502 - sparse_categorical_accuracy: 0.8709 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4393 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.5128 - val_sparse_categorical_accuracy: 0.8524\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4235 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.4952 - val_sparse_categorical_accuracy: 0.8462\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4159 - sparse_categorical_accuracy: 0.8765 - val_loss: 0.4308 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4084 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4517 - val_sparse_categorical_accuracy: 0.8699\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3973 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.5063 - val_sparse_categorical_accuracy: 0.8606\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3856 - sparse_categorical_accuracy: 0.8838 - val_loss: 0.4429 - val_sparse_categorical_accuracy: 0.8699\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.4608 - val_sparse_categorical_accuracy: 0.8548\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3737 - sparse_categorical_accuracy: 0.8889 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.8722\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3688 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.3747 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3585 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3438 - sparse_categorical_accuracy: 0.8963 - val_loss: 0.4133 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3431 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.3944 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3409 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.3768 - val_sparse_categorical_accuracy: 0.8909\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3346 - sparse_categorical_accuracy: 0.8981 - val_loss: 0.3503 - val_sparse_categorical_accuracy: 0.8986\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3303 - sparse_categorical_accuracy: 0.9003 - val_loss: 0.3718 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3267 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.9068\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.3592 - sparse_categorical_accuracy: 0.9065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3592246472835541, 0.9064816236495972]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = Sequential()\n",
        "model_3._name=\"Experiement3_1LSTM_with_spatial_Attention\"\n",
        "model_3.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_3.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_3.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_3.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_3.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_3.summary()\n",
        "\n",
        "model_3.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_3.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z_oal8obkntv",
        "outputId": "c4289891-bb57-4439-e4a0-b71adc79d211"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement3_1LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,500\n",
            "Trainable params: 30,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 4s 11ms/step - loss: 1.2637 - sparse_categorical_accuracy: 0.6079 - val_loss: 0.9515 - val_sparse_categorical_accuracy: 0.7052\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.7749 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.6936 - val_sparse_categorical_accuracy: 0.7984\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.5914 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.5706 - val_sparse_categorical_accuracy: 0.8532\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4934 - sparse_categorical_accuracy: 0.8622 - val_loss: 0.5075 - val_sparse_categorical_accuracy: 0.8652\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4359 - sparse_categorical_accuracy: 0.8796 - val_loss: 0.4649 - val_sparse_categorical_accuracy: 0.8777\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3939 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.4400 - val_sparse_categorical_accuracy: 0.8804\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3572 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3973 - val_sparse_categorical_accuracy: 0.8901\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3356 - sparse_categorical_accuracy: 0.9058 - val_loss: 0.3848 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3126 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2918 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.3664 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2776 - sparse_categorical_accuracy: 0.9185 - val_loss: 0.3686 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2637 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.3282 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2508 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.3630 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2389 - sparse_categorical_accuracy: 0.9318 - val_loss: 0.3295 - val_sparse_categorical_accuracy: 0.9115\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2284 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.3493 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2222 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.3378 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2112 - sparse_categorical_accuracy: 0.9379 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.9157\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2044 - sparse_categorical_accuracy: 0.9411 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1968 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9165\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1905 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1867 - sparse_categorical_accuracy: 0.9466 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9254\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1764 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.3106 - val_sparse_categorical_accuracy: 0.9161\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.3147 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1667 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.3098 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1650 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1556 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.3261 - val_sparse_categorical_accuracy: 0.9184\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.2758 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1505 - sparse_categorical_accuracy: 0.9552 - val_loss: 0.2918 - val_sparse_categorical_accuracy: 0.9278\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1486 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.3208 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.9571 - val_loss: 0.2979 - val_sparse_categorical_accuracy: 0.9235\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.2828 - sparse_categorical_accuracy: 0.9308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28276267647743225, 0.9307680130004883]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = Sequential()\n",
        "model_4._name=\"Experiement4_1LSTM_with_spatial_temporal_Attention\"\n",
        "model_4.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_4.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_4.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_4.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_4.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_4.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_4.summary()\n",
        "\n",
        "model_4.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_4.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bFDquPOITjLt",
        "outputId": "748df464-c216-4dcb-8cce-0845acf26f69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement4_1LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,958,500\n",
            "Trainable params: 1,958,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 5s 14ms/step - loss: 1.3689 - sparse_categorical_accuracy: 0.5742 - val_loss: 1.0746 - val_sparse_categorical_accuracy: 0.6633\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.8954 - sparse_categorical_accuracy: 0.7327 - val_loss: 0.7793 - val_sparse_categorical_accuracy: 0.7786\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7279 - sparse_categorical_accuracy: 0.7902 - val_loss: 0.7661 - val_sparse_categorical_accuracy: 0.7988\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6199 - sparse_categorical_accuracy: 0.8188 - val_loss: 0.6260 - val_sparse_categorical_accuracy: 0.8183\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5498 - sparse_categorical_accuracy: 0.8397 - val_loss: 0.6316 - val_sparse_categorical_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.4950 - sparse_categorical_accuracy: 0.8558 - val_loss: 0.4861 - val_sparse_categorical_accuracy: 0.8672\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4500 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.4827 - val_sparse_categorical_accuracy: 0.8707\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.4076 - sparse_categorical_accuracy: 0.8815 - val_loss: 0.4632 - val_sparse_categorical_accuracy: 0.8583\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3876 - sparse_categorical_accuracy: 0.8864 - val_loss: 0.5524 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3623 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.4231 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3350 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.3884 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3243 - sparse_categorical_accuracy: 0.9027 - val_loss: 0.3367 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3134 - sparse_categorical_accuracy: 0.9086 - val_loss: 0.3785 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2888 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8897\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2809 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3414 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.9017\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2593 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3490 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2567 - sparse_categorical_accuracy: 0.9226 - val_loss: 0.3549 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2466 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.3746 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2447 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.3569 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.3670 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2281 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2219 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.3676 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2125 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.3619 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2197 - sparse_categorical_accuracy: 0.9366 - val_loss: 0.3167 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2097 - sparse_categorical_accuracy: 0.9339 - val_loss: 0.3546 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2102 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1992 - sparse_categorical_accuracy: 0.9357 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1987 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.3120 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.3318 - val_sparse_categorical_accuracy: 0.9080\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9159\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3405660092830658, 0.9159438610076904]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5a = Sequential()\n",
        "model_5a._name=\"Experiement5a_2LSTM_with_temporal_Attention\"\n",
        "model_5a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5a.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5a.summary()\n",
        "\n",
        "model_5a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e3gMzO2LTs-0",
        "outputId": "41c09263-119c-4a5d-a79c-99333aff2315"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5a_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 153,356\n",
            "Trainable params: 153,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.2954 - sparse_categorical_accuracy: 0.5812 - val_loss: 0.9202 - val_sparse_categorical_accuracy: 0.7157\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.7909 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.7619 - val_sparse_categorical_accuracy: 0.7817\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.6198 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.6218 - val_sparse_categorical_accuracy: 0.8144\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5161 - sparse_categorical_accuracy: 0.8456 - val_loss: 0.6163 - val_sparse_categorical_accuracy: 0.8124\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4650 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.4570 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4220 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8691\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.4915 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3535 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.3863 - val_sparse_categorical_accuracy: 0.8866\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3407 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.3738 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3171 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.3601 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3103 - sparse_categorical_accuracy: 0.9074 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2954 - sparse_categorical_accuracy: 0.9090 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2827 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.3416 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2771 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3411 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2686 - sparse_categorical_accuracy: 0.9192 - val_loss: 0.2949 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2588 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9216 - val_loss: 0.3142 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2413 - sparse_categorical_accuracy: 0.9222 - val_loss: 0.3322 - val_sparse_categorical_accuracy: 0.9056\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2402 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.2989 - val_sparse_categorical_accuracy: 0.9130\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2282 - sparse_categorical_accuracy: 0.9285 - val_loss: 0.2879 - val_sparse_categorical_accuracy: 0.9134\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2296 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.2854 - val_sparse_categorical_accuracy: 0.9130\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9283 - val_loss: 0.3016 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2179 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.2887 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2096 - sparse_categorical_accuracy: 0.9368 - val_loss: 0.3290 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.2670 - val_sparse_categorical_accuracy: 0.9177\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2015 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.2909 - val_sparse_categorical_accuracy: 0.9212\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2052 - sparse_categorical_accuracy: 0.9355 - val_loss: 0.2885 - val_sparse_categorical_accuracy: 0.9177\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.3191 - val_sparse_categorical_accuracy: 0.9064\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 4s 19ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9386 - val_loss: 0.3790 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 4s 18ms/step - loss: 0.1906 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.2698 - val_sparse_categorical_accuracy: 0.9200\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2684 - sparse_categorical_accuracy: 0.9308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26835426688194275, 0.9307680130004883]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5b = Sequential()\n",
        "model_5b._name=\"Experiement5b_2LSTM_with_temporal_Attention\"\n",
        "model_5b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5b.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5b.summary()\n",
        "\n",
        "model_5b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U-KradvolxlI",
        "outputId": "4b7f5056-974e-4b0b-d168-380e99d40aad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5b_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,952,780\n",
            "Trainable params: 1,952,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.4785 - sparse_categorical_accuracy: 0.5314 - val_loss: 1.0519 - val_sparse_categorical_accuracy: 0.6777\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.0453 - sparse_categorical_accuracy: 0.6800 - val_loss: 0.9701 - val_sparse_categorical_accuracy: 0.7029\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8752 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.7526\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.7646 - sparse_categorical_accuracy: 0.7710 - val_loss: 0.6411 - val_sparse_categorical_accuracy: 0.8132\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.8010 - val_loss: 0.6016 - val_sparse_categorical_accuracy: 0.8404\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6184 - sparse_categorical_accuracy: 0.8221 - val_loss: 0.5831 - val_sparse_categorical_accuracy: 0.8280\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5787 - sparse_categorical_accuracy: 0.8282 - val_loss: 0.5602 - val_sparse_categorical_accuracy: 0.8373\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5440 - sparse_categorical_accuracy: 0.8382 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5076 - sparse_categorical_accuracy: 0.8534 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4915 - sparse_categorical_accuracy: 0.8554 - val_loss: 0.5227 - val_sparse_categorical_accuracy: 0.8485\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4737 - sparse_categorical_accuracy: 0.8612 - val_loss: 0.4970 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4569 - sparse_categorical_accuracy: 0.8619 - val_loss: 0.5135 - val_sparse_categorical_accuracy: 0.8548\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4380 - sparse_categorical_accuracy: 0.8708 - val_loss: 0.5278 - val_sparse_categorical_accuracy: 0.8485\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4291 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.8773\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4169 - sparse_categorical_accuracy: 0.8751 - val_loss: 0.4683 - val_sparse_categorical_accuracy: 0.8641\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4093 - sparse_categorical_accuracy: 0.8757 - val_loss: 0.4611 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.8856 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.8835\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3830 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.4304 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3729 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.3883 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3679 - sparse_categorical_accuracy: 0.8909 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.8773\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8912 - val_loss: 0.4742 - val_sparse_categorical_accuracy: 0.8695\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3544 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3832 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3487 - sparse_categorical_accuracy: 0.8953 - val_loss: 0.3626 - val_sparse_categorical_accuracy: 0.8948\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3451 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.4241 - val_sparse_categorical_accuracy: 0.8656\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3378 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.4371 - val_sparse_categorical_accuracy: 0.8656\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3258 - sparse_categorical_accuracy: 0.9020 - val_loss: 0.3894 - val_sparse_categorical_accuracy: 0.8878\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3255 - sparse_categorical_accuracy: 0.9019 - val_loss: 0.4463 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3066 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3640 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3122 - sparse_categorical_accuracy: 0.9033 - val_loss: 0.4014 - val_sparse_categorical_accuracy: 0.8885\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3032 - sparse_categorical_accuracy: 0.9095 - val_loss: 0.3701 - val_sparse_categorical_accuracy: 0.8975\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3612 - sparse_categorical_accuracy: 0.9029\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36121025681495667, 0.9028544425964355]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5c = Sequential()\n",
        "model_5c._name=\"Experiement5c_2LSTM_with_temporal_Attention\"\n",
        "model_5c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5c.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5c.summary()\n",
        "\n",
        "model_5c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rkhgkmJLmoei",
        "outputId": "b9b6b378-4f30-45e1-9869-389f73de5fc2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5c_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 100, 1600)         60800     \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 185,612\n",
            "Trainable params: 185,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.2001 - sparse_categorical_accuracy: 0.6137 - val_loss: 0.7411 - val_sparse_categorical_accuracy: 0.7744\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6075 - sparse_categorical_accuracy: 0.8238 - val_loss: 0.5414 - val_sparse_categorical_accuracy: 0.8482\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4529 - sparse_categorical_accuracy: 0.8715 - val_loss: 0.4984 - val_sparse_categorical_accuracy: 0.8633\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3783 - sparse_categorical_accuracy: 0.8922 - val_loss: 0.3838 - val_sparse_categorical_accuracy: 0.8905\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3332 - sparse_categorical_accuracy: 0.9015 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2980 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3511 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2799 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2569 - sparse_categorical_accuracy: 0.9233 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9192\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.2659 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2288 - sparse_categorical_accuracy: 0.9320 - val_loss: 0.2611 - val_sparse_categorical_accuracy: 0.9243\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2173 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.2760 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2053 - sparse_categorical_accuracy: 0.9378 - val_loss: 0.5238 - val_sparse_categorical_accuracy: 0.8707\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9397 - val_loss: 0.2473 - val_sparse_categorical_accuracy: 0.9309\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1915 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.2352 - val_sparse_categorical_accuracy: 0.9332\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1810 - sparse_categorical_accuracy: 0.9457 - val_loss: 0.2710 - val_sparse_categorical_accuracy: 0.9134\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.9480 - val_loss: 0.2465 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.9476 - val_loss: 0.2559 - val_sparse_categorical_accuracy: 0.9278\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.2238 - val_sparse_categorical_accuracy: 0.9301\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1555 - sparse_categorical_accuracy: 0.9512 - val_loss: 0.2644 - val_sparse_categorical_accuracy: 0.9258\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1557 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.2587 - val_sparse_categorical_accuracy: 0.9274\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1437 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.2423 - val_sparse_categorical_accuracy: 0.9344\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1430 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.2132 - val_sparse_categorical_accuracy: 0.9433\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1361 - sparse_categorical_accuracy: 0.9567 - val_loss: 0.2321 - val_sparse_categorical_accuracy: 0.9402\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1308 - sparse_categorical_accuracy: 0.9589 - val_loss: 0.2850 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9591 - val_loss: 0.2416 - val_sparse_categorical_accuracy: 0.9340\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1263 - sparse_categorical_accuracy: 0.9591 - val_loss: 0.2679 - val_sparse_categorical_accuracy: 0.9305\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9601 - val_loss: 0.2495 - val_sparse_categorical_accuracy: 0.9313\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9594 - val_loss: 0.2270 - val_sparse_categorical_accuracy: 0.9390\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.3091 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1238 - sparse_categorical_accuracy: 0.9616 - val_loss: 0.2737 - val_sparse_categorical_accuracy: 0.9184\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2969 - sparse_categorical_accuracy: 0.9111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29685431718826294, 0.9110550284385681]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6a = Sequential()\n",
        "model_6a._name=\"Experiement6a_2LSTM_with_spatial_Attention\"\n",
        "model_6a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6a.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6a.summary()\n",
        "\n",
        "model_6a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oUrucUvKVIVv",
        "outputId": "4b418f07-be12-4216-a067-4fe09d783742"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6a_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,856\n",
            "Trainable params: 31,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 18ms/step - loss: 1.4279 - sparse_categorical_accuracy: 0.5421 - val_loss: 1.0280 - val_sparse_categorical_accuracy: 0.6668\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.9410 - sparse_categorical_accuracy: 0.7057 - val_loss: 0.8029 - val_sparse_categorical_accuracy: 0.7363\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.7538 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.6370 - val_sparse_categorical_accuracy: 0.8019\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6397 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.5499 - val_sparse_categorical_accuracy: 0.8435\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5562 - sparse_categorical_accuracy: 0.8371 - val_loss: 0.4927 - val_sparse_categorical_accuracy: 0.8559\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5008 - sparse_categorical_accuracy: 0.8515 - val_loss: 0.4932 - val_sparse_categorical_accuracy: 0.8540\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4681 - sparse_categorical_accuracy: 0.8616 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4305 - sparse_categorical_accuracy: 0.8731 - val_loss: 0.4411 - val_sparse_categorical_accuracy: 0.8683\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4128 - sparse_categorical_accuracy: 0.8777 - val_loss: 0.4443 - val_sparse_categorical_accuracy: 0.8583\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3839 - sparse_categorical_accuracy: 0.8858 - val_loss: 0.4673 - val_sparse_categorical_accuracy: 0.8683\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3691 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.3781 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3353 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.3585 - val_sparse_categorical_accuracy: 0.8909\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3261 - sparse_categorical_accuracy: 0.9014 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.9083\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3129 - sparse_categorical_accuracy: 0.9052 - val_loss: 0.3699 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3114 - sparse_categorical_accuracy: 0.9062 - val_loss: 0.3656 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8967\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2861 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.9169 - val_loss: 0.3127 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2745 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2685 - sparse_categorical_accuracy: 0.9188 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2652 - sparse_categorical_accuracy: 0.9158 - val_loss: 0.3067 - val_sparse_categorical_accuracy: 0.9083\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2548 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.3075 - val_sparse_categorical_accuracy: 0.9052\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.2947 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2488 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3027 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9247 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2440 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.3254 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2313 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.3220 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2257 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.3053 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2249 - sparse_categorical_accuracy: 0.9312 - val_loss: 0.2995 - val_sparse_categorical_accuracy: 0.9126\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3148 - sparse_categorical_accuracy: 0.9191\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3147575259208679, 0.9190979599952698]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6b = Sequential()\n",
        "model_6b._name=\"Experiement6b_2LSTM_with_spatial_Attention\"\n",
        "model_6b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6b.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6b.summary()\n",
        "\n",
        "model_6b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X8LFx9armQNL",
        "outputId": "85ad3836-0590-44b9-9f18-558a016b475e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6b_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_14 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 12)                120012    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 127,280\n",
            "Trainable params: 127,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 17ms/step - loss: 1.4094 - sparse_categorical_accuracy: 0.5563 - val_loss: 1.0765 - val_sparse_categorical_accuracy: 0.6649\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.9758 - sparse_categorical_accuracy: 0.7008 - val_loss: 0.7963 - val_sparse_categorical_accuracy: 0.7573\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.8050 - sparse_categorical_accuracy: 0.7515 - val_loss: 0.8007 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6942 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.7284 - val_sparse_categorical_accuracy: 0.7794\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6308 - sparse_categorical_accuracy: 0.8048 - val_loss: 0.6184 - val_sparse_categorical_accuracy: 0.8206\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5895 - sparse_categorical_accuracy: 0.8181 - val_loss: 0.6628 - val_sparse_categorical_accuracy: 0.8155\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5381 - sparse_categorical_accuracy: 0.8333 - val_loss: 0.4733 - val_sparse_categorical_accuracy: 0.8633\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5047 - sparse_categorical_accuracy: 0.8453 - val_loss: 0.5065 - val_sparse_categorical_accuracy: 0.8513\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4759 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.5268 - val_sparse_categorical_accuracy: 0.8536\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4490 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.4923 - val_sparse_categorical_accuracy: 0.8660\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4412 - sparse_categorical_accuracy: 0.8670 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.8637\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4174 - sparse_categorical_accuracy: 0.8724 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8664\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4125 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.4161 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3937 - sparse_categorical_accuracy: 0.8788 - val_loss: 0.4050 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.8842 - val_loss: 0.4454 - val_sparse_categorical_accuracy: 0.8734\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3672 - sparse_categorical_accuracy: 0.8860 - val_loss: 0.3577 - val_sparse_categorical_accuracy: 0.9017\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3550 - sparse_categorical_accuracy: 0.8920 - val_loss: 0.3595 - val_sparse_categorical_accuracy: 0.9045\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3473 - sparse_categorical_accuracy: 0.8919 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.8994\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3369 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.4083 - val_sparse_categorical_accuracy: 0.8800\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3337 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.3646 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3210 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.3846 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3123 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3784 - val_sparse_categorical_accuracy: 0.9014\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3071 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2985 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3774 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3061 - sparse_categorical_accuracy: 0.9053 - val_loss: 0.3237 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.9103 - val_loss: 0.4052 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2866 - sparse_categorical_accuracy: 0.9104 - val_loss: 0.3492 - val_sparse_categorical_accuracy: 0.9064\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2752 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2749 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.3350 - val_sparse_categorical_accuracy: 0.9068\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2666 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.9157\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3046 - sparse_categorical_accuracy: 0.9123\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.304587721824646, 0.9123166799545288]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6c = Sequential()\n",
        "model_6c._name=\"Experiement6c_2LSTM_with_spatial_Attention\"\n",
        "model_6c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6c.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6c.summary()\n",
        "\n",
        "model_6c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VzqGdGRHmxDg",
        "outputId": "8b3b32d6-bcd6-4db5-8e73-ca2be415e8c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6c_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,612\n",
            "Trainable params: 32,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 17ms/step - loss: 1.3470 - sparse_categorical_accuracy: 0.5654 - val_loss: 0.9629 - val_sparse_categorical_accuracy: 0.7014\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8659 - sparse_categorical_accuracy: 0.7326 - val_loss: 0.7576 - val_sparse_categorical_accuracy: 0.7740\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 4s 17ms/step - loss: 0.6831 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.6295 - val_sparse_categorical_accuracy: 0.8144\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5785 - sparse_categorical_accuracy: 0.8393 - val_loss: 0.5307 - val_sparse_categorical_accuracy: 0.8509\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5028 - sparse_categorical_accuracy: 0.8609 - val_loss: 0.4906 - val_sparse_categorical_accuracy: 0.8664\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4375 - sparse_categorical_accuracy: 0.8754 - val_loss: 0.4287 - val_sparse_categorical_accuracy: 0.8722\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3952 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.4524 - val_sparse_categorical_accuracy: 0.8645\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3723 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.3703 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3428 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3253 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.3751 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3050 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.3561 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2959 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.3767 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2843 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3302 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2740 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.9068\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.3134 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2544 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2456 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3221 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2380 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.3283 - val_sparse_categorical_accuracy: 0.9045\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2270 - sparse_categorical_accuracy: 0.9313 - val_loss: 0.2994 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2274 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.3040 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9341 - val_loss: 0.3511 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2111 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.2707 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2046 - sparse_categorical_accuracy: 0.9377 - val_loss: 0.2757 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9364 - val_loss: 0.3296 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9373 - val_loss: 0.3060 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1897 - sparse_categorical_accuracy: 0.9398 - val_loss: 0.2743 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1903 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.2777 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1857 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.2885 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1792 - sparse_categorical_accuracy: 0.9446 - val_loss: 0.2869 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.1806 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.2754 - val_sparse_categorical_accuracy: 0.9239\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3044 - sparse_categorical_accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3044148087501526, 0.924144446849823]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7a = Sequential()\n",
        "model_7a._name=\"Experiement7a_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7a.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7a.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "\n",
        "model_7a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7a.summary()\n",
        "\n",
        "model_7a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d0mKCC0ulmYf",
        "outputId": "c96ad5b8-be3d-4c88-cb60-ded7df3dec16"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7a_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_22 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,960,612\n",
            "Trainable params: 1,960,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 19ms/step - loss: 1.4059 - sparse_categorical_accuracy: 0.5514 - val_loss: 1.3026 - val_sparse_categorical_accuracy: 0.6027\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.9017 - sparse_categorical_accuracy: 0.7304 - val_loss: 0.7770 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.7303 - sparse_categorical_accuracy: 0.7850 - val_loss: 0.5881 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.6226 - sparse_categorical_accuracy: 0.8184 - val_loss: 0.5775 - val_sparse_categorical_accuracy: 0.8482\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5407 - sparse_categorical_accuracy: 0.8418 - val_loss: 0.5272 - val_sparse_categorical_accuracy: 0.8497\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4929 - sparse_categorical_accuracy: 0.8560 - val_loss: 0.4604 - val_sparse_categorical_accuracy: 0.8695\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4510 - sparse_categorical_accuracy: 0.8685 - val_loss: 0.5798 - val_sparse_categorical_accuracy: 0.8291\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4209 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.4870 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3938 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.3994 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3771 - sparse_categorical_accuracy: 0.8871 - val_loss: 0.3681 - val_sparse_categorical_accuracy: 0.9021\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3554 - sparse_categorical_accuracy: 0.8947 - val_loss: 0.4686 - val_sparse_categorical_accuracy: 0.8555\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3353 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4074 - val_sparse_categorical_accuracy: 0.8932\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3221 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.3431 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3145 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3216 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2961 - sparse_categorical_accuracy: 0.9098 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3079 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9186 - val_loss: 0.3182 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2623 - sparse_categorical_accuracy: 0.9195 - val_loss: 0.3168 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2612 - sparse_categorical_accuracy: 0.9207 - val_loss: 0.3263 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2529 - sparse_categorical_accuracy: 0.9213 - val_loss: 0.2894 - val_sparse_categorical_accuracy: 0.9270\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.3044 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2399 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.3320 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2268 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.2994 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9317 - val_loss: 0.2741 - val_sparse_categorical_accuracy: 0.9250\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2277 - sparse_categorical_accuracy: 0.9298 - val_loss: 0.3128 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2200 - sparse_categorical_accuracy: 0.9311 - val_loss: 0.3092 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2141 - sparse_categorical_accuracy: 0.9352 - val_loss: 0.3326 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2165 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.2750 - val_sparse_categorical_accuracy: 0.9328\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1974 - sparse_categorical_accuracy: 0.9367 - val_loss: 0.2873 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1999 - sparse_categorical_accuracy: 0.9367 - val_loss: 0.2782 - val_sparse_categorical_accuracy: 0.9254\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.9230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30051830410957336, 0.923040509223938]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7b = Sequential()\n",
        "model_7b._name=\"Experiement7b_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7b.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7b.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "\n",
        "model_7b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7b.summary()\n",
        "\n",
        "model_7b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A-aARh2Qm56G",
        "outputId": "d93799fb-04a0-42d2-fd2a-fd80a520d358"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7b_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_20 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,959,856\n",
            "Trainable params: 1,959,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 19ms/step - loss: 1.5433 - sparse_categorical_accuracy: 0.5096 - val_loss: 1.1725 - val_sparse_categorical_accuracy: 0.6151\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.0232 - sparse_categorical_accuracy: 0.6791 - val_loss: 0.8839 - val_sparse_categorical_accuracy: 0.7258\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8172 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7188 - val_sparse_categorical_accuracy: 0.7969\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6994 - sparse_categorical_accuracy: 0.7836 - val_loss: 0.5954 - val_sparse_categorical_accuracy: 0.8353\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6372 - sparse_categorical_accuracy: 0.8071 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.8384\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5805 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.5879 - val_sparse_categorical_accuracy: 0.8334\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5402 - sparse_categorical_accuracy: 0.8407 - val_loss: 0.4692 - val_sparse_categorical_accuracy: 0.8703\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5014 - sparse_categorical_accuracy: 0.8486 - val_loss: 0.5269 - val_sparse_categorical_accuracy: 0.8377\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4772 - sparse_categorical_accuracy: 0.8584 - val_loss: 0.4714 - val_sparse_categorical_accuracy: 0.8594\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4563 - sparse_categorical_accuracy: 0.8646 - val_loss: 0.4307 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4345 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.3920 - val_sparse_categorical_accuracy: 0.8757\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4102 - sparse_categorical_accuracy: 0.8708 - val_loss: 0.4211 - val_sparse_categorical_accuracy: 0.8850\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3963 - sparse_categorical_accuracy: 0.8802 - val_loss: 0.4054 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3807 - sparse_categorical_accuracy: 0.8822 - val_loss: 0.3866 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.3901 - val_sparse_categorical_accuracy: 0.8936\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3525 - sparse_categorical_accuracy: 0.8961 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8843\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3415 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.3906 - val_sparse_categorical_accuracy: 0.8874\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3339 - sparse_categorical_accuracy: 0.8985 - val_loss: 0.3687 - val_sparse_categorical_accuracy: 0.8948\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3281 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.3229 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.9048 - val_loss: 0.3474 - val_sparse_categorical_accuracy: 0.9014\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3155 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2975 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2977 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2899 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2960 - sparse_categorical_accuracy: 0.9093 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2838 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.3135 - val_sparse_categorical_accuracy: 0.9115\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2829 - sparse_categorical_accuracy: 0.9144 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2747 - sparse_categorical_accuracy: 0.9144 - val_loss: 0.3346 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2697 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2677 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3078 - val_sparse_categorical_accuracy: 0.9146\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2632 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.2734 - val_sparse_categorical_accuracy: 0.9219\n",
            "199/199 [==============================] - 2s 8ms/step - loss: 0.2863 - sparse_categorical_accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2862699627876282, 0.9268254041671753]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7c = Sequential()\n",
        "model_7c._name=\"Experiement7c_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7c.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7c.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_7c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7c.summary()\n",
        "\n",
        "model_7c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NdWJNBDynDOa",
        "outputId": "62092723-5711-459c-a754-50d839808b80"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7c_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_24 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " lstm_25 (LSTM)              (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161,188\n",
            "Trainable params: 161,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 8s 21ms/step - loss: 1.3187 - sparse_categorical_accuracy: 0.5608 - val_loss: 0.8294 - val_sparse_categorical_accuracy: 0.7313\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.7471 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.6162 - val_sparse_categorical_accuracy: 0.8136\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5494 - sparse_categorical_accuracy: 0.8388 - val_loss: 0.5155 - val_sparse_categorical_accuracy: 0.8458\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4440 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.8672\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3933 - sparse_categorical_accuracy: 0.8852 - val_loss: 0.4170 - val_sparse_categorical_accuracy: 0.8742\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3454 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.3712 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.9025 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.2936 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.4057 - val_sparse_categorical_accuracy: 0.8936\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2787 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2598 - sparse_categorical_accuracy: 0.9208 - val_loss: 0.3204 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2405 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2335 - sparse_categorical_accuracy: 0.9305 - val_loss: 0.3473 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2191 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.3429 - val_sparse_categorical_accuracy: 0.9095\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.3125 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1966 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.3080 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1913 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.3336 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1811 - sparse_categorical_accuracy: 0.9454 - val_loss: 0.3059 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1725 - sparse_categorical_accuracy: 0.9451 - val_loss: 0.3003 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.1700 - sparse_categorical_accuracy: 0.9464 - val_loss: 0.3107 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9495 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.2883 - val_sparse_categorical_accuracy: 0.9247\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1498 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.2871 - val_sparse_categorical_accuracy: 0.9293\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1455 - sparse_categorical_accuracy: 0.9543 - val_loss: 0.2970 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 4s 17ms/step - loss: 0.1418 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.9289\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1385 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9576 - val_loss: 0.4237 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1323 - sparse_categorical_accuracy: 0.9581 - val_loss: 0.3115 - val_sparse_categorical_accuracy: 0.9223\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1255 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9293\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.9615 - val_loss: 0.3121 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9593 - val_loss: 0.2997 - val_sparse_categorical_accuracy: 0.9274\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.9317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28986841440200806, 0.9317142367362976]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7d = Sequential()\n",
        "model_7d._name=\"Experiement7d_2LSTM_without_Attention\"\n",
        "model_7d.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7d.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7d.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7d.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7d.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7d.summary()\n",
        "\n",
        "model_7d.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7d.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7d.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PWR8jJ0_ynSW",
        "outputId": "9f58e07a-ca46-498f-efdf-c09b30739a2a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7d_2LSTM_without_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_26 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_27 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,780\n",
            "Trainable params: 24,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 6s 17ms/step - loss: 1.4992 - sparse_categorical_accuracy: 0.5020 - val_loss: 1.2027 - val_sparse_categorical_accuracy: 0.5977\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.0291 - sparse_categorical_accuracy: 0.6687 - val_loss: 0.8551 - val_sparse_categorical_accuracy: 0.7250\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.7987 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.6738 - val_sparse_categorical_accuracy: 0.8093\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.8010 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.8206\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6088 - sparse_categorical_accuracy: 0.8234 - val_loss: 0.5623 - val_sparse_categorical_accuracy: 0.8361\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5637 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.5036 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5199 - sparse_categorical_accuracy: 0.8492 - val_loss: 0.5059 - val_sparse_categorical_accuracy: 0.8563\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.5016 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.4836 - val_sparse_categorical_accuracy: 0.8579\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4469 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.4403 - val_sparse_categorical_accuracy: 0.8715\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8730 - val_loss: 0.4510 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4100 - sparse_categorical_accuracy: 0.8803 - val_loss: 0.4412 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3899 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.4041 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3822 - sparse_categorical_accuracy: 0.8870 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8870\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.4004 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3599 - sparse_categorical_accuracy: 0.8932 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8928\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3472 - sparse_categorical_accuracy: 0.8965 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3372 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.3745 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3725 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3208 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3132 - sparse_categorical_accuracy: 0.9035 - val_loss: 0.3725 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3804 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3045 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2976 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.3453 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9106 - val_loss: 0.3731 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2814 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2758 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3432 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2713 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3667 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2656 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3438 - val_sparse_categorical_accuracy: 0.9021\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3340 - sparse_categorical_accuracy: 0.9079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33398544788360596, 0.9079009890556335]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "\n",
        "-   Experiement1_**1LSTM_without**_Attention: 0.8959154486656189\n",
        "-   Experiement2_1LSTM_with_temporal_Attention: 0.9064816236495972\n",
        "-   Experiement3_1LSTM_with_spatial_Attention: 0.9307680130004883\n",
        "-   Experiement4_1LSTM_with_spatial_temporal_Attention: 0.9159438610076904\n",
        "-   Experiement5a_2LSTM_with_temporal_Attention: 0.9307680130004883\n",
        "-   Experiement5b_2LSTM_with_temporal_Attention: 0.9028544425964355\n",
        "-   Experiement5c_2LSTM_with_temporal_Attention: 0.9110550284385681\n",
        "-   Experiement6a_2LSTM_with_spatial_Attention: 0.9190979599952698\n",
        "-   Experiement6b_2LSTM_with_spatial_Attention: 0.9123166799545288\n",
        "-   Experiement6c_2LSTM_with_spatial_Attention: 0.924144446849823\n",
        "-   Experiement7a_2LSTM_with_spatial_temporal_Attention: 0.923040509223938\n",
        "-   Experiement7b_2LSTM_with_spatial_temporal_Attention: 0.9268254041671753\n",
        "-   Experiement7c_2LSTM_with_spatial_temporal_Attention: 0.9317142367362976\n",
        "-   Experiement7d_**2LSTM_without**_Attention: 0.9079009890556335\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ld1WcEvoxPA0"
      }
    }
  ]
}
