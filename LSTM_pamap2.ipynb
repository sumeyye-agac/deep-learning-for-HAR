{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_pamap2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM models for Human Activity Recognition**\n",
        "\n",
        "Experiements on [Pamap2](https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring) dataset using different combinations of \n",
        "*with/without* x *temporal and/or spatial attention* x *1 or 2 LSTM layer(s)*\n",
        "\n"
      ],
      "metadata": {
        "id": "-hUwKQBBY40r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oSzZmQw9QLlv",
        "outputId": "8efa9816-aae4-4480-a3fa-fc2f0b6be171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py \n",
        "!pip install -q tensorflow-addons\n",
        "!pip install keras\n",
        "!pip install pyts\n"
      ],
      "metadata": {
        "id": "UAFxJj4VR7Vf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eabdb3c7-1696-478d-8be3-2bfa28967eb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyts\n",
            "  Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from pyts) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyts) (3.1.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/Debarshi-Bhattacharya/Ensem_HAR/blob/9d7769f34258185c56feb7c34f6059e07469030f/Implementation_on_PAMAP2/datapreprocessing.ipynb\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import h5py\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "'''\n",
        "0: 'transient', 1: 'lying', 2: 'sitting', 3: 'standing', 4: 'walking', 5: 'running', 6: 'cycling', 7: 'Nordic_walking', 9: 'watching_TV', \n",
        "10: 'computer_work', 11: 'car driving', 12: 'ascending_stairs', 13: 'descending_stairs', 16: 'vacuum_cleaning', 17: 'ironing', \n",
        "18: 'folding_laundry', 19: 'house_cleaning', 20: 'playing_soccer', 24: 'rope_jumping'\n",
        "'''\n",
        "\n",
        "def read_files():\n",
        "    list_of_files = ['Protocol/subject101.dat',\n",
        "                     'Protocol/subject102.dat',\n",
        "                     'Protocol/subject103.dat',\n",
        "                     'Protocol/subject104.dat',\n",
        "                     'Protocol/subject105.dat',\n",
        "                     'Protocol/subject106.dat',\n",
        "                     'Protocol/subject107.dat',\n",
        "                     'Protocol/subject108.dat',\n",
        "                     'Protocol/subject109.dat']\n",
        "    \n",
        "    subjectID = [1,2,3,4,5,6,7,8,9]\n",
        "    \n",
        "    # there are 54 columns in the data files\n",
        "    colNames = [\"timestamp\", \"activityID\",\"heartrate\"] # 1, 2, 3\n",
        "    IMUhand = ['handTemperature', \n",
        "               'handAcc16_1', 'handAcc16_2', 'handAcc16_3', \n",
        "               'handAcc6_1', 'handAcc6_2', 'handAcc6_3', \n",
        "               'handGyro1', 'handGyro2', 'handGyro3', \n",
        "               'handMagne1', 'handMagne2', 'handMagne3',\n",
        "               'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4'] # 4-20\n",
        "    IMUchest = ['chestTemperature', \n",
        "               'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', \n",
        "               'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', \n",
        "               'chestGyro1', 'chestGyro2', 'chestGyro3', \n",
        "               'chestMagne1', 'chestMagne2', 'chestMagne3',\n",
        "               'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4'] # 21-37\n",
        "    IMUankle = ['ankleTemperature', \n",
        "               'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', \n",
        "               'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', \n",
        "               'ankleGyro1', 'ankleGyro2', 'ankleGyro3', \n",
        "               'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n",
        "               'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'] # 38-54\n",
        "    \n",
        "    columns = colNames + IMUhand + IMUchest + IMUankle\n",
        "    \n",
        "    dataCollection = pd.DataFrame()\n",
        "\n",
        "    for file in list_of_files:\n",
        "        print(file)\n",
        "        procData = pd.read_table(file, header=None, sep='\\s+')\n",
        "        procData.columns = columns\n",
        "        procData['subject_id'] = int(file[-5])\n",
        "        dataCollection = dataCollection.append(procData, ignore_index=True) \n",
        "        \n",
        "    dataCollection.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    return dataCollection\n",
        "\n",
        "data = read_files()\n",
        "data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "5TfLqWMp7WWi",
        "outputId": "33ee15e7-a713-49f3-d953-95850af06b29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Protocol/subject101.dat\n",
            "Protocol/subject102.dat\n",
            "Protocol/subject103.dat\n",
            "Protocol/subject104.dat\n",
            "Protocol/subject105.dat\n",
            "Protocol/subject106.dat\n",
            "Protocol/subject107.dat\n",
            "Protocol/subject108.dat\n",
            "Protocol/subject109.dat\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   timestamp  activityID  heartrate  handTemperature  handAcc16_1  \\\n",
              "0       8.38           0      104.0             30.0      2.37223   \n",
              "1       8.39           0        NaN             30.0      2.18837   \n",
              "2       8.40           0        NaN             30.0      2.37357   \n",
              "3       8.41           0        NaN             30.0      2.07473   \n",
              "4       8.42           0        NaN             30.0      2.22936   \n",
              "\n",
              "   handAcc16_2  handAcc16_3  handAcc6_1  handAcc6_2  handAcc6_3  ...  \\\n",
              "0      8.60074      3.51048     2.43954     8.76165     3.35465  ...   \n",
              "1      8.56560      3.66179     2.39494     8.55081     3.64207  ...   \n",
              "2      8.60107      3.54898     2.30514     8.53644     3.73280  ...   \n",
              "3      8.52853      3.66021     2.33528     8.53622     3.73277  ...   \n",
              "4      8.83122      3.70000     2.23055     8.59741     3.76295  ...   \n",
              "\n",
              "   ankleGyro2  ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  \\\n",
              "0    0.009250   -0.017580     -61.1888     -38.9599     -58.1438   \n",
              "1   -0.004638    0.000368     -59.8479     -38.8919     -58.5253   \n",
              "2    0.000148    0.022495     -60.7361     -39.4138     -58.3999   \n",
              "3   -0.020301    0.011275     -60.4091     -38.7635     -58.3956   \n",
              "4   -0.014303   -0.002823     -61.5199     -39.3879     -58.2694   \n",
              "\n",
              "   ankleOrientation1  ankleOrientation2  ankleOrientation3  ankleOrientation4  \\\n",
              "0                1.0                0.0                0.0                0.0   \n",
              "1                1.0                0.0                0.0                0.0   \n",
              "2                1.0                0.0                0.0                0.0   \n",
              "3                1.0                0.0                0.0                0.0   \n",
              "4                1.0                0.0                0.0                0.0   \n",
              "\n",
              "   subject_id  \n",
              "0           1  \n",
              "1           1  \n",
              "2           1  \n",
              "3           1  \n",
              "4           1  \n",
              "\n",
              "[5 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09192e17-e1bb-4c45-b60c-89cfff09dd8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>activityID</th>\n",
              "      <th>heartrate</th>\n",
              "      <th>handTemperature</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>ankleOrientation1</th>\n",
              "      <th>ankleOrientation2</th>\n",
              "      <th>ankleOrientation3</th>\n",
              "      <th>ankleOrientation4</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.38</td>\n",
              "      <td>0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.37223</td>\n",
              "      <td>8.60074</td>\n",
              "      <td>3.51048</td>\n",
              "      <td>2.43954</td>\n",
              "      <td>8.76165</td>\n",
              "      <td>3.35465</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>-0.017580</td>\n",
              "      <td>-61.1888</td>\n",
              "      <td>-38.9599</td>\n",
              "      <td>-58.1438</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.39</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.18837</td>\n",
              "      <td>8.56560</td>\n",
              "      <td>3.66179</td>\n",
              "      <td>2.39494</td>\n",
              "      <td>8.55081</td>\n",
              "      <td>3.64207</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004638</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>-59.8479</td>\n",
              "      <td>-38.8919</td>\n",
              "      <td>-58.5253</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.40</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.37357</td>\n",
              "      <td>8.60107</td>\n",
              "      <td>3.54898</td>\n",
              "      <td>2.30514</td>\n",
              "      <td>8.53644</td>\n",
              "      <td>3.73280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.022495</td>\n",
              "      <td>-60.7361</td>\n",
              "      <td>-39.4138</td>\n",
              "      <td>-58.3999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.41</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.07473</td>\n",
              "      <td>8.52853</td>\n",
              "      <td>3.66021</td>\n",
              "      <td>2.33528</td>\n",
              "      <td>8.53622</td>\n",
              "      <td>3.73277</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020301</td>\n",
              "      <td>0.011275</td>\n",
              "      <td>-60.4091</td>\n",
              "      <td>-38.7635</td>\n",
              "      <td>-58.3956</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.42</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.22936</td>\n",
              "      <td>8.83122</td>\n",
              "      <td>3.70000</td>\n",
              "      <td>2.23055</td>\n",
              "      <td>8.59741</td>\n",
              "      <td>3.76295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014303</td>\n",
              "      <td>-0.002823</td>\n",
              "      <td>-61.5199</td>\n",
              "      <td>-39.3879</td>\n",
              "      <td>-58.2694</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09192e17-e1bb-4c45-b60c-89cfff09dd8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09192e17-e1bb-4c45-b60c-89cfff09dd8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09192e17-e1bb-4c45-b60c-89cfff09dd8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dataCleaning(dataCollection):\n",
        "    dataCollection = dataCollection.drop(['timestamp', 'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4',\n",
        "                                         'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4',\n",
        "                                         'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'],\n",
        "                                         axis = 1)  # removal of orientation columns as they are not needed\n",
        "    dataCollection = dataCollection.drop(dataCollection[dataCollection.activityID == 0].index) # removal of any row of activity 0 as it is transient activity which it is not used\n",
        "    dataCollection = dataCollection.apply(pd.to_numeric, errors = 'coerce') # removal of non numeric data in cells\n",
        "    dataCollection = dataCollection.drop(['heartrate'], axis = 1)\n",
        "    dataCollection = dataCollection.dropna()\n",
        "\n",
        "    dataCollection = dataCollection.drop(['handTemperature', 'chestTemperature', 'ankleTemperature'],\n",
        "                                         axis = 1)  # removal of temperature columns as they are not needed - sumeyye\n",
        "    print(\"data cleaned!\")\n",
        "    return dataCollection\n",
        "\n",
        "cleaned_data = dataCleaning(data)\n",
        "print(cleaned_data['activityID'].value_counts())\n",
        "cleaned_data.head(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "AD-d8IZUcjo4",
        "outputId": "d8d45602-8c2f-499a-adca-ad4d3e08060e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data cleaned!\n",
            "17    237902\n",
            "4     229709\n",
            "1     192290\n",
            "3     188984\n",
            "2     184645\n",
            "7     184444\n",
            "16    174976\n",
            "6     163302\n",
            "12    117094\n",
            "13    104865\n",
            "5      95641\n",
            "24     47579\n",
            "Name: activityID, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1      2.21530      8.27915      5.58753     2.24689   \n",
              "2929           1      2.29196      7.67288      5.74467     2.27373   \n",
              "2930           1      2.29090      7.14240      5.82342     2.26966   \n",
              "2931           1      2.21800      7.14365      5.89930     2.22177   \n",
              "2932           1      2.30106      7.25857      6.09259     2.20720   \n",
              "2933           1      2.07165      7.25965      6.01218     2.19238   \n",
              "2934           1      2.41148      7.59780      5.93915     2.23988   \n",
              "2935           1      2.32815      7.63431      5.70686     2.31663   \n",
              "2936           1      2.25096      7.78598      5.62821     2.28637   \n",
              "2937           1      2.14107      7.52262      5.78141     2.31538   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928     8.55387     5.77143  -0.004750   0.037579  -0.011145  ...   \n",
              "2929     8.14592     5.78739  -0.171710   0.025479  -0.009538  ...   \n",
              "2930     7.66268     5.78846  -0.238241   0.011214   0.000831  ...   \n",
              "2931     7.25535     5.88000  -0.192912   0.019053   0.013374  ...   \n",
              "2932     7.24042     5.95555  -0.069961  -0.018328   0.004582  ...   \n",
              "2933     7.21038     6.01604   0.063895   0.007175   0.024701  ...   \n",
              "2934     7.46679     6.03053   0.190837   0.003116   0.038762  ...   \n",
              "2935     7.64745     6.01495   0.200328  -0.009266   0.068567  ...   \n",
              "2936     7.70801     5.93935   0.204098  -0.068256   0.050000  ...   \n",
              "2937     7.72276     5.78828   0.171291  -0.055411   0.021576  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "2933      9.60177     -1.75239     0.311276    0.003793   -0.026906   \n",
              "2934      9.67694     -1.76748     0.326060    0.036814   -0.032277   \n",
              "2935      9.61685     -1.76749     0.326380   -0.010352   -0.016621   \n",
              "2936      9.61686     -1.72212     0.326234    0.039346    0.020393   \n",
              "2937      9.63189     -1.70699     0.326105    0.029874   -0.010763   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "2933    0.004125     -61.3257     -36.9744     -57.7501           1  \n",
              "2934   -0.006866     -61.5520     -36.9632     -57.9957           1  \n",
              "2935    0.006548     -61.5738     -36.1724     -59.3487           1  \n",
              "2936   -0.011880     -61.7741     -37.1744     -58.1199           1  \n",
              "2937    0.005133     -60.7680     -37.4206     -58.8735           1  \n",
              "\n",
              "[10 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-995d8752-721e-4d1a-85cb-104d97ec6c08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>2.07165</td>\n",
              "      <td>7.25965</td>\n",
              "      <td>6.01218</td>\n",
              "      <td>2.19238</td>\n",
              "      <td>7.21038</td>\n",
              "      <td>6.01604</td>\n",
              "      <td>0.063895</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60177</td>\n",
              "      <td>-1.75239</td>\n",
              "      <td>0.311276</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-61.3257</td>\n",
              "      <td>-36.9744</td>\n",
              "      <td>-57.7501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>2.41148</td>\n",
              "      <td>7.59780</td>\n",
              "      <td>5.93915</td>\n",
              "      <td>2.23988</td>\n",
              "      <td>7.46679</td>\n",
              "      <td>6.03053</td>\n",
              "      <td>0.190837</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.038762</td>\n",
              "      <td>...</td>\n",
              "      <td>9.67694</td>\n",
              "      <td>-1.76748</td>\n",
              "      <td>0.326060</td>\n",
              "      <td>0.036814</td>\n",
              "      <td>-0.032277</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-61.5520</td>\n",
              "      <td>-36.9632</td>\n",
              "      <td>-57.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>2.32815</td>\n",
              "      <td>7.63431</td>\n",
              "      <td>5.70686</td>\n",
              "      <td>2.31663</td>\n",
              "      <td>7.64745</td>\n",
              "      <td>6.01495</td>\n",
              "      <td>0.200328</td>\n",
              "      <td>-0.009266</td>\n",
              "      <td>0.068567</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61685</td>\n",
              "      <td>-1.76749</td>\n",
              "      <td>0.326380</td>\n",
              "      <td>-0.010352</td>\n",
              "      <td>-0.016621</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>-61.5738</td>\n",
              "      <td>-36.1724</td>\n",
              "      <td>-59.3487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>2.25096</td>\n",
              "      <td>7.78598</td>\n",
              "      <td>5.62821</td>\n",
              "      <td>2.28637</td>\n",
              "      <td>7.70801</td>\n",
              "      <td>5.93935</td>\n",
              "      <td>0.204098</td>\n",
              "      <td>-0.068256</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61686</td>\n",
              "      <td>-1.72212</td>\n",
              "      <td>0.326234</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-61.7741</td>\n",
              "      <td>-37.1744</td>\n",
              "      <td>-58.1199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>2.14107</td>\n",
              "      <td>7.52262</td>\n",
              "      <td>5.78141</td>\n",
              "      <td>2.31538</td>\n",
              "      <td>7.72276</td>\n",
              "      <td>5.78828</td>\n",
              "      <td>0.171291</td>\n",
              "      <td>-0.055411</td>\n",
              "      <td>0.021576</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63189</td>\n",
              "      <td>-1.70699</td>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.029874</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>0.005133</td>\n",
              "      <td>-60.7680</td>\n",
              "      <td>-37.4206</td>\n",
              "      <td>-58.8735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-995d8752-721e-4d1a-85cb-104d97ec6c08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-995d8752-721e-4d1a-85cb-104d97ec6c08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-995d8752-721e-4d1a-85cb-104d97ec6c08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_label(dataCollection): \n",
        "    # Convert original labels {1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24} to new labels. \n",
        "    mapping = {24:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,12:8,13:9,16:10,17:11} # old activity Id to new activity Id \n",
        "    for i in [24,12,13,16,17]:\n",
        "        dataCollection.loc[dataCollection.activityID == i, 'activityID'] = mapping[i]\n",
        "\n",
        "    return dataCollection\n",
        "data_reset = reset_label(cleaned_data)  \n",
        "data_reset.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "nBYOFqYrcu2i",
        "outputId": "830f52df-c9e5-424b-c40b-df091b143e30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1      2.21530      8.27915      5.58753     2.24689   \n",
              "2929           1      2.29196      7.67288      5.74467     2.27373   \n",
              "2930           1      2.29090      7.14240      5.82342     2.26966   \n",
              "2931           1      2.21800      7.14365      5.89930     2.22177   \n",
              "2932           1      2.30106      7.25857      6.09259     2.20720   \n",
              "2933           1      2.07165      7.25965      6.01218     2.19238   \n",
              "2934           1      2.41148      7.59780      5.93915     2.23988   \n",
              "2935           1      2.32815      7.63431      5.70686     2.31663   \n",
              "2936           1      2.25096      7.78598      5.62821     2.28637   \n",
              "2937           1      2.14107      7.52262      5.78141     2.31538   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928     8.55387     5.77143  -0.004750   0.037579  -0.011145  ...   \n",
              "2929     8.14592     5.78739  -0.171710   0.025479  -0.009538  ...   \n",
              "2930     7.66268     5.78846  -0.238241   0.011214   0.000831  ...   \n",
              "2931     7.25535     5.88000  -0.192912   0.019053   0.013374  ...   \n",
              "2932     7.24042     5.95555  -0.069961  -0.018328   0.004582  ...   \n",
              "2933     7.21038     6.01604   0.063895   0.007175   0.024701  ...   \n",
              "2934     7.46679     6.03053   0.190837   0.003116   0.038762  ...   \n",
              "2935     7.64745     6.01495   0.200328  -0.009266   0.068567  ...   \n",
              "2936     7.70801     5.93935   0.204098  -0.068256   0.050000  ...   \n",
              "2937     7.72276     5.78828   0.171291  -0.055411   0.021576  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "2933      9.60177     -1.75239     0.311276    0.003793   -0.026906   \n",
              "2934      9.67694     -1.76748     0.326060    0.036814   -0.032277   \n",
              "2935      9.61685     -1.76749     0.326380   -0.010352   -0.016621   \n",
              "2936      9.61686     -1.72212     0.326234    0.039346    0.020393   \n",
              "2937      9.63189     -1.70699     0.326105    0.029874   -0.010763   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "2933    0.004125     -61.3257     -36.9744     -57.7501           1  \n",
              "2934   -0.006866     -61.5520     -36.9632     -57.9957           1  \n",
              "2935    0.006548     -61.5738     -36.1724     -59.3487           1  \n",
              "2936   -0.011880     -61.7741     -37.1744     -58.1199           1  \n",
              "2937    0.005133     -60.7680     -37.4206     -58.8735           1  \n",
              "\n",
              "[10 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8cbf13e-b086-455f-980c-d9c03df49ea8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>2.07165</td>\n",
              "      <td>7.25965</td>\n",
              "      <td>6.01218</td>\n",
              "      <td>2.19238</td>\n",
              "      <td>7.21038</td>\n",
              "      <td>6.01604</td>\n",
              "      <td>0.063895</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60177</td>\n",
              "      <td>-1.75239</td>\n",
              "      <td>0.311276</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-61.3257</td>\n",
              "      <td>-36.9744</td>\n",
              "      <td>-57.7501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>2.41148</td>\n",
              "      <td>7.59780</td>\n",
              "      <td>5.93915</td>\n",
              "      <td>2.23988</td>\n",
              "      <td>7.46679</td>\n",
              "      <td>6.03053</td>\n",
              "      <td>0.190837</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.038762</td>\n",
              "      <td>...</td>\n",
              "      <td>9.67694</td>\n",
              "      <td>-1.76748</td>\n",
              "      <td>0.326060</td>\n",
              "      <td>0.036814</td>\n",
              "      <td>-0.032277</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-61.5520</td>\n",
              "      <td>-36.9632</td>\n",
              "      <td>-57.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>2.32815</td>\n",
              "      <td>7.63431</td>\n",
              "      <td>5.70686</td>\n",
              "      <td>2.31663</td>\n",
              "      <td>7.64745</td>\n",
              "      <td>6.01495</td>\n",
              "      <td>0.200328</td>\n",
              "      <td>-0.009266</td>\n",
              "      <td>0.068567</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61685</td>\n",
              "      <td>-1.76749</td>\n",
              "      <td>0.326380</td>\n",
              "      <td>-0.010352</td>\n",
              "      <td>-0.016621</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>-61.5738</td>\n",
              "      <td>-36.1724</td>\n",
              "      <td>-59.3487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>2.25096</td>\n",
              "      <td>7.78598</td>\n",
              "      <td>5.62821</td>\n",
              "      <td>2.28637</td>\n",
              "      <td>7.70801</td>\n",
              "      <td>5.93935</td>\n",
              "      <td>0.204098</td>\n",
              "      <td>-0.068256</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61686</td>\n",
              "      <td>-1.72212</td>\n",
              "      <td>0.326234</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-61.7741</td>\n",
              "      <td>-37.1744</td>\n",
              "      <td>-58.1199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>2.14107</td>\n",
              "      <td>7.52262</td>\n",
              "      <td>5.78141</td>\n",
              "      <td>2.31538</td>\n",
              "      <td>7.72276</td>\n",
              "      <td>5.78828</td>\n",
              "      <td>0.171291</td>\n",
              "      <td>-0.055411</td>\n",
              "      <td>0.021576</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63189</td>\n",
              "      <td>-1.70699</td>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.029874</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>0.005133</td>\n",
              "      <td>-60.7680</td>\n",
              "      <td>-37.4206</td>\n",
              "      <td>-58.8735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8cbf13e-b086-455f-980c-d9c03df49ea8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8cbf13e-b086-455f-980c-d9c03df49ea8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8cbf13e-b086-455f-980c-d9c03df49ea8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=data_reset.drop(['activityID'],axis=1)\n",
        "y=data_reset['activityID']\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NpsSGkYYdKF5",
        "outputId": "3a13c76f-2714-4e96-8076-059af5456cee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  handAcc6_2  \\\n",
              "2928      2.21530      8.27915      5.58753     2.24689     8.55387   \n",
              "2929      2.29196      7.67288      5.74467     2.27373     8.14592   \n",
              "2930      2.29090      7.14240      5.82342     2.26966     7.66268   \n",
              "2931      2.21800      7.14365      5.89930     2.22177     7.25535   \n",
              "2932      2.30106      7.25857      6.09259     2.20720     7.24042   \n",
              "\n",
              "      handAcc6_3  handGyro1  handGyro2  handGyro3  handMagne1  ...  \\\n",
              "2928     5.77143  -0.004750   0.037579  -0.011145     8.93200  ...   \n",
              "2929     5.78739  -0.171710   0.025479  -0.009538     9.58300  ...   \n",
              "2930     5.78846  -0.238241   0.011214   0.000831     9.05516  ...   \n",
              "2931     5.88000  -0.192912   0.019053   0.013374     9.92698  ...   \n",
              "2932     5.95555  -0.069961  -0.018328   0.004582     9.15626  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "\n",
              "[5 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>handMagne1</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>8.93200</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>9.58300</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>9.05516</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>9.92698</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>9.15626</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_subID=X['subject_id']\n",
        "\n",
        "def scale(df): # minmax scale\n",
        "    features=df.columns[0:X.shape[1]]\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    df[features]=scaler.fit_transform(df[features])\n",
        "    return df\n",
        "\n",
        "data_scaled =scale(X)\n",
        "data_scaled.shape\n",
        "X_scaled=pd.concat([pd.DataFrame(y,columns = ['activityID']),pd.DataFrame(data_scaled)],axis=1)\n",
        "X_scaled=pd.concat([pd.DataFrame(X_scaled),pd.DataFrame(X_subID,columns = ['subject_id'])],axis=1)\n",
        "\n",
        "X_scaled.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "E9rs2zewb8Pi",
        "outputId": "c60b7b47-d498-4dc1-f618-3c5e69fe160d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1     0.417516    -0.133999    -0.174116    0.113009   \n",
              "2929           1     0.418253    -0.138662    -0.172903    0.113480   \n",
              "2930           1     0.418242    -0.142743    -0.172296    0.113408   \n",
              "2931           1     0.417542    -0.142733    -0.171710    0.112568   \n",
              "2932           1     0.418340    -0.141849    -0.170219    0.112313   \n",
              "2933           1     0.416137    -0.141841    -0.170839    0.112053   \n",
              "2934           1     0.419401    -0.139240    -0.171403    0.112886   \n",
              "2935           1     0.418600    -0.138959    -0.173195    0.114232   \n",
              "2936           1     0.417859    -0.137792    -0.173802    0.113701   \n",
              "2937           1     0.416803    -0.139818    -0.172620    0.114210   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928    0.134484    0.093285   0.031349  -0.125912  -0.003356  ...   \n",
              "2929    0.127909    0.093543   0.025227  -0.126503  -0.003244  ...   \n",
              "2930    0.120122    0.093560   0.022788  -0.127200  -0.002519  ...   \n",
              "2931    0.113557    0.095039   0.024450  -0.126817  -0.001641  ...   \n",
              "2932    0.113316    0.096259   0.028958  -0.128644  -0.002256  ...   \n",
              "2933    0.112832    0.097235   0.033865  -0.127398  -0.000850  ...   \n",
              "2934    0.116965    0.097469   0.038519  -0.127596   0.000134  ...   \n",
              "2935    0.119876    0.097218   0.038867  -0.128201   0.002218  ...   \n",
              "2936    0.120852    0.095997   0.039005  -0.131084   0.000920  ...   \n",
              "2937    0.121090    0.093558   0.037803  -0.130456  -0.001068  ...   \n",
              "\n",
              "      ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  ankleGyro3  \\\n",
              "2928    -0.029670     0.015502    0.186908    0.141361   -0.082024   \n",
              "2929    -0.029426     0.015259    0.187797    0.143168   -0.081745   \n",
              "2930    -0.029180     0.016977    0.185013    0.139803   -0.082458   \n",
              "2931    -0.029913     0.016243    0.185156    0.141920   -0.080374   \n",
              "2932    -0.029425     0.015991    0.186831    0.140026   -0.082553   \n",
              "2933    -0.029425     0.016241    0.186951    0.141412   -0.081868   \n",
              "2934    -0.029668     0.016480    0.188585    0.141073   -0.082588   \n",
              "2935    -0.029669     0.016486    0.186252    0.142060   -0.081710   \n",
              "2936    -0.028937     0.016483    0.188710    0.144395   -0.082916   \n",
              "2937    -0.028692     0.016481    0.188241    0.142430   -0.081802   \n",
              "\n",
              "      ankleMagne1  ankleMagne2  ankleMagne3  subject_id  subject_id  \n",
              "2928    -0.154691    -0.129512    -0.644683        -1.0           1  \n",
              "2929    -0.153053    -0.124827    -0.644651        -1.0           1  \n",
              "2930    -0.148886    -0.120213    -0.646624        -1.0           1  \n",
              "2931    -0.152093    -0.131573    -0.640759        -1.0           1  \n",
              "2932    -0.147852    -0.131743    -0.640798        -1.0           1  \n",
              "2933    -0.156337    -0.130467    -0.639719        -1.0           1  \n",
              "2934    -0.158048    -0.130370    -0.641687        -1.0           1  \n",
              "2935    -0.158213    -0.123558    -0.652528        -1.0           1  \n",
              "2936    -0.159728    -0.132190    -0.642682        -1.0           1  \n",
              "2937    -0.152118    -0.134311    -0.648720        -1.0           1  \n",
              "\n",
              "[10 rows x 39 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b815e042-a857-4dcd-adc2-d7441be0438a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417516</td>\n",
              "      <td>-0.133999</td>\n",
              "      <td>-0.174116</td>\n",
              "      <td>0.113009</td>\n",
              "      <td>0.134484</td>\n",
              "      <td>0.093285</td>\n",
              "      <td>0.031349</td>\n",
              "      <td>-0.125912</td>\n",
              "      <td>-0.003356</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029670</td>\n",
              "      <td>0.015502</td>\n",
              "      <td>0.186908</td>\n",
              "      <td>0.141361</td>\n",
              "      <td>-0.082024</td>\n",
              "      <td>-0.154691</td>\n",
              "      <td>-0.129512</td>\n",
              "      <td>-0.644683</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418253</td>\n",
              "      <td>-0.138662</td>\n",
              "      <td>-0.172903</td>\n",
              "      <td>0.113480</td>\n",
              "      <td>0.127909</td>\n",
              "      <td>0.093543</td>\n",
              "      <td>0.025227</td>\n",
              "      <td>-0.126503</td>\n",
              "      <td>-0.003244</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029426</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.187797</td>\n",
              "      <td>0.143168</td>\n",
              "      <td>-0.081745</td>\n",
              "      <td>-0.153053</td>\n",
              "      <td>-0.124827</td>\n",
              "      <td>-0.644651</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418242</td>\n",
              "      <td>-0.142743</td>\n",
              "      <td>-0.172296</td>\n",
              "      <td>0.113408</td>\n",
              "      <td>0.120122</td>\n",
              "      <td>0.093560</td>\n",
              "      <td>0.022788</td>\n",
              "      <td>-0.127200</td>\n",
              "      <td>-0.002519</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029180</td>\n",
              "      <td>0.016977</td>\n",
              "      <td>0.185013</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>-0.082458</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>-0.120213</td>\n",
              "      <td>-0.646624</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417542</td>\n",
              "      <td>-0.142733</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.112568</td>\n",
              "      <td>0.113557</td>\n",
              "      <td>0.095039</td>\n",
              "      <td>0.024450</td>\n",
              "      <td>-0.126817</td>\n",
              "      <td>-0.001641</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029913</td>\n",
              "      <td>0.016243</td>\n",
              "      <td>0.185156</td>\n",
              "      <td>0.141920</td>\n",
              "      <td>-0.080374</td>\n",
              "      <td>-0.152093</td>\n",
              "      <td>-0.131573</td>\n",
              "      <td>-0.640759</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418340</td>\n",
              "      <td>-0.141849</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>0.112313</td>\n",
              "      <td>0.113316</td>\n",
              "      <td>0.096259</td>\n",
              "      <td>0.028958</td>\n",
              "      <td>-0.128644</td>\n",
              "      <td>-0.002256</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029425</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>0.186831</td>\n",
              "      <td>0.140026</td>\n",
              "      <td>-0.082553</td>\n",
              "      <td>-0.147852</td>\n",
              "      <td>-0.131743</td>\n",
              "      <td>-0.640798</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>-0.141841</td>\n",
              "      <td>-0.170839</td>\n",
              "      <td>0.112053</td>\n",
              "      <td>0.112832</td>\n",
              "      <td>0.097235</td>\n",
              "      <td>0.033865</td>\n",
              "      <td>-0.127398</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029425</td>\n",
              "      <td>0.016241</td>\n",
              "      <td>0.186951</td>\n",
              "      <td>0.141412</td>\n",
              "      <td>-0.081868</td>\n",
              "      <td>-0.156337</td>\n",
              "      <td>-0.130467</td>\n",
              "      <td>-0.639719</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>0.419401</td>\n",
              "      <td>-0.139240</td>\n",
              "      <td>-0.171403</td>\n",
              "      <td>0.112886</td>\n",
              "      <td>0.116965</td>\n",
              "      <td>0.097469</td>\n",
              "      <td>0.038519</td>\n",
              "      <td>-0.127596</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029668</td>\n",
              "      <td>0.016480</td>\n",
              "      <td>0.188585</td>\n",
              "      <td>0.141073</td>\n",
              "      <td>-0.082588</td>\n",
              "      <td>-0.158048</td>\n",
              "      <td>-0.130370</td>\n",
              "      <td>-0.641687</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>-0.138959</td>\n",
              "      <td>-0.173195</td>\n",
              "      <td>0.114232</td>\n",
              "      <td>0.119876</td>\n",
              "      <td>0.097218</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>-0.128201</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029669</td>\n",
              "      <td>0.016486</td>\n",
              "      <td>0.186252</td>\n",
              "      <td>0.142060</td>\n",
              "      <td>-0.081710</td>\n",
              "      <td>-0.158213</td>\n",
              "      <td>-0.123558</td>\n",
              "      <td>-0.652528</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417859</td>\n",
              "      <td>-0.137792</td>\n",
              "      <td>-0.173802</td>\n",
              "      <td>0.113701</td>\n",
              "      <td>0.120852</td>\n",
              "      <td>0.095997</td>\n",
              "      <td>0.039005</td>\n",
              "      <td>-0.131084</td>\n",
              "      <td>0.000920</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028937</td>\n",
              "      <td>0.016483</td>\n",
              "      <td>0.188710</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.082916</td>\n",
              "      <td>-0.159728</td>\n",
              "      <td>-0.132190</td>\n",
              "      <td>-0.642682</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416803</td>\n",
              "      <td>-0.139818</td>\n",
              "      <td>-0.172620</td>\n",
              "      <td>0.114210</td>\n",
              "      <td>0.121090</td>\n",
              "      <td>0.093558</td>\n",
              "      <td>0.037803</td>\n",
              "      <td>-0.130456</td>\n",
              "      <td>-0.001068</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028692</td>\n",
              "      <td>0.016481</td>\n",
              "      <td>0.188241</td>\n",
              "      <td>0.142430</td>\n",
              "      <td>-0.081802</td>\n",
              "      <td>-0.152118</td>\n",
              "      <td>-0.134311</td>\n",
              "      <td>-0.648720</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 39 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b815e042-a857-4dcd-adc2-d7441be0438a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b815e042-a857-4dcd-adc2-d7441be0438a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b815e042-a857-4dcd-adc2-d7441be0438a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SLIDING_WINDOW_LENGTH = 100\n",
        "\n",
        "def segment_signal(data, window_size): # data is numpy array\n",
        "    n = len(data)\n",
        "    X, y = [], []\n",
        "    start, end = 0, 0\n",
        "    while start + window_size - 1 < n:\n",
        "        end = start + window_size-1\n",
        "        # if the frame contains the same activity and from the same object\n",
        "        X.append(data[start:(end+1),1:-1])\n",
        "        y.append(data[start][0])\n",
        "        start += window_size #without overlap (for 50% overlap use window_size//2)\n",
        "    print(np.asarray(X).shape, np.asarray(y).shape)\n",
        "    return {'inputs' : np.asarray(X), 'labels': np.asarray(y,dtype=int)}\n",
        "\n",
        "data_segmented=segment_signal(X_scaled.to_numpy(),SLIDING_WINDOW_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sJOSDb87g3KU",
        "outputId": "139ffe00-08b1-49da-97b4-dcd2a24f39c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37) (19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(data,file_name): # save the data in h5 format\n",
        "    f = h5py.File(file_name,'w')\n",
        "    for key in data:\n",
        "        f.create_dataset(key,data = data[key])       \n",
        "    f.close()   \n",
        "\n",
        "file_name = 'pamap_scaled_segmented_100.h5'\n",
        "\n",
        "save_data(data_segmented, file_name)\n",
        "print(\"File is saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f3dWmwoLhWEH",
        "outputId": "91c9ca52-e2a3-4d1a-e48f-eb2ea51ce885"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File is saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "path = \"pamap_scaled_segmented_100.h5\"\n",
        "\n",
        "f = h5py.File(path, 'r')\n",
        "\n",
        "data_x = np.array(f[\"inputs\"][:]) \n",
        "data_y = np.array(f[\"labels\"][:])\n",
        "\n",
        "print(data_x.shape)\n",
        "print(data_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y7xOGP679WKn",
        "outputId": "8d9b8d2e-b82f-4bbe-effe-208030760677"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37)\n",
            "(19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/53731141/cifar10-randomize-train-and-test-set\n",
        "def shuffle_train_data(X_train, Y_train): \n",
        "    \"\"\"called after each epoch\"\"\" \n",
        "    perm = np.random.permutation(len(Y_train)) \n",
        "    Xtr_shuf = X_train[perm] \n",
        "    Ytr_shuf = Y_train[perm] \n",
        "    return Xtr_shuf, Ytr_shuf \n",
        "X_shuffled, y_shuffled = shuffle_train_data(data_x, data_y) \n",
        "print(X_shuffled.shape) \n",
        "print(y_shuffled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G7Kk09rA3Ems",
        "outputId": "7673f5de-0b17-42b5-9cdf-88246c1cd6f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37)\n",
            "(19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/53731141/cifar10-randomize-train-and-test-set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.33, random_state=1234)\n",
        "# Check shape\n",
        "print(X_train.shape) \n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "67z8auk8inFT",
        "outputId": "10d47e02-02f2-43ef-b404-db99c1b0f36e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12873, 100, 37)\n",
            "(12873,)\n",
            "(6341, 100, 37)\n",
            "(6341,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM, Flatten, Input\n",
        "from keras import optimizers, losses, metrics, initializers\n",
        "\n",
        "from collections import Counter\n",
        "NUM_CLASSES = len(Counter(y_shuffled).keys()) # Hardcoded number of classes in the gesture recognition problem\n",
        "BATCH_SIZE = 50 # Batch Size\n",
        "NUM_UNITS_LSTM = 16 # Number of unit in the long short-term recurrent layers\n",
        "NB_SENSOR_CHANNELS = data_x.shape[2]\n",
        "SLIDING_WINDOW_LENGTH = data_x.shape[1]"
      ],
      "metadata": {
        "id": "7iIHarpB2u13"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model._name=\"Experiement1_1LSTM_without_Attention\"\n",
        "model.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "0NVDN3J_S9Fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "de073bfb-146f-43a0-b122-42d6bb7484ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement1_1LSTM_without_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,668\n",
            "Trainable params: 22,668\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 11s 13ms/step - loss: 1.4186 - sparse_categorical_accuracy: 0.5538 - val_loss: 1.1230 - val_sparse_categorical_accuracy: 0.6342\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9893 - sparse_categorical_accuracy: 0.7056 - val_loss: 0.8632 - val_sparse_categorical_accuracy: 0.7417\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.8170 - sparse_categorical_accuracy: 0.7614 - val_loss: 0.7470 - val_sparse_categorical_accuracy: 0.8000\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.7059 - sparse_categorical_accuracy: 0.7953 - val_loss: 0.7048 - val_sparse_categorical_accuracy: 0.7938\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.6686 - val_sparse_categorical_accuracy: 0.8190\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5822 - sparse_categorical_accuracy: 0.8344 - val_loss: 0.6048 - val_sparse_categorical_accuracy: 0.8350\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.5284 - sparse_categorical_accuracy: 0.8459 - val_loss: 0.5608 - val_sparse_categorical_accuracy: 0.8388\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.5013 - sparse_categorical_accuracy: 0.8546 - val_loss: 0.5580 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4767 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.5591 - val_sparse_categorical_accuracy: 0.8384\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4495 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.5095 - val_sparse_categorical_accuracy: 0.8551\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4420 - sparse_categorical_accuracy: 0.8746 - val_loss: 0.5107 - val_sparse_categorical_accuracy: 0.8501\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4212 - sparse_categorical_accuracy: 0.8770 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8614\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4066 - sparse_categorical_accuracy: 0.8836 - val_loss: 0.4674 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3881 - sparse_categorical_accuracy: 0.8865 - val_loss: 0.4859 - val_sparse_categorical_accuracy: 0.8652\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8908 - val_loss: 0.4393 - val_sparse_categorical_accuracy: 0.8847\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3719 - sparse_categorical_accuracy: 0.8921 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.8691\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3542 - sparse_categorical_accuracy: 0.8957 - val_loss: 0.4574 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3392 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.4553 - val_sparse_categorical_accuracy: 0.8757\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9010 - val_loss: 0.4194 - val_sparse_categorical_accuracy: 0.8800\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3308 - sparse_categorical_accuracy: 0.9040 - val_loss: 0.4056 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.9055 - val_loss: 0.4101 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3135 - sparse_categorical_accuracy: 0.9064 - val_loss: 0.3856 - val_sparse_categorical_accuracy: 0.8928\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3088 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4173 - val_sparse_categorical_accuracy: 0.8878\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3057 - sparse_categorical_accuracy: 0.9091 - val_loss: 0.4159 - val_sparse_categorical_accuracy: 0.8870\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2966 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.3756 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2948 - sparse_categorical_accuracy: 0.9116 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2876 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3782 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.9120 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2722 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.3747 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2731 - sparse_categorical_accuracy: 0.9180 - val_loss: 0.3801 - val_sparse_categorical_accuracy: 0.8936\n",
            "199/199 [==============================] - 1s 4ms/step - loss: 0.3725 - sparse_categorical_accuracy: 0.8959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37246766686439514, 0.8959154486656189]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ManzhuYu/Code-SpatioTemporalAttention-LSTM-main/blob/main/modelbase.py\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2._name=\"Experiement2_1LSTM_with_temporal_Attention\"\n",
        "model_2.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_2.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_2.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_2.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_2.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_2.summary()\n",
        "\n",
        "model_2.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_2.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9BzwM2gLTSR5",
        "outputId": "0fa58fad-6892-46be-b3b9-763ba75ce82b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement2_1LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,950,668\n",
            "Trainable params: 1,950,668\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 4s 13ms/step - loss: 1.3131 - sparse_categorical_accuracy: 0.5967 - val_loss: 1.0788 - val_sparse_categorical_accuracy: 0.6781\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9614 - sparse_categorical_accuracy: 0.7121 - val_loss: 0.8401 - val_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.8328 - sparse_categorical_accuracy: 0.7567 - val_loss: 0.7572 - val_sparse_categorical_accuracy: 0.7922\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.7555 - sparse_categorical_accuracy: 0.7802 - val_loss: 0.6876 - val_sparse_categorical_accuracy: 0.8035\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6970 - sparse_categorical_accuracy: 0.7979 - val_loss: 0.6230 - val_sparse_categorical_accuracy: 0.8346\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6389 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.8146 - val_sparse_categorical_accuracy: 0.7794\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.8212 - val_loss: 0.6371 - val_sparse_categorical_accuracy: 0.8082\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.5718 - sparse_categorical_accuracy: 0.8322 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.8381\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5464 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.6169 - val_sparse_categorical_accuracy: 0.8315\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.5203 - sparse_categorical_accuracy: 0.8496 - val_loss: 0.5373 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4984 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.4582 - val_sparse_categorical_accuracy: 0.8668\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4821 - sparse_categorical_accuracy: 0.8592 - val_loss: 0.5083 - val_sparse_categorical_accuracy: 0.8520\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4590 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.5593 - val_sparse_categorical_accuracy: 0.8252\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4502 - sparse_categorical_accuracy: 0.8709 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4393 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.5128 - val_sparse_categorical_accuracy: 0.8524\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4235 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.4952 - val_sparse_categorical_accuracy: 0.8462\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4159 - sparse_categorical_accuracy: 0.8765 - val_loss: 0.4308 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4084 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4517 - val_sparse_categorical_accuracy: 0.8699\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3973 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.5063 - val_sparse_categorical_accuracy: 0.8606\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3856 - sparse_categorical_accuracy: 0.8838 - val_loss: 0.4429 - val_sparse_categorical_accuracy: 0.8699\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.4608 - val_sparse_categorical_accuracy: 0.8548\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3737 - sparse_categorical_accuracy: 0.8889 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.8722\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3688 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.3747 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3585 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3438 - sparse_categorical_accuracy: 0.8963 - val_loss: 0.4133 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3431 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.3944 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3409 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.3768 - val_sparse_categorical_accuracy: 0.8909\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3346 - sparse_categorical_accuracy: 0.8981 - val_loss: 0.3503 - val_sparse_categorical_accuracy: 0.8986\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3303 - sparse_categorical_accuracy: 0.9003 - val_loss: 0.3718 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3267 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.9068\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.3592 - sparse_categorical_accuracy: 0.9065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3592246472835541, 0.9064816236495972]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = Sequential()\n",
        "model_3._name=\"Experiement3_1LSTM_with_spatial_Attention\"\n",
        "model_3.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_3.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_3.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_3.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_3.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_3.summary()\n",
        "\n",
        "model_3.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_3.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z_oal8obkntv",
        "outputId": "c4289891-bb57-4439-e4a0-b71adc79d211"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement3_1LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,500\n",
            "Trainable params: 30,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 4s 11ms/step - loss: 1.2637 - sparse_categorical_accuracy: 0.6079 - val_loss: 0.9515 - val_sparse_categorical_accuracy: 0.7052\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.7749 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.6936 - val_sparse_categorical_accuracy: 0.7984\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.5914 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.5706 - val_sparse_categorical_accuracy: 0.8532\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4934 - sparse_categorical_accuracy: 0.8622 - val_loss: 0.5075 - val_sparse_categorical_accuracy: 0.8652\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4359 - sparse_categorical_accuracy: 0.8796 - val_loss: 0.4649 - val_sparse_categorical_accuracy: 0.8777\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3939 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.4400 - val_sparse_categorical_accuracy: 0.8804\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3572 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3973 - val_sparse_categorical_accuracy: 0.8901\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3356 - sparse_categorical_accuracy: 0.9058 - val_loss: 0.3848 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3126 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2918 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.3664 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2776 - sparse_categorical_accuracy: 0.9185 - val_loss: 0.3686 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2637 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.3282 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2508 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.3630 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2389 - sparse_categorical_accuracy: 0.9318 - val_loss: 0.3295 - val_sparse_categorical_accuracy: 0.9115\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2284 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.3493 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2222 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.3378 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2112 - sparse_categorical_accuracy: 0.9379 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.9157\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2044 - sparse_categorical_accuracy: 0.9411 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1968 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9165\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1905 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1867 - sparse_categorical_accuracy: 0.9466 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9254\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1764 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.3106 - val_sparse_categorical_accuracy: 0.9161\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.3147 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1667 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.3098 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1650 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1556 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.3261 - val_sparse_categorical_accuracy: 0.9184\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.2758 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1505 - sparse_categorical_accuracy: 0.9552 - val_loss: 0.2918 - val_sparse_categorical_accuracy: 0.9278\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1486 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.3208 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.9571 - val_loss: 0.2979 - val_sparse_categorical_accuracy: 0.9235\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.2828 - sparse_categorical_accuracy: 0.9308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28276267647743225, 0.9307680130004883]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = Sequential()\n",
        "model_4._name=\"Experiement4_1LSTM_with_spatial_temporal_Attention\"\n",
        "model_4.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_4.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_4.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_4.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_4.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_4.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_4.summary()\n",
        "\n",
        "model_4.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_4.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bFDquPOITjLt",
        "outputId": "748df464-c216-4dcb-8cce-0845acf26f69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement4_1LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,958,500\n",
            "Trainable params: 1,958,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 5s 14ms/step - loss: 1.3689 - sparse_categorical_accuracy: 0.5742 - val_loss: 1.0746 - val_sparse_categorical_accuracy: 0.6633\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.8954 - sparse_categorical_accuracy: 0.7327 - val_loss: 0.7793 - val_sparse_categorical_accuracy: 0.7786\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7279 - sparse_categorical_accuracy: 0.7902 - val_loss: 0.7661 - val_sparse_categorical_accuracy: 0.7988\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6199 - sparse_categorical_accuracy: 0.8188 - val_loss: 0.6260 - val_sparse_categorical_accuracy: 0.8183\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5498 - sparse_categorical_accuracy: 0.8397 - val_loss: 0.6316 - val_sparse_categorical_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.4950 - sparse_categorical_accuracy: 0.8558 - val_loss: 0.4861 - val_sparse_categorical_accuracy: 0.8672\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4500 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.4827 - val_sparse_categorical_accuracy: 0.8707\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.4076 - sparse_categorical_accuracy: 0.8815 - val_loss: 0.4632 - val_sparse_categorical_accuracy: 0.8583\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3876 - sparse_categorical_accuracy: 0.8864 - val_loss: 0.5524 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3623 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.4231 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3350 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.3884 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3243 - sparse_categorical_accuracy: 0.9027 - val_loss: 0.3367 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3134 - sparse_categorical_accuracy: 0.9086 - val_loss: 0.3785 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2888 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8897\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2809 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3414 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.9017\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2593 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3490 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2567 - sparse_categorical_accuracy: 0.9226 - val_loss: 0.3549 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2466 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.3746 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2447 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.3569 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.3670 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2281 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2219 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.3676 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2125 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.3619 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2197 - sparse_categorical_accuracy: 0.9366 - val_loss: 0.3167 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2097 - sparse_categorical_accuracy: 0.9339 - val_loss: 0.3546 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2102 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1992 - sparse_categorical_accuracy: 0.9357 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1987 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.3120 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.3318 - val_sparse_categorical_accuracy: 0.9080\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9159\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3405660092830658, 0.9159438610076904]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5a = Sequential()\n",
        "model_5a._name=\"Experiement5a_2LSTM_with_temporal_Attention\"\n",
        "model_5a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5a.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5a.summary()\n",
        "\n",
        "model_5a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e3gMzO2LTs-0",
        "outputId": "41c09263-119c-4a5d-a79c-99333aff2315"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5a_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 153,356\n",
            "Trainable params: 153,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.2954 - sparse_categorical_accuracy: 0.5812 - val_loss: 0.9202 - val_sparse_categorical_accuracy: 0.7157\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.7909 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.7619 - val_sparse_categorical_accuracy: 0.7817\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.6198 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.6218 - val_sparse_categorical_accuracy: 0.8144\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5161 - sparse_categorical_accuracy: 0.8456 - val_loss: 0.6163 - val_sparse_categorical_accuracy: 0.8124\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4650 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.4570 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4220 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8691\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.4915 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3535 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.3863 - val_sparse_categorical_accuracy: 0.8866\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3407 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.3738 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3171 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.3601 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3103 - sparse_categorical_accuracy: 0.9074 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2954 - sparse_categorical_accuracy: 0.9090 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2827 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.3416 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2771 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3411 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2686 - sparse_categorical_accuracy: 0.9192 - val_loss: 0.2949 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2588 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9216 - val_loss: 0.3142 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2413 - sparse_categorical_accuracy: 0.9222 - val_loss: 0.3322 - val_sparse_categorical_accuracy: 0.9056\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2402 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.2989 - val_sparse_categorical_accuracy: 0.9130\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2282 - sparse_categorical_accuracy: 0.9285 - val_loss: 0.2879 - val_sparse_categorical_accuracy: 0.9134\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2296 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.2854 - val_sparse_categorical_accuracy: 0.9130\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9283 - val_loss: 0.3016 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2179 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.2887 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2096 - sparse_categorical_accuracy: 0.9368 - val_loss: 0.3290 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.2670 - val_sparse_categorical_accuracy: 0.9177\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2015 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.2909 - val_sparse_categorical_accuracy: 0.9212\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2052 - sparse_categorical_accuracy: 0.9355 - val_loss: 0.2885 - val_sparse_categorical_accuracy: 0.9177\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.3191 - val_sparse_categorical_accuracy: 0.9064\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 4s 19ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9386 - val_loss: 0.3790 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 4s 18ms/step - loss: 0.1906 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.2698 - val_sparse_categorical_accuracy: 0.9200\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2684 - sparse_categorical_accuracy: 0.9308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26835426688194275, 0.9307680130004883]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5b = Sequential()\n",
        "model_5b._name=\"Experiement5b_2LSTM_with_temporal_Attention\"\n",
        "model_5b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5b.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5b.summary()\n",
        "\n",
        "model_5b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U-KradvolxlI",
        "outputId": "4b7f5056-974e-4b0b-d168-380e99d40aad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5b_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,952,780\n",
            "Trainable params: 1,952,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.4785 - sparse_categorical_accuracy: 0.5314 - val_loss: 1.0519 - val_sparse_categorical_accuracy: 0.6777\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.0453 - sparse_categorical_accuracy: 0.6800 - val_loss: 0.9701 - val_sparse_categorical_accuracy: 0.7029\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8752 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.7526\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.7646 - sparse_categorical_accuracy: 0.7710 - val_loss: 0.6411 - val_sparse_categorical_accuracy: 0.8132\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.8010 - val_loss: 0.6016 - val_sparse_categorical_accuracy: 0.8404\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6184 - sparse_categorical_accuracy: 0.8221 - val_loss: 0.5831 - val_sparse_categorical_accuracy: 0.8280\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5787 - sparse_categorical_accuracy: 0.8282 - val_loss: 0.5602 - val_sparse_categorical_accuracy: 0.8373\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5440 - sparse_categorical_accuracy: 0.8382 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5076 - sparse_categorical_accuracy: 0.8534 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4915 - sparse_categorical_accuracy: 0.8554 - val_loss: 0.5227 - val_sparse_categorical_accuracy: 0.8485\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4737 - sparse_categorical_accuracy: 0.8612 - val_loss: 0.4970 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4569 - sparse_categorical_accuracy: 0.8619 - val_loss: 0.5135 - val_sparse_categorical_accuracy: 0.8548\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4380 - sparse_categorical_accuracy: 0.8708 - val_loss: 0.5278 - val_sparse_categorical_accuracy: 0.8485\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4291 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.8773\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4169 - sparse_categorical_accuracy: 0.8751 - val_loss: 0.4683 - val_sparse_categorical_accuracy: 0.8641\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4093 - sparse_categorical_accuracy: 0.8757 - val_loss: 0.4611 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.8856 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.8835\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3830 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.4304 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3729 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.3883 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3679 - sparse_categorical_accuracy: 0.8909 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.8773\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8912 - val_loss: 0.4742 - val_sparse_categorical_accuracy: 0.8695\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3544 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3832 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3487 - sparse_categorical_accuracy: 0.8953 - val_loss: 0.3626 - val_sparse_categorical_accuracy: 0.8948\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3451 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.4241 - val_sparse_categorical_accuracy: 0.8656\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3378 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.4371 - val_sparse_categorical_accuracy: 0.8656\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3258 - sparse_categorical_accuracy: 0.9020 - val_loss: 0.3894 - val_sparse_categorical_accuracy: 0.8878\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3255 - sparse_categorical_accuracy: 0.9019 - val_loss: 0.4463 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3066 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3640 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3122 - sparse_categorical_accuracy: 0.9033 - val_loss: 0.4014 - val_sparse_categorical_accuracy: 0.8885\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3032 - sparse_categorical_accuracy: 0.9095 - val_loss: 0.3701 - val_sparse_categorical_accuracy: 0.8975\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3612 - sparse_categorical_accuracy: 0.9029\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36121025681495667, 0.9028544425964355]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5c = Sequential()\n",
        "model_5c._name=\"Experiement5c_2LSTM_with_temporal_Attention\"\n",
        "model_5c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5c.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5c.summary()\n",
        "\n",
        "model_5c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rkhgkmJLmoei",
        "outputId": "b9b6b378-4f30-45e1-9869-389f73de5fc2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5c_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 100, 1600)         60800     \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 185,612\n",
            "Trainable params: 185,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.2001 - sparse_categorical_accuracy: 0.6137 - val_loss: 0.7411 - val_sparse_categorical_accuracy: 0.7744\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6075 - sparse_categorical_accuracy: 0.8238 - val_loss: 0.5414 - val_sparse_categorical_accuracy: 0.8482\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4529 - sparse_categorical_accuracy: 0.8715 - val_loss: 0.4984 - val_sparse_categorical_accuracy: 0.8633\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3783 - sparse_categorical_accuracy: 0.8922 - val_loss: 0.3838 - val_sparse_categorical_accuracy: 0.8905\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3332 - sparse_categorical_accuracy: 0.9015 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2980 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3511 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2799 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2569 - sparse_categorical_accuracy: 0.9233 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9192\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.2659 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2288 - sparse_categorical_accuracy: 0.9320 - val_loss: 0.2611 - val_sparse_categorical_accuracy: 0.9243\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2173 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.2760 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2053 - sparse_categorical_accuracy: 0.9378 - val_loss: 0.5238 - val_sparse_categorical_accuracy: 0.8707\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9397 - val_loss: 0.2473 - val_sparse_categorical_accuracy: 0.9309\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1915 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.2352 - val_sparse_categorical_accuracy: 0.9332\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1810 - sparse_categorical_accuracy: 0.9457 - val_loss: 0.2710 - val_sparse_categorical_accuracy: 0.9134\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.9480 - val_loss: 0.2465 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.9476 - val_loss: 0.2559 - val_sparse_categorical_accuracy: 0.9278\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.2238 - val_sparse_categorical_accuracy: 0.9301\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1555 - sparse_categorical_accuracy: 0.9512 - val_loss: 0.2644 - val_sparse_categorical_accuracy: 0.9258\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1557 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.2587 - val_sparse_categorical_accuracy: 0.9274\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1437 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.2423 - val_sparse_categorical_accuracy: 0.9344\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1430 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.2132 - val_sparse_categorical_accuracy: 0.9433\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1361 - sparse_categorical_accuracy: 0.9567 - val_loss: 0.2321 - val_sparse_categorical_accuracy: 0.9402\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1308 - sparse_categorical_accuracy: 0.9589 - val_loss: 0.2850 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9591 - val_loss: 0.2416 - val_sparse_categorical_accuracy: 0.9340\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1263 - sparse_categorical_accuracy: 0.9591 - val_loss: 0.2679 - val_sparse_categorical_accuracy: 0.9305\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9601 - val_loss: 0.2495 - val_sparse_categorical_accuracy: 0.9313\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9594 - val_loss: 0.2270 - val_sparse_categorical_accuracy: 0.9390\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.3091 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1238 - sparse_categorical_accuracy: 0.9616 - val_loss: 0.2737 - val_sparse_categorical_accuracy: 0.9184\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2969 - sparse_categorical_accuracy: 0.9111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29685431718826294, 0.9110550284385681]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6a = Sequential()\n",
        "model_6a._name=\"Experiement6a_2LSTM_with_spatial_Attention\"\n",
        "model_6a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6a.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6a.summary()\n",
        "\n",
        "model_6a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oUrucUvKVIVv",
        "outputId": "4b418f07-be12-4216-a067-4fe09d783742"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6a_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,856\n",
            "Trainable params: 31,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 18ms/step - loss: 1.4279 - sparse_categorical_accuracy: 0.5421 - val_loss: 1.0280 - val_sparse_categorical_accuracy: 0.6668\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.9410 - sparse_categorical_accuracy: 0.7057 - val_loss: 0.8029 - val_sparse_categorical_accuracy: 0.7363\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.7538 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.6370 - val_sparse_categorical_accuracy: 0.8019\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6397 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.5499 - val_sparse_categorical_accuracy: 0.8435\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5562 - sparse_categorical_accuracy: 0.8371 - val_loss: 0.4927 - val_sparse_categorical_accuracy: 0.8559\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5008 - sparse_categorical_accuracy: 0.8515 - val_loss: 0.4932 - val_sparse_categorical_accuracy: 0.8540\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4681 - sparse_categorical_accuracy: 0.8616 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4305 - sparse_categorical_accuracy: 0.8731 - val_loss: 0.4411 - val_sparse_categorical_accuracy: 0.8683\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4128 - sparse_categorical_accuracy: 0.8777 - val_loss: 0.4443 - val_sparse_categorical_accuracy: 0.8583\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3839 - sparse_categorical_accuracy: 0.8858 - val_loss: 0.4673 - val_sparse_categorical_accuracy: 0.8683\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3691 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.3781 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3353 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.3585 - val_sparse_categorical_accuracy: 0.8909\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3261 - sparse_categorical_accuracy: 0.9014 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.9083\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3129 - sparse_categorical_accuracy: 0.9052 - val_loss: 0.3699 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3114 - sparse_categorical_accuracy: 0.9062 - val_loss: 0.3656 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8967\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2861 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.9169 - val_loss: 0.3127 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2745 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2685 - sparse_categorical_accuracy: 0.9188 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2652 - sparse_categorical_accuracy: 0.9158 - val_loss: 0.3067 - val_sparse_categorical_accuracy: 0.9083\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2548 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.3075 - val_sparse_categorical_accuracy: 0.9052\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.2947 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2488 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3027 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9247 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2440 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.3254 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2313 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.3220 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2257 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.3053 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2249 - sparse_categorical_accuracy: 0.9312 - val_loss: 0.2995 - val_sparse_categorical_accuracy: 0.9126\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3148 - sparse_categorical_accuracy: 0.9191\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3147575259208679, 0.9190979599952698]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6b = Sequential()\n",
        "model_6b._name=\"Experiement6b_2LSTM_with_spatial_Attention\"\n",
        "model_6b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6b.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6b.summary()\n",
        "\n",
        "model_6b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X8LFx9armQNL",
        "outputId": "85ad3836-0590-44b9-9f18-558a016b475e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6b_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_14 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 12)                120012    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 127,280\n",
            "Trainable params: 127,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 17ms/step - loss: 1.4094 - sparse_categorical_accuracy: 0.5563 - val_loss: 1.0765 - val_sparse_categorical_accuracy: 0.6649\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.9758 - sparse_categorical_accuracy: 0.7008 - val_loss: 0.7963 - val_sparse_categorical_accuracy: 0.7573\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.8050 - sparse_categorical_accuracy: 0.7515 - val_loss: 0.8007 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6942 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.7284 - val_sparse_categorical_accuracy: 0.7794\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6308 - sparse_categorical_accuracy: 0.8048 - val_loss: 0.6184 - val_sparse_categorical_accuracy: 0.8206\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5895 - sparse_categorical_accuracy: 0.8181 - val_loss: 0.6628 - val_sparse_categorical_accuracy: 0.8155\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5381 - sparse_categorical_accuracy: 0.8333 - val_loss: 0.4733 - val_sparse_categorical_accuracy: 0.8633\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5047 - sparse_categorical_accuracy: 0.8453 - val_loss: 0.5065 - val_sparse_categorical_accuracy: 0.8513\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4759 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.5268 - val_sparse_categorical_accuracy: 0.8536\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4490 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.4923 - val_sparse_categorical_accuracy: 0.8660\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4412 - sparse_categorical_accuracy: 0.8670 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.8637\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4174 - sparse_categorical_accuracy: 0.8724 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8664\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4125 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.4161 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3937 - sparse_categorical_accuracy: 0.8788 - val_loss: 0.4050 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.8842 - val_loss: 0.4454 - val_sparse_categorical_accuracy: 0.8734\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3672 - sparse_categorical_accuracy: 0.8860 - val_loss: 0.3577 - val_sparse_categorical_accuracy: 0.9017\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3550 - sparse_categorical_accuracy: 0.8920 - val_loss: 0.3595 - val_sparse_categorical_accuracy: 0.9045\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3473 - sparse_categorical_accuracy: 0.8919 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.8994\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3369 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.4083 - val_sparse_categorical_accuracy: 0.8800\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3337 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.3646 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3210 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.3846 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3123 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3784 - val_sparse_categorical_accuracy: 0.9014\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3071 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2985 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3774 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3061 - sparse_categorical_accuracy: 0.9053 - val_loss: 0.3237 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.9103 - val_loss: 0.4052 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2866 - sparse_categorical_accuracy: 0.9104 - val_loss: 0.3492 - val_sparse_categorical_accuracy: 0.9064\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2752 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2749 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.3350 - val_sparse_categorical_accuracy: 0.9068\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2666 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.9157\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3046 - sparse_categorical_accuracy: 0.9123\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.304587721824646, 0.9123166799545288]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6c = Sequential()\n",
        "model_6c._name=\"Experiement6c_2LSTM_with_spatial_Attention\"\n",
        "model_6c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6c.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6c.summary()\n",
        "\n",
        "model_6c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VzqGdGRHmxDg",
        "outputId": "8b3b32d6-bcd6-4db5-8e73-ca2be415e8c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6c_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,612\n",
            "Trainable params: 32,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 17ms/step - loss: 1.3470 - sparse_categorical_accuracy: 0.5654 - val_loss: 0.9629 - val_sparse_categorical_accuracy: 0.7014\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8659 - sparse_categorical_accuracy: 0.7326 - val_loss: 0.7576 - val_sparse_categorical_accuracy: 0.7740\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 4s 17ms/step - loss: 0.6831 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.6295 - val_sparse_categorical_accuracy: 0.8144\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5785 - sparse_categorical_accuracy: 0.8393 - val_loss: 0.5307 - val_sparse_categorical_accuracy: 0.8509\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5028 - sparse_categorical_accuracy: 0.8609 - val_loss: 0.4906 - val_sparse_categorical_accuracy: 0.8664\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4375 - sparse_categorical_accuracy: 0.8754 - val_loss: 0.4287 - val_sparse_categorical_accuracy: 0.8722\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3952 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.4524 - val_sparse_categorical_accuracy: 0.8645\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3723 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.3703 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3428 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3253 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.3751 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3050 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.3561 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2959 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.3767 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2843 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3302 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2740 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.9068\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.3134 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2544 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2456 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3221 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2380 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.3283 - val_sparse_categorical_accuracy: 0.9045\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2270 - sparse_categorical_accuracy: 0.9313 - val_loss: 0.2994 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2274 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.3040 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9341 - val_loss: 0.3511 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2111 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.2707 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2046 - sparse_categorical_accuracy: 0.9377 - val_loss: 0.2757 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9364 - val_loss: 0.3296 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9373 - val_loss: 0.3060 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1897 - sparse_categorical_accuracy: 0.9398 - val_loss: 0.2743 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1903 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.2777 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1857 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.2885 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1792 - sparse_categorical_accuracy: 0.9446 - val_loss: 0.2869 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.1806 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.2754 - val_sparse_categorical_accuracy: 0.9239\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3044 - sparse_categorical_accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3044148087501526, 0.924144446849823]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7a = Sequential()\n",
        "model_7a._name=\"Experiement7a_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7a.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7a.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "\n",
        "model_7a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7a.summary()\n",
        "\n",
        "model_7a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d0mKCC0ulmYf",
        "outputId": "c96ad5b8-be3d-4c88-cb60-ded7df3dec16"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7a_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_22 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,960,612\n",
            "Trainable params: 1,960,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 19ms/step - loss: 1.4059 - sparse_categorical_accuracy: 0.5514 - val_loss: 1.3026 - val_sparse_categorical_accuracy: 0.6027\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.9017 - sparse_categorical_accuracy: 0.7304 - val_loss: 0.7770 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.7303 - sparse_categorical_accuracy: 0.7850 - val_loss: 0.5881 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.6226 - sparse_categorical_accuracy: 0.8184 - val_loss: 0.5775 - val_sparse_categorical_accuracy: 0.8482\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5407 - sparse_categorical_accuracy: 0.8418 - val_loss: 0.5272 - val_sparse_categorical_accuracy: 0.8497\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4929 - sparse_categorical_accuracy: 0.8560 - val_loss: 0.4604 - val_sparse_categorical_accuracy: 0.8695\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4510 - sparse_categorical_accuracy: 0.8685 - val_loss: 0.5798 - val_sparse_categorical_accuracy: 0.8291\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4209 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.4870 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3938 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.3994 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3771 - sparse_categorical_accuracy: 0.8871 - val_loss: 0.3681 - val_sparse_categorical_accuracy: 0.9021\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3554 - sparse_categorical_accuracy: 0.8947 - val_loss: 0.4686 - val_sparse_categorical_accuracy: 0.8555\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3353 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4074 - val_sparse_categorical_accuracy: 0.8932\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3221 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.3431 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3145 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3216 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2961 - sparse_categorical_accuracy: 0.9098 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3079 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9186 - val_loss: 0.3182 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2623 - sparse_categorical_accuracy: 0.9195 - val_loss: 0.3168 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2612 - sparse_categorical_accuracy: 0.9207 - val_loss: 0.3263 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2529 - sparse_categorical_accuracy: 0.9213 - val_loss: 0.2894 - val_sparse_categorical_accuracy: 0.9270\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.3044 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2399 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.3320 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2268 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.2994 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9317 - val_loss: 0.2741 - val_sparse_categorical_accuracy: 0.9250\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2277 - sparse_categorical_accuracy: 0.9298 - val_loss: 0.3128 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2200 - sparse_categorical_accuracy: 0.9311 - val_loss: 0.3092 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2141 - sparse_categorical_accuracy: 0.9352 - val_loss: 0.3326 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2165 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.2750 - val_sparse_categorical_accuracy: 0.9328\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1974 - sparse_categorical_accuracy: 0.9367 - val_loss: 0.2873 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1999 - sparse_categorical_accuracy: 0.9367 - val_loss: 0.2782 - val_sparse_categorical_accuracy: 0.9254\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.9230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30051830410957336, 0.923040509223938]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7b = Sequential()\n",
        "model_7b._name=\"Experiement7b_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7b.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7b.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "\n",
        "model_7b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7b.summary()\n",
        "\n",
        "model_7b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A-aARh2Qm56G",
        "outputId": "d93799fb-04a0-42d2-fd2a-fd80a520d358"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7b_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_20 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,959,856\n",
            "Trainable params: 1,959,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 19ms/step - loss: 1.5433 - sparse_categorical_accuracy: 0.5096 - val_loss: 1.1725 - val_sparse_categorical_accuracy: 0.6151\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.0232 - sparse_categorical_accuracy: 0.6791 - val_loss: 0.8839 - val_sparse_categorical_accuracy: 0.7258\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8172 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7188 - val_sparse_categorical_accuracy: 0.7969\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6994 - sparse_categorical_accuracy: 0.7836 - val_loss: 0.5954 - val_sparse_categorical_accuracy: 0.8353\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6372 - sparse_categorical_accuracy: 0.8071 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.8384\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5805 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.5879 - val_sparse_categorical_accuracy: 0.8334\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5402 - sparse_categorical_accuracy: 0.8407 - val_loss: 0.4692 - val_sparse_categorical_accuracy: 0.8703\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5014 - sparse_categorical_accuracy: 0.8486 - val_loss: 0.5269 - val_sparse_categorical_accuracy: 0.8377\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4772 - sparse_categorical_accuracy: 0.8584 - val_loss: 0.4714 - val_sparse_categorical_accuracy: 0.8594\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4563 - sparse_categorical_accuracy: 0.8646 - val_loss: 0.4307 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4345 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.3920 - val_sparse_categorical_accuracy: 0.8757\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4102 - sparse_categorical_accuracy: 0.8708 - val_loss: 0.4211 - val_sparse_categorical_accuracy: 0.8850\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3963 - sparse_categorical_accuracy: 0.8802 - val_loss: 0.4054 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3807 - sparse_categorical_accuracy: 0.8822 - val_loss: 0.3866 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.3901 - val_sparse_categorical_accuracy: 0.8936\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3525 - sparse_categorical_accuracy: 0.8961 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8843\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3415 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.3906 - val_sparse_categorical_accuracy: 0.8874\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3339 - sparse_categorical_accuracy: 0.8985 - val_loss: 0.3687 - val_sparse_categorical_accuracy: 0.8948\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3281 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.3229 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.9048 - val_loss: 0.3474 - val_sparse_categorical_accuracy: 0.9014\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3155 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2975 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2977 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2899 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2960 - sparse_categorical_accuracy: 0.9093 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2838 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.3135 - val_sparse_categorical_accuracy: 0.9115\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2829 - sparse_categorical_accuracy: 0.9144 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2747 - sparse_categorical_accuracy: 0.9144 - val_loss: 0.3346 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2697 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2677 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3078 - val_sparse_categorical_accuracy: 0.9146\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2632 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.2734 - val_sparse_categorical_accuracy: 0.9219\n",
            "199/199 [==============================] - 2s 8ms/step - loss: 0.2863 - sparse_categorical_accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2862699627876282, 0.9268254041671753]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7c = Sequential()\n",
        "model_7c._name=\"Experiement7c_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7c.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7c.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_7c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7c.summary()\n",
        "\n",
        "model_7c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NdWJNBDynDOa",
        "outputId": "62092723-5711-459c-a754-50d839808b80"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7c_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_24 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " lstm_25 (LSTM)              (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161,188\n",
            "Trainable params: 161,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 8s 21ms/step - loss: 1.3187 - sparse_categorical_accuracy: 0.5608 - val_loss: 0.8294 - val_sparse_categorical_accuracy: 0.7313\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.7471 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.6162 - val_sparse_categorical_accuracy: 0.8136\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5494 - sparse_categorical_accuracy: 0.8388 - val_loss: 0.5155 - val_sparse_categorical_accuracy: 0.8458\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4440 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.8672\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3933 - sparse_categorical_accuracy: 0.8852 - val_loss: 0.4170 - val_sparse_categorical_accuracy: 0.8742\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3454 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.3712 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.9025 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.2936 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.4057 - val_sparse_categorical_accuracy: 0.8936\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2787 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2598 - sparse_categorical_accuracy: 0.9208 - val_loss: 0.3204 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2405 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2335 - sparse_categorical_accuracy: 0.9305 - val_loss: 0.3473 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2191 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.3429 - val_sparse_categorical_accuracy: 0.9095\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.3125 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1966 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.3080 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1913 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.3336 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1811 - sparse_categorical_accuracy: 0.9454 - val_loss: 0.3059 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1725 - sparse_categorical_accuracy: 0.9451 - val_loss: 0.3003 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.1700 - sparse_categorical_accuracy: 0.9464 - val_loss: 0.3107 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9495 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.2883 - val_sparse_categorical_accuracy: 0.9247\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1498 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.2871 - val_sparse_categorical_accuracy: 0.9293\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1455 - sparse_categorical_accuracy: 0.9543 - val_loss: 0.2970 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 4s 17ms/step - loss: 0.1418 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.9289\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1385 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9576 - val_loss: 0.4237 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1323 - sparse_categorical_accuracy: 0.9581 - val_loss: 0.3115 - val_sparse_categorical_accuracy: 0.9223\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1255 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9293\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.9615 - val_loss: 0.3121 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9593 - val_loss: 0.2997 - val_sparse_categorical_accuracy: 0.9274\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.9317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28986841440200806, 0.9317142367362976]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7d = Sequential()\n",
        "model_7d._name=\"Experiement7d_2LSTM_without_Attention\"\n",
        "model_7d.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7d.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7d.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7d.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7d.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7d.summary()\n",
        "\n",
        "model_7d.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7d.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7d.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PWR8jJ0_ynSW",
        "outputId": "9f58e07a-ca46-498f-efdf-c09b30739a2a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7d_2LSTM_without_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_26 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_27 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,780\n",
            "Trainable params: 24,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 6s 17ms/step - loss: 1.4992 - sparse_categorical_accuracy: 0.5020 - val_loss: 1.2027 - val_sparse_categorical_accuracy: 0.5977\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.0291 - sparse_categorical_accuracy: 0.6687 - val_loss: 0.8551 - val_sparse_categorical_accuracy: 0.7250\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.7987 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.6738 - val_sparse_categorical_accuracy: 0.8093\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.8010 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.8206\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6088 - sparse_categorical_accuracy: 0.8234 - val_loss: 0.5623 - val_sparse_categorical_accuracy: 0.8361\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5637 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.5036 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5199 - sparse_categorical_accuracy: 0.8492 - val_loss: 0.5059 - val_sparse_categorical_accuracy: 0.8563\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.5016 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.4836 - val_sparse_categorical_accuracy: 0.8579\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4469 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.4403 - val_sparse_categorical_accuracy: 0.8715\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8730 - val_loss: 0.4510 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4100 - sparse_categorical_accuracy: 0.8803 - val_loss: 0.4412 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3899 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.4041 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3822 - sparse_categorical_accuracy: 0.8870 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8870\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.4004 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3599 - sparse_categorical_accuracy: 0.8932 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8928\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3472 - sparse_categorical_accuracy: 0.8965 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3372 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.3745 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3725 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3208 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3132 - sparse_categorical_accuracy: 0.9035 - val_loss: 0.3725 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3804 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3045 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2976 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.3453 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9106 - val_loss: 0.3731 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2814 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2758 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3432 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2713 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3667 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2656 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3438 - val_sparse_categorical_accuracy: 0.9021\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3340 - sparse_categorical_accuracy: 0.9079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33398544788360596, 0.9079009890556335]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "\n",
        "![image.png](data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAv4AAAFJCAYAAADjQinpAAAgAElEQVR4nOydu5KqzNfGH75672P+Ae7A8grwCtDEyNQMQkl2Zmg2CYSS7dTIRLgCuQJrgoHAuZL+AkAaBMTxDM+vaqpGoA+rVx9Wr+4G5XA4CBBCCCGEEEJazX8A8PHx8ex8EAA/Pz+d1gXlp/yUv7vyk/bDOt5NqPfX4efnB//37EwQQgghhBBC7g8Nf0IIIYQQQjoADX9CCCGEEEI6AA1/QgghhBBCOgANf0IIIYQQQjoADX9CCCGEEEI6AA1/QgghhBBCOgANf0IIIYQQQjoADX9CCCGEEEI6AA1/QgghhBBCOgAN/zYTORgqChRlCCd6dmZIV4icIYYtq3BtlImQ1sCxrlu8vL59mIoJv+Lus8cTGv6vTORgOHQQ4XcVxf+0ADuEEDvM1dyd0krpmwoUJf4bOpHUuIp/QzhRBGdY1fB8mIoCpabik3NEcIav2qm9I6fl6ZsKFJM1lJB3p3qsIy/J3Wyb68fNLowLNPxfmfALweAP0no9+HN5j9Y4jG9itLcRCgEhBBZfn/DVOXbJ79DWoNkhhBBSY9NgGMB6W2hl/gaupkG7OLeEPA59JSBW+rOzQQi5Ab8ZH8mTeKRtcyFdGBdo+L8okTOEMnIBdwRFUdCzArijkpmsb+a88elE1TcVxMEVKMnM+hL01QpNqn5/MgWsT8mzH8FZ7mEvphemSDIiOMMerCCA1cv0FznDTNdHj4QPc+jAOa7WmPCPKy6K5EmJV3kcKY5h3v1dWo9SD4ppDqUwWfzZCtA9ykBOI109qpOjKl/V5Xk+LCHkPtT3SeX93Wl/VDrWXdqfNe5Dq/qIU1lyTuNcfrJw5TK2m/vZNmX9/OXj1+m4UKXX82PGd9V4W1Ymj6wHh8NBkNegqIvQ1oRmh0KIUNiaJuywECC0hQbpeuG3Z0AYXllKnjBgiOItz4AAkKRZTEorXE/zFApbk9IJbaFptghDW2gladTR9bqYl7+g87Rck5+Zbj1hSDqLdZiEy+kg/1z8W36uqh7F+kV5RUozIyDl7bSu/EJ+z6hIs0aO2nydtqHafN5Ipkvoev0n7Sdfx8/0SaX9XXl/lBvrLu7PmvahBXJ9RBzHMV7PEMj1vRXjd6mM7eNxtk0xvmvHrzq91sVTqNuF/OfSeHA9OBwOgh7/Fyb8CpLlrBBfwQAnK1vhFwJjke1xU+dYGMHp1puG6CsBIUJM170LPJ4q5gsD7ibxx35aGCzm4KLrbYm2awSBhV7iFRi5wP47VZCBRVIJen0NSOuEOsZU2+P4mPQcoGNiBPgK0aAeabD/5td/ch6KkXt7gfUJDHdUcU6kQo4r83V3mQghBcrbcn1/d9of5fhFf9a0D63vIwx46RYRfQIjvexv4GpTjAuDYr2M7eaxts2141eFXs/GI9XtJP/pOJXLyxPqAQ3/lyTe5nBczlJGcOFi1HDLznV731TMdwLeJY1Mn8Bwl3AiHxvXwKTd2+Oeh+ElZyziv92dT7FV1qPIwcwawEvzEtp3OM+hYyUEhJhg0/Sg+DX5eohMhJDG3Li/u3pP+D36iAf36c/nmbZNmoUb6fGW9eHB9YCG/0uiYr4LYWtGXKlCG5pmI9wVPOm9PjR3mXnmIwdLV0O/d3mKkTPM7YP83l/SyGJPjdUbYW//bXQ2gFyGOp5CczdXviXJxTLbXJrVlUvrUfiFQOsjve1/WgiuylcdOlYihK252ByFr5Djmnw9VCZCSEx5W76qv7vhuJjjt31Erw8tWKPoR7tNn/5uPN62OeFWff3ZeCrGqQLPqAc0/F+WEF9IKlXhBPwRdY6dN4gPsigKlJ6Fgdf0dWYuRtKhlO14AYzS3z1YAw+XHGzX/9rQoGFaXM8kv0TFeIrskJI6xz97n9PZ5YdPDUwxO60rl9YjfQVvIC1Nwsgtf96C3BLqSX2skKM2X4XyfIJMhJAi1X3Sr/u7q8bFGn7bR6hz/LOR5SeV5SZ9+jtyT9vmTD8P3K6vPxtPvm7D/lee/yfUA+VwOIiPj4/7pkIa8fPzgy7rorn8EZzhDPiXdgTF37/hNA7fVDCC97BXe91X/z5MZYOJaPa2pmfQTP7Xl+O3NJP/FnX9kfESkpGv4+1tyyRP122bV+Ln5wf/PTsThLwq+kpAPDsThBBCCCE3glt9yIvS9J3sJsxL3nlf+k7eJu95R827oc+8w/kl0LFqhWetLXKco+w7Bpe8p1o+DF34ffJO8bJ46+IoS7Ob7yQn19CVtkzIa0HDn7wo6Vtdkj9vAGvmIDoeDtJghwJit8Iq93sONXIwW0+PXyH2MJIMcRfW1yKJ04A7MuGfxFnyOtLIwXC0j+8nh5L2uY+OlMVLyC/xP2EN5Dc9rKBX1tMALhYN3wbhw5Trsdhhrjao/ycU0qxtc4QQQl4FGv7kZfnte9XPvfO+6p28tZx9r/Av4yWkjNrvGBQ5815zmYp3il9OPs0uv5OcEELeCRr+5DW59h25D3ov7k3eK0zICb/4jsGz6dw7yQkh5P2g4U9ekyvetXuX9+Le873ChFRS9h2DJmTPR84Sx/WyineKXxRHCd18JzkhhLwfNPzJa3LRO9lv8c77M+//vde7oQkpofo7Bg3eUw0dK89IvoypYIZp1naq3il+Em9NHGV09p3khBDyXvA9/i9E8V23kTPEDP86s2Te9Xf9Un7K32X5Sfs5reMRnOEn/uza9HYfH+bwG38bHZLvBuzbXoefnx96/Al5P+LXKWYe1eLvW8QZf8DsNV/L2HX5CWkJ/ieswaRFRj8A6JgMLHyy63hzbjGuvCb8gBchpJSuf8Cs6/ITcm/8jQtjsnp2Nm6OPjEw2vhY6e2a0pB2QI//21D1QavYMyl/aCpyhkdPZflHdco+wFP2wSDyPLr+AbOuy09I2/Gxyb0goXqMi28XPzxXd/3SD9BdnnbduBu/DIKH3cs57S+zcqzTw6V682EOHTimbNNkz5/oLjdeVI8JrbCnDoeDIK9BURehrQnNDssf9gwBzRZh8X8RClvThB0KIUJbaMfrQngGhOGlz0Ag/pHFJ/9+Al2vi7XyV+m47Hel3j1hQNK7ZwjAEF5pnIX6F9pCQyGN4++6eCn/TeQnpAXk67gnjLp2kmvznjCQb5/i7HU5bvl3yfj3m7Rr+6QzsnWMU71Dsm2qdChKyvhSvWXpeAYEINlGqX5q7aQm48pr2lNVHA4HQY//G1H5QSt9AiN9RV+0xRrxB3rqP6pT+OjPRR8MIo+g6x8w67r8hLSa6Bv74qWqNl/14blff5Du9KN3F6ddMe5m7MFv2FVhYHHsTHVMjABfYfyrvt+/QG+FdHp9DUj7cHWMqRbrp+nHB9tkT9HwfxdqP2il468NrLcRou0amI6ztwk0/qjOG34wqM10/QNmXZefkLaj/sFA/n1tm7+GX6VdM+4CAAZg93Ahl+rhVnWm6XjREnuKhv+7cOaDVup4Cqw/8bkGponb4Xcf1fntB4PITen6B8y6Lj8hnUDyite1+aoPz9V+kK75B+h+lTbKx904wdPVDCLjYpl2pnJfemm/f8U4kdJ0vGiTPUXD/12o/aAV4qUruHDl5cYLPqpT/cEg8hS6/gGzrstPSOvJb/GobfNVH56r/CDdhR+g+03aQPm4CyRbA9v2mtJbYmCK2Wlfes7OKXLp82VUjhe/H1de3Z7iB7xeiGs/cuGbCjYT8VIV7BK6/pEPyk/5uyw/aT8nddw3oWwmEO86aKF83H33sfjW5PXuw1Q2mIg2fbTtfeAHvNpE5GDpGpiwJRFCCHkH9AmMd37tZem462PDsZi8MPyAV1tQ59jxa0OEEELeBh0r8cYWcum4++Yy3R2Wz7Ohx58QQgghhJAOQMOfEEIIIYSQDkDDnxBCCCGEkA5Aw58QQgghhJAOQMOfEEIIIYSQDkDDnxBCCCGEkA5Aw58QQgghhJAO8B8AKIry7HwQQgghhBBC7sh/ACAEv/z0Cpx8zrxjUH7KT/m7Kz9pP6zj3YR6fx1+fn641YcQQgghhJAuQMOfEEIIIYSQDkDDnxBCCCGEkA5Aw58QQgghhJAOQMOfEEIIIYSQDlAw/H2YigJF/jP9h2TENx+XVjkRnOEQTlS47JtQFAVPzRohpHNEzhDDkw6JEEII+T0lHn8NdiggRPK30h+SEX31uLSa4psKlM0EnvHUTHDiQQipoMJhQcg7kIxv6V/lRLfuudy907bgm7Iz0wSH0hdA1tnQQdT03kk01bqtvNe0zrWYZlt9fFNSgDzQ+DAVE44zLC3ESLqeefPj8KY5zD1f9G6Vh/VhDh04pqzMbJWiUfhCfuNbEZxhD1YQwOplle3Zk5GXmHgQQgghN8eHOdpnjsbQBqxZySS27LnP2JCLHAxHgJc6Kr0BrF5m5PmmghG8zJEpVngt92IX8WEu+wiFgBAhbFiYHZVed68QS41uq+/V1KUOUWL4J8avPHvWV/AGsQIiZ4b19B/mavq8izX+JYXrYZA23MjBbD1NFCjgYSR5rQO4WEAIgV0WUUZd2MDCuh/G1w0XI2WJfnjaGVSn7cL6WiSdhAF3ZMKHivkuhK0lqx27OUpy9XCePfF4LlWTtPR2ftbOFRHSbiI4w6IHq8xh0dw5QshT8TdwjUVmS6hzLIwA623zOhpt1wiMSWbM6xMY2OM7cUxuXANeZ8fQV0XH6mhjqRhPtYb3ZOp0S72f48xWn92xUeorDwOrh541wCJnrBvSbx0TI8BXmDTIwEIvGXBGLrD/jrI0/lYrpT5sll6vrwFpx6GOMdXiBn8u/LFC6BPQmf7KlE3SkHh58rP2/YjbHUiL8T9hDYoerAqHRRPnCCFPJvren1zr9csMPR2rcIp16pDsfWGReHDVPwPA3Uh1uoe+FtsgiL6x1/ZYPuHMImlKhO06wOBPmau15l6dbmv1Xl2XukTzt/pE3zhtpmcw5IGqwrt/j7C3CE9egIpJWviF4EpPESFvhT6B4Y4a7lE+7xwh5J2ItmsEMOCFNjS4GKWGnL5KJrepkdeDFWjo95KAwQCLox3gwXBHXB1+ISJnBmvgocw5X3cPQL1ua+5V1qUO0dDwj+DMLAw8gdDeFwrKxTJ1tUYOlm7c6NTxFFpuJt6ca8LeIjx5T8q9BoS0AR0rISDEBBseUiQtJfwKTi/6JnrrKUKxgq7OsRMh7H1myOkr6WUkwoOBAcqHAh0TLvG/DJEzjPVaYtnX3SunTrfSvTN1qSuc2eMfL5HIMy91vijMmg1MMUuWTSwMvGR7kDrHP3svzcQv2IpxTdhfh1cxniJ3uDc9FT5yAXf0i3yQ+9DrQ3OXmS6kCSch7UbHSoSwNRebjg1WpF3EDrrTfnw6PufACVE2PwAA3xzBTff8q2NM5XbCceJl8E0Fva9F6XnKuntH6nR7kd6r61KrORwO4vd4woAhvCtiIBlFXXgGBCD/acIOn5S5B5CXv1i3Cr89I1c2Rgsq4XVt8f2h/Hn5Q1sTWtLgQ1vL9wVShT/e02wRFtpJaGvSs6GwtXb3IeS1OWnjlf24Jwzpd3EsTNuFCG2hVbSLsvttGCfekZzeizoDBNI+q+6eCIWtSTqs023Nvcq61BEOh4NQDoeD+Pj4+OW0wYepbDDp4OGIe/Dz84Pf6+L9ofyUn/J3V37SfljHu8lt9E578xb8/Pzgv+ui0LESVAEhhBBCCLkPkbMEvB2N/htwpeFPCCGEEELI/VDnO6yenYmW0Px1noQQQgghhJC3RTkcDuLZmSCEEEIIIe3j4+N/z84CkfgPAA/bvAhdP/hE+Sk/5e+u/KT9sI4T8ny41YcQQgghhJAOQMOfEEIIIYSQDkDDnxBCCCGEkA5Aw58QQgghhJAOQMOfEEIIIYSQDlAw/H2YigJF/jP9h2TENx+XVjkRnOEQTiRd8s2sHIYOosqwhBByWyJniKHDXuccLCdCCGlOicdfgx0KCJH8rR7zgWR99bi0muHDXPYRCgEhQtiwMHvG4JJMPp46JyKEvCglDounxkPIBcjONUWpnsDlnqtx0JXE4ZuyM9MEh9IXwQcUBfW2TfJM+ier1hnm7ylmdbihU5OEmX/Wb3LvgvhfkWZbfXxT8njLA4QPUzHhOMPSRhdJ1zNvfhzeNIe554tem/KwPsyhA8eUG3G2StEofCG/8a0IzrAHKwhg9VLvvo7Vbg4VAKBiPNUuK9kb4JsKlM0EnvHwpAkhhJA74sMc7TNHY2gD1ux08hk5GI4AL3VGegNYvdSAr4/DNxWM4GWOTLHCK7kXu4pvAsoG9bZNhETvgBCA8ACrlzfMDS+5JwCxyq6bI8AOk+shAAulTg3fBEaQ4hA41o+6e2Xxv9OEssTwT4xfeWatr+ANYo935Mywnv7DXE2fd7HGv6RReRikjS5yMFtPE4+5gIeRNLML4GIBIQR2WUQZdWEDC+t+GF83XIyUJfph2uA/48KvTduF9bVIOhAD7siEDxXzXQhbS1Y7jgb/MUPYrgMM/pTk9Y683irI4/DNkolcosTqCWV7PDtdl796kp7eznv52r8iVqbfModFc+dIPu5iPNc6X+qdQtX6K3MMFbegcmWiFfgbuMYisyXUORZGgPU2r9xou0ZgTDKDXZ/AwB7f0bk4fGxcA15Hx9BXRl/lDfUyoi0QGJD0DhhArPc6fMA1INUJYGEA6+3poxsX8CryUXfv3Tmz1Wd3LDx95WFg9dCzBljkjHVD+q1jYgT4CpPGGljoJZ31yAX2R41psP9WN8b6sFl6vb4GpI1eHWOqxZ3BufDHjkCfoIkzPXJmsAYe2H88Dn1iIFhvj6tM2zXiOlM1qfM/YQ3a49npuvwxZZN0JB7AvJdvP2q5MViq3wqHRRPnSI6SeK51vgCocwrV66/oGNKxOsqdeHxnPHP17kTf+5Nrvf7pyrr6ZwC4G6le9dDXEjujLo7oG3ttj+UTziyS61H/AHDznvS+BnyF2W93lG23SVUbfZ/G1euXJBABew1Ylm0XqrsHYBUC615yvQcspNWAd6D5W32ib5w2sTMY8kBV4d2/R9hbhE+InCF66ylCWv2PRZ/ACNbYRgCiLdaYYqzWTOr0CQx31AJPd0LX5QdQOUkPvxA08BS2iov028A5UuWJT7jW+VJ8TnYKndffqWMot/owcpuVGWkH+iqZYKYGfA9WoKHfaxA2GGBxtAM8GO6oA6uDLUGPtwKNJOPbCnDU+3wnbcPx4knAxaoNYqM9jcdwpTMHNfeiLRAA8EJAAzAyK+J/URoa/hGcmYWBJxDae4xyLcfFMh04IgdLN26Q6ngKLTdLb841YW8RPsU3FfS+FiVbf8j90fHXBtbbCNF2DUzHmQ5KJ3WpV3CCTSu2unRd/st59Fa8x3Jb/arz3XmnyI2cJ02p1F/kYGYNsj3eoY3Hn7gijyD8Ckqv6ytpxUd4MDBAVXWpiiOefN4mn+Qx6Kv8HnsDKNf7mW1A4Vez9Orqx/GeD/TWQCgAXQV2ArD3Zw4pvxhn9vjHS2PyVhd1vijMmg1MMYuf7VkYeMn2IHWOf/ZemqVfsBR/Tdhfh1cxniLb4xo5WLoA3FGL9k2/F+p4Cqw/8bkGpmP1eK1+UqdjJULYmovNmyur6/JX0utDc5dZm5YcDu3nMfq9jfOk3Cl0sf7CLwRaH+lt/9NClWlH3oe4jp3Wg7Svq8I3R3CTPf+1cahjTOV20ql+on34Zrx3v3TvhQ+4iCcF6hjQXOkwbwQsXWA6LoRRgakGqX7Ez/V7Z+6VUDnXfFUOh4P4PZ4wYAjvihhIRlEXngEByH+asMMnZe4BnNbFUNgaBDRbyGKHtnZSJvlrEDDer1ZSfln+Yt9S+O0ZOXnfUNwTivoPbU1oSYOv0+/xnmaLsFBOoa1Jz4bC1qr7kHw85fWsqIfq+OPnbCmOnI4q9Veex1xfaBjCkNqEXE7ktTnp4yrrgSeM9HdoC62ub6vrCwph29BPvCe5xcMS2wbCjo8NCVuDMDwIEaKg93wcRiG8l18UKtSJ7N4xfnGaRm6Rs+ZeMf+ajRMZX/lPORwO4uPj45fTBh+mssGkFQcJn8/Pzw9+r4v3p0x+31SwmYhOHKym/Kz/7ZGfYwM5pV11nDRHafykqQCTOx6WvXf870Dzw72l6FixYyf3InKwdA1MulrBui4/IYSQzhA5ALz7GeX3jv9d+O/ZGSCkEnWOnXh2Jp5I1+Unb4yOlej68EoIuQR1Dtzz1fn3jv9duNLjTwghhBBCCHkHlMPhQJ8iIYQQQgi5OR8f/3t2FojEfwB42OZF6PrBJ8pP+Sl/d+Un7Yd1vJv8/Byo9xfh5+eHW30IIYQQQgjpAjT8CSGEEEII6QA0/AkhhBBCCOkANPwJIYQQQgjpADT8CSGEEEII6QAFw9+HqShQ5D/Tf0hGfPNxaZUTwRkO4UTSFWcolUX+HiGEdIXIGWJY0wGeu08IIeQ1KPH4a7BDASGSv9Vjvr6orx6XVjN8fK6nCJNyCG3A+nzCxMQ3oSgKnjonIoS8OaeOjcc7W07zQDpOMr6lf5WTx9xzFzroOIa+HLU6a1QnIjjDGid1XRxN61yLabbVxzehDB3ExSN33j5MxYQjKVEuxJxyj0qJw5vmMPd80WNUHtaHOXTgmKnSTPjSKkWj8IX8xrciOMMerCCA1VMSWXWsdnOoScjwK4DW711StlfjmwqUzQSe8dBkCSEd4PWcLaRb+DBH+8zRGNqANTs13CMHwxHgpc5IbwCrZ8JP4qhz0HEMfUXqdNawTiQYXpmTui6Oy+JvLYfDQWR4wgAEjn+asMPkjgGh2aEIbU1o6cXk+fzvJExoC02zxfGOAWF4QggRCluDQPzjSC7eyrD59DxDymNoCw2G8BqEP6btGQJpGBEKW8vkTfN0LItCfu9BXhcZWf7bjSx/Wt9SQls76qBcL0m9OtbdVK/vw2lbNIQtyZqrA54hydqO+kH9y/qvkue0XhT749P+uxBX0jeW9eVlfb84efaU/P2yvJflwROGZgvbkJ/L8lGXHnlPcnXcM07G1WK7FyLf9pOnMjujQFn49Hob+sh3pcq2EaKgs4Z1Iu1jSnVaF0fj+NvL4XAQZ7b67DBPXN76ysPA6qFnDbBILwIADOm3jokR4CsEou0aQWChl3jWRy6w/46yNP5We5rqw2bp9foaYCziPKpjTLU9vqPz4b10ZqhPUOcIUOe742wy7C+lVQ9yb/SJgWC9Pa4ybdeI60zkYCZ5CzyM4lUb/xPWwMtm/2KF9/dlurC+FomXy4A7SrxckYNhwWuxH7VrC0Xn9V8rj4s1/iXXPQyOHisdKyF5wLwBrJmDCCrmuxC2lvTt0kpmRlXYW+W9Ig+BhXU/jHVpuBgpS/TD1BP3Ce7OaC/R9/7kWq+vnVxT/wwAdyPVhR76WmxnAPnV/RE87OantZu8FlU6a1onUtyRUti9UR/HpfG3leZv9Ym+cVpkZzDkzl9c1iCvCXuL8AXU+QJGsMa2RcbVS6NPsvKOtlhjirFaM6nTJzDcUbL9qy1UTFLDLwTphBcA1DkWRoB1mypn1/VfK0+5swUobHEcuRcl2TRsVLG1s1neq2WpcuSQjqOvkklhauT1YAUa0p23dNC9H9frTMV8JzsqJMcYOUtDwz+CM7Mw8ARCe49R7pSMi2Xa+UcOlm7cINXxFFpult6ca8LeIjyA+FxD7rDIBi4G+ENnwoPQ8dcG1tsI0XYNTMeZl7J0Upd6LCfYHM9/dItBqypn1/X/C3kiBzNrkO2FDm009mVdEFYetMsdKm3TBXkU4VdQel1fSUae8GBUjMV00L0f53RWVSdy6BMYqHYU1MXRKP6WUWL4JwdcpYOxkTODNfCw0hMluSPphLyBKWbxsz0LAy/ZHqTO8c/eS7P0C7YiXBP21+FVjKfIDvfqf2HvR5IHDPDeffvAm6GOp8D6E59rYDpWj9fqJ3U6ViKErbnYtNXa6PWhucusTksT7jZB/QPl8pQ7WxB+IdD6SKuB/2mh8ZB2TdiL8k5ITNyWT/uxtK1X4ZsjuMYkHovpoHs/anT22zrRNI5fx9826g5dnCc+aMYzM7ehqAvPQO4AZ/HAXds4rYvJgUDpoLYQhcOdSZnkrz3mMPatKTvc61X9bvnh3pju6r9anvpD37k+wzCEIZXdMc7Sw73nwzY93Funi3we8nU6f4jz9GUL5P05aeOV/Vh8yNvwRPLijqq2XX+ov2tj6KvS7MUFCZV1Qj7QW3wZQdM4ztzrAIfDQSiHw0F8fHz8ctrgw1Q2mNATfhN+fn7we128P2Xy+6aCzUSgC28dpP6p//P6Z59L3peu93Fd5TZ6Z993C35+fi443FuKjhWVQO5F5GDpGpiwgnUT6p8QQgiAyFkCHu3NW/DfszNASCXqHDvx7EyQp0H9l6BjJTj0EUK6hTrfYfXsTLSEKz3+hBBCCCGEkHdAORwO9KkRQgghhJCb8/Hxv2dngUj8B4CHbV6Erh98ovyUn/J3V37SfljHCXk+3OpDCCGEEEJIB6DhTwghhBBCSAeg4U8IIYQQQkgHoOFPCCGEEEJIB6DhTwghhBBCSAcoGP4+TEWBIv+Z/kMy4puPS6ucCM5wCCcqueMMoSgKnpo9Qgh5EpEzxLCsc2x4n8SwnAghz6bE46/BDgWESP5Wj/lKpL56XFoXETmYraewjSel75ucdBBCruTUsfF4Z0u1c+U58ZCnk4xv6V/lpCj3XInua8ZJ35SdmSY4lL4IPqAoqLdtkmfSP1nvzjB/TzGrww2d8uh/HUfD+F+VZlt9fBPK0EFc5nKn68NUTDiJR7zYcCPpejbAxOFNc5h7vugJKQ/rwxw6cEy5EWerFI3CF/Ib34rgDHuwggBWT8nLOltj+m+OP5eV603wTQXKZgLvWZMOQkhreVlnC+kIPszRPnM0hjZgzU6N+sjBcAR4qTPSG8DqZQZ83TjpmwpG8DJHpliBNf75+CagbFBv20RI9A4IAXFf98oAACAASURBVAgPsHrITdwML7knALHKrpsjwA6T6yEAC5UTvkviSOtm3b13oMTwT4xfeWatr+ANLMycCJEzw3r6D3M1fd7FGv+SRuVhkDbcxFMeJg3Ow0ia2QVwsYAQArssooy6sIGFdT+MrxsuRsoS/TDtND5j5dam7cL6WiQdiAF3ZMKHivkuhK0lqx27OVSgRNbH0uWB2TdLJnKJEqsnlO3x7FD+bstfLU+ds6W4VTN10JQ7NvLOlqqwt8p7WR6aO3LycZfL8ntHUb0Dq+iVzsaSMifWLcux5fgbuMYiG1/VORZGgPU2X2DRdo3AmGQGuz6BgT2+k8eqx0kfG9eA19Ex9JXRV3kju4xoCwQGJL0DBnDU+13xAdeAVDeBhQGst2fuvQlntvrsjsLpKw8Dq4eeNcAiZwkb0m8dEyPAV5g01sBCL+kARy6wP2pMg/23ujHWh83S6/U1IO041DGmWtwZnAt/7Aj0CSonnJGD2Yms5FHoEwPBentcedmuEdeZqkmd/wlr0B7PDuXvtvz18lQ4W6BjdXw+8YzOHEQVjo08VWFvlfeKPDRx5OQoiedaR1FdmUYOhgWv9H4kG/NFJ9YNy7HlRN/7k2u9vnZyTf0zANyNpKse+lpsZ5xJAHttj+UTziyS61H/AHDznvq+hpze3VG23UZW7SoE1r3kXg9YCFSOB2VxRN+nz/X65++9C83f6hN947SZnsGQO/8K7/49wt4ifPiFAC5G0uTBHdXsQSS3RZ/ACNbYRgCiLdaYYqzWTOr0CQx31AJPbwLlp/yV8pQ7W4CC53vkXpRk07BRlWe8Ud6rZal05JxJ71pHUfG5XJmGXwhqvdKnTqxrdEBK0FfJpC014HuwAg39XoOwwQCLox3gwXBHPC/3LujxVqCRtJfeCnDU+3wnbdHxYgP+uP67BQIAXghoAEZmeRJ1cbSZhoZ/BGdmYeAJhPYeo1zLcbFMO+PIwdKNG6Q6nkLLzdKbc03YW4QHAOir3MTBMwDD+8UEgvwSHX9tYL2NEG3XwHSceSlLJ3Wpp22CTSu2elD+rst/sTzJKuVxL3Ro49R/en1Ydb4741C5rS7Op4frHT0XMvhTEf81OiAIv4LS6/pKWkURHgwMUKWCanRMeF7urdBXkmEu4q0+pXqXtwH5QG8NhALQVWAnAHt/5hBxMY4Swq/qoHX3XpEze/zjpbHImcEaeFjpgDpfFGbNBqaYxc/2LAy8ZHuQOsc/ey/N0i/Y63hN2F+HVzGeonC4lzwTdTwF1p/4XAPTsXq8Vj+p07ESIWzNxea9LT/K33H5Y8rkKXe2IPxCoPWROkL9TwvlZlQJ14S9KO+35yaOnqoy7fWhucts/JDvlXGXcmwnsd5OyzZt61X45giuvOe/OgFM5bp3TnfkpfHNeG99qd59wEXFpABAxXyyMg51DGiudGA3ApYuMB3X33sbDoeD+D2eMGAI74oYSEZRF54BAch/mrDDJ2XuAZzWxVDYGgQ0W8hih7Z2Uib5axAw3q9WUn7Kn1ItT9zn2tJ9WdRcn2EYwpDK7hhnci20NaFJHcq5sFpN5yPfr9NFPg/58SO0NenZUNhadX9XJstpP9k0/voyFZ6Rkye7V57Ha8qx7Zy08cqy9YSR/g5todW07dpxshD2DbuFlpBbkCvRGYQdH8URtgZheBAiREHv+TiMQnivJn7Nzu4d4z8Th/BQqJsN773Bn3I4HMTHx8cvpw0+TGWDybsfpHsRfn5+8HtdvD9l8vumgs1EoAsvZqD8lP98+2efe3tYpo+i62Ncd1EaP2kqwKTmMO613Dv+d6D54d5SdKzYWZJ7ETlYugYmXa1glL/b8hNCSIeIHADe/Yzye8f/Llzp8Se3pOveEMpP+Sl/d+Un7Yd1vKs09/iT+3Olx58QQgghhBDyDiiHw0E8OxOEEEIIIaR9fHz879lZIBL/AeDS24vQ9WVQyk/5KX935Sfth3W8m/z8HKj3F+Hn54dbfQghhBBCCOkCNPwJIYQQQgjpADT8CSGEEEII6QA0/AkhhBBCCOkANPwJIYQQQgjpAAXD34epKFDkP9N/SEZ883FplRPBGQ7hRPLv55QFIYREzhDDrEMiFZwrJ5YjIYRklHj8NdihgBDJ3+oxHzfWV49L6xIM7/FlccQ3s0nH0AGHLkJInqLD4tnxvCtdl/9JyGOcolRP0HLP5fUUOcPKe3JY+u1ejFK91Dtcz+r67HPnHbq+Kd830cZq02yrj29KhqfcQfowFROOVMhyw80V/rFw4/CmOcw9X/TKlIf1YQ4dOKaslGyVolH4Qn7jWxGcYQ9WEMDqNTGyiysj9xgwfJjLPkIhIEQIGxZmHJUIIYS0Ah/maJ85GkMbsGanY2nkYDgCvNQB5w1g9VKDzMfnepqMkwKhDVifmanmmwqUzQSe8UCxyFnO6aXc4Vqvayn2s89VOXR9U8EIXnZPrPB67ujrKTH8E+NXNmj1FbxBbHhGzgzr6T/M1fR5F2v8SwrJwyBtuJGDmVT4HkbSzC6AiwWEENhlEWXUhQ0srPthfN1wMVKW6Idpp/EZdwa1abuwvhZJB2LAHZnwoWK+C2FryWrHbo40V+5IKUwSAEDHSkgVxxvAmt3aI69jdcyHivFUu2nsr45vlkzkEgVUTyjbM1PvuvzVk/T0dt5T2H5vXpl+yxwWzZ0j+bhPHR/XOV/qnULV+itzDN3S0XLPciQX4W/gGovMllDnWBgB1tt82UbbNQJjkhlg+gQG9viOgPw4CYRfAbR+7xj2VXcSdJ3f6aVe15c/V8THxjXgdaC+nNnqszs2Sn3lYWD10LMGWOSMdUP6rWNiBPgKk8YaWOglHeXIBfbfUZbG3+rCrQ+bpdfra0DacahjTLW4MzgX/qhYfYJqR4CK+U427tNJQpJHeVAcuZWx3IYI23WAwZ+SSVJL0ScGgvX2uMq0XSOuM1WTOv8T1qA9M/Wuyx9TNklH4gHMewr3o5Zv0yjVb4XDoolzJEdJPNc6XwDUOYXq9Vd0DN3Q0XLXciSXEH3vT671+qcOLvXPAHA3Unn30NdiOwPIj8UjeOXORPJWlDtcm+v63HOl8Uff2Gt7LDtwrrP5W32ib5w20zMYcgdb4d2/R9hbhC8iexkiBzNrkC09hjbu6Y+PnBmsgYcOTEQz9AmMYI1tBCDaYo0pxmrNpE6fwHBHLfB0J3RdfgCVk/TwC0EDT2GruEi/DZwjVZ74hGudL8XnZKfQef2dOoaaOlrOyXXrciQPQF8lE6/UIOvBCjSkTlx1vjuO82F/yfNwb029w7Wprqufq48fwQCLo93owXBHrVxNbmj4R3BmFgaeQGjvMcqVhItl2sFGDpZu3CDV8RRabpbenGvC3iJ8Kf4GLgb4oyIeuLQ+0sUj/9NCcMu0JCJniN56irBTVj8A6PhrA+tthGi7Bqbj49Jd+aQu9QpOsGnFVpeuy3857V4Ru61+5YGx0ilya+fJGSr1d4Gj5bxcbCevTPhVPpLqK8lYEx6MdCwuoM4XmcOEvD+5bV15muq69rma+GNnxS/y/Aac2eMfL3XIHmd1vijMggxMMYuf7VkYeMn2IHWOf/ZemqVfsBR/Tdhfh1cxniK/x1Ne8hkBXrp9IjnzcPSGwajZMvR7fFNB72uRO3PQJdTxFFh/4nMNTMfq8Vr9pE7HSoSwNRebNx/Ruy5/Jb0+NHeZtWnJ4dB+HqPf2zhPyp1CF+vvLo6WDrSTFyeuY6f1IO3rqvDNEdx0z79v5rdjyA468v7I+myq60vqhHxPHWMq9wdtHlcOh4P4PZ4wYAjvihhIRk4XoS00QCD31+6yPq2LobA1CGi2COWrtiaViSbssHgNAsb7lRTll+Uv9i2F356Rk/cNxT2hqP/Q1oRmh8f/q/R7vKfZIiyUU2hr0rOhsLW4vpSRj6e8nhX1UB1//JwtxZHTUaX+yvPoGbLshjCkNiGXU5VcjyxHUs1JH1dZDzxhpL+LY2GuIiV9ZMUYmas3uXpMHklR79V6ifVers86Xcf34qpR91xd/OKkrrVhXClyOByEcjgcxMfHxy+nDT5MZYNJKw4SPp+fnx/8XhfvT5n8vqlgMxGdON9A+Vn/2yM/xwZySrvqOGnK/fXO/qYpPz8/FxzuLUXHigVN7kXkYOkamHS1gnVdfkIIIeQMkbMEPNqiTfnv2RkgpBJ1jp14diaeSNflJ2+MjpXgMEwIuT/qfIfVszPxRlzp8SeEEEIIIYS8A8rhcKBPkRBCCCGE3JyPj/89OwtE4j8APGzzInT94BPlp/yUv7vyk/bDOk7I8+FWH0IIIYQQQjoADX9CCCGEEEI6AA1/QgghhBBCOgANf0IIIYQQQjoADf9f4cNUTPjPzgYhhBBCCCENKRj+PkxFgSL/mY8xb33zcWmVE8EZDuFE8u/nlAUhhETOEMOsQ2pdeu8Ky4kQ8s6UePw12KGAEMnf6jFfX9RXj0vrEgzv8WVxxDezScfQAYcaQkieosPiXdJ7dL7JSyKPcYpSPaHKPVeoNzVxRM6wOhx5Lj6gKECtPzV5Jv2T9ecM8/cUsyLcEOW2UyHu9M+vuD90qsPm7r0Bzbb6+KZkeModdrzlxZEaV2WjO2o3Dm+aw9zzRS9KeVgf5tCBY6YN2YQvrVI0Cl/Ib3wrgjPswQoCWL3mRvZ3hdy3wYe57CMUAkKEsGFhxl6LEEJIK/BhjvaZozG0AWt2apxHDoYjwEsdcN4AVi/dalsXh4/P9TQZQwVCG7A+uWr/CvgmoGwAz6h5KEKid0AIQHiA1UNui7XhJfcEIFbZdXMJhMl1G8CszDDXpbBJ/DCA1L1rjgA7TO6FAKws7bJ772SelRj+ifErz5D1FbxBbHhGzgzr6T/M1fR5F2v8S7ziHgZpo4sczKRG52EkzewCuFhACIFdFlFGXdjAwrofxtcNFyNliX6YNvjPWDG1abuwvhZJB2LAHZnwoWK+C2FryWrHbo40V+5IKUwSSuSu6rCuQsfqmA8V46l2y8hfHt8smcglCqieUMpbs977DEbX5a+epKe3816+9u/CK9NvmcOiuXPkeemVO1quc/bUO6Gq60uZI6q45ZWe4rvgb+Aai8yWUOdYGAHW23xhR9s1AmNyNMigT2Bgj+/oXBzyGAqEXwG0fu/eUpEG6Ku8oV5GtAUCyRCHDhhArPczrHY46n08bZYnZwkYkwYP+oBrQKpzwMIA1ttm6bwCZ7b67I7C6SsPA6uHnjXAImesG9JvHRMjwFeYNNbAQi/pPEcusD9qTIP9t3rbTH3YLL1eXwPSRq+OMdXizuBceC/dsqNPUD3hVDHfSdt8jpOEErmTzuYrrIzsSiJs1wEGf0omSS1FnxgI1tvjKtN2jbjOVE3q/E9YAy/Tl1jh9TaONafr8seUTdKReADzXr79qOXGWal+KxwWTZwjT02vJJ5rnT0A6pxQ9fWl6IjSsRJy3z+ANeNWy1sTfe9PrvX6pw4u9c8AcDeSnnvoa4mdcSYOeTI5glfuaCQvifoHgJv38Pc15Owsd5Rtt6ly/mzXwODPmcR8wAIgm6WrEFj3kvh7wELEk5Do+zR4r99IpJeh+Vt9om+cNrEzGPLAUeHdv0fYW4QvInsZHkzkzGANPLzgEYj7oU9gBGtsIwDRFmtMMVZrJnX6BIY7aoGnO6Hr8gOonKSHXwgaeApbxUX6beAcObdN8cHpXevsKT4nO6HO15dTR1Ru9WHkni0Bckf0VTLhS1dgerACDU2c9+p8d7QBwv6SZ+XeCT3eCjSS9tJbAY56n+/y23TcEU76qsgBrAHO2k7OEjAW2SoBkKw4APBCQAMwMisCvyENDf8IzszCwBMI7T1GuamVi2XakUcOlm7cINXxFFpult6ca8LeInwp/gYuBsic7uVy35rIGaK3niLslNUPADr+2sB6GyHaroHpOGuUpZO61Es3waYVW126Lv/ltHtF7Lb6lQ2icqfIo9PD7Z01Z6isL5GDmTXI9pSHNrq10fJ5hF9B6XV9Ja3ACA9GbixuFoc6X2TOFPIW6Kv8PnwDKNd7yTagyAF6ayA8s6UIfjyhmOj5a711fE5AV4GdAOx99apC+HWRWE/nzB7/eK+l7HFW5wsYrrxn3sAUs/jZnoWBl2wPUuf4Z++lWfoFS/HXhP11eBXjKfJ7V+V9niPAy22fyMsNWz73cBt8U0Hva5E7c9Al1PEUWH/icw1Mx+rxWv2kTsdKhLA1F5s3t3y7Ln8lvT40d5m16TtOvF+PR+v3MendxllT4Yy5tL6EXwi0PtLb/qeFclOSXEOs81O9pH1dFb45gpvs+a+Nwzfzr+A+cd6Rd8I34731pS5QH3CRTQp8E+h9AWKHs7aTvwE0uyJeiXQ+qY4BzZUO80bA0gWm46aSvACHw0H8Hk8YMIR3RQwkI6eL0BYaIJD7a3dZn9bFUNgaBDRbhPJVW5PKRBN2WLwGAeP9Soryy/IX+5bCb8/IyfuG4p5Q1H9oa0Kzw+P/Vfo93tNsERbKKbQ16dlQ2FpcX8p4RnpZPOX1uqj36vjj52wpjlydqKwv5Xn0DFl2QxhSG5TLiVzGSR9XqRdPGOnv4lhYbOy1uu3O+Pna5Bbz8u0r+bPjYzzC1iAMD0KEKOg9H4dRCH9883oxXOH+MX7pWa+Qv7I8anZu0alQ507Dv/KfcjgcxMfHxy+nDT5MZYNJKw4SPp+fnx/8XhfvT5n8vqlgMxGdON9A+Vn/uyz/dXAsegdYx7uK0vhJUwEm4rwH/rfcO/53oPnh3lJ0rNjRknsROVi6Rn7vXZfouvyEEEI6Q+QA8O5nlN87/nfhv2dngJBK1Dl24tmZeCJdl5+QxuhYia4P54S8N+ocOHcW95Xjfxeu9PgTQgghhBBC3gHlcDjQp0gIIYQQQm7Ox8f/np0FIvEfAB62eRG6fvCJ8lN+yt9d+Un7YR3vJj8/B+r9Rfj5+eFWH0IIIYQQQroADX9CCCGEEEI6AA1/QgghhBBCOgANf0IIIYQQQjoADX9CCCGEEEI6QMHw92EqChT5z/QfkhHffFxa5URwhkM4kfz7OWVBCCHvSuQMMcw60ovvPzo/hBDSJUo8/hrsUECI5G/1mK8h6qvHpXUJhvf4sjjim9mkY+iAQxchXaHoiOgaj5a/6+X9JOQxTlGqJ2i550r0lNwv9c3V3SPPo0ovlXXiMmesb8rPmvDzN8+n3WKbq9lWH9+UCkHuIH2YignHGZY23Ei6nikoDm+aw9zzRa9MeVgf5tCBY8rKzFYpGoUv5De+FcEZ9mAFAaxeQ4Wf64iuxoe57CMUAkKEsGFhxlGJEEJIK/BhjvaZozG0AWt2OpZGDoYjwEsdcN4AVi8z5HxTgbKZwDNKUqi5R55HtV7K6sRnzmhv4oz1TQUjeNlzYgVduleZdkdsrhLDPzF+ZYNWX8EbxIUQOTOsp/8wV9PnXazxLylcD4O04UYOZutpUogCHkbS7CqAiwWEENhlEWXUhQ0srPthfN1wMVKW6IeFClKbtgvra5F0IAbckQkfKua7ELaWrHbs5khz5Y6UwiQBOKmcYocyMa5Dx+qYDxXjqXbrBF4a3yyZyCUKqJ5Q1szw34yuy189SU9v571C7fLmlTsirnOG1DtpqsuzzFFT3BJ6jeOjrN6Wyd/c6fNa6ZFK/A1cY5GNneocCyPAepsv22i7RmBMjoYb9AkM7PGdPFa3W+BVdxJ0nfvqxcfGNeBdXCe6Y3Od2eqTGbT6ysPA6qFnDbDIWbmG9FvHxAjwFSaNNbDQSzrKkQvs05YKDfbfaqXXh83S6/U1IO041DGmWtwZnAt/rBD6BNWOABXznTSzPE4SEHdY2hTjmxv7VUTYrgMM/jwswaejTwwE6+1xlWm7RlxnqiZ1/iesQfkM/x3puvwxZZN0JB7AvFdoP2rTNo0SR8S1zhAAdU6a+vIsOmp0rITcNw5gzX65LF5abyscMY3lfKH0SCXR9/7kWq9/amypfwaAu5HKu4e+FtsZpG3oWIVTrFPnc+8Li8JYVu6MlYi+sdf2WF51PrPdNlfzt/pE3zhtpmcw5A62wrt/j7C3CF+k4GV4JJEzgzXw0CnHhT6BEayxjQBEW6wRT7QqJ3X6BIY7aoGnO6Hr8gOonKSHXwgaeArbxLXOkOJzspPmfHmeOmpyqw8jtzrfVSsMKRfV2wZOnwenRx6AvkomXqkh14MVaOj3np0xcg+i7RoBDHihDQ0uRkejvcYZWyQYYHG0/zwY7uiiVeG221wNDf8IzszCwBMI7b2kCABwsUw72MjB0o0bpDqeQsvN0ptzTdhbhC/F38DFAH9UAL0+tNQouzORM0RvPUXY1hpYiY6/NrDeRoi2a2A6Pm6/Kp/UpV7ICTat2OrSdfkvp63emSO3dmacobI8Iwcza5DtuQ5tVC2Kq/Pdmfzett4+Oj1yW8KvoPS6vpIMPuHBSMdi0i58M7Z3xAq6OsdOhLD3FUZ7Y2esjskFZzy6YHOd2eMfL5HIsx91vijMngxMMUuWZSwMvGR7kDrHP3svzdIvWIq/Juyvw6sYT5Hf4ykvFY0AL11yUuf4Z+P0LMSN8U0Fva9F7sxBl1DHU2D9ic81ME32VZ2f1OlYiRC25mLz5iN61+WvpNeH5i6zNic5HNrKbZwZ5U6ai8sz/EKg9ZHe9j8tlJtrl/DoetuBdvLixHX6tN5Nz+yh9c0RXHnPP2kxISrmgnlnrIw6xlRu1xeMD52xuQ6Hg/g9njBgCO+KGEhGThehLTRAIPfX7rI+rYuhsDUIaLYI5au2JpWJJuyweA0CxvuVFOWX5S/2LYXfnpGT9w3FPaGo/6NOE/2X6b1YLqGtSboPha3ln7OlOHJlVlmechzy43JdM4Qh1dHQ1oRWDFCQK71fV2/z8jeV8/npkWpO+rjKeucJI/1dHAsLjT1XF3Nto/4eeRxFvV+is6wvietEuT0Uj5VZE83XGbnKVKbdEZvrcDgI5XA4iI+Pj19OG3yYygaTVhwkfD4/Pz/4vS7enzL5fVPBZiJau9dOhvKz/t9PfvbV5Pl0vY13lfvrnf1bU35+fi443FuKjhULmtyLyMHSNTDpagXruvyEEELIGSJnCXi0RZvy37MzQEgl6hw78exMPJGuy09uiI6V4LBICGkf6nyH1bMz8UZc6fEnhBBCCCGEvAPK4XCgT5EQQgghhJCW8x8AHrZ5Ebp+8InyU37K3135SfthHe8m1PvrcIPDvYQQQgghhJB3gIY/IYQQQgghHYCGPyGEEEIIIR2Ahj8hhBBCCCEdgIY/IYQQQgghHaBg+PswFQWK/Gf6D8mIbz4urXIiOMMhnCh/1Tfl8jDxzBwSQrpD5AwxLHZIHeGc7F0uG0IIuYYSj78GOxQQIvlbPeZrj/rqcWk1xTcVjOBlZSEe/Elo38wmHUMHHOYIIXnKHRbP5zRfj3fuvGrZkCPyGKco1ZO53HMlOk3un1Svc+HIU4ic4e/0WfLMad2J4AzrHdhdd+g22+rjm5LhKXemPkzFhCMpUW64OeUeCz4Ob5rD3PNFD055WB/m0IFjygrLVikahS/kN74VwRn2YAUBrF5qZPvYuAa80slIcWXkHh2KD3PZRygEhAhhw8KMvRYh5E15RecOeSY+zNE+czSGNmDNTsfSyMFwBHipA84bwOplxppvKlA2E3jGZeHIs/DxuZ4mto1AaAPWZ6aVSn0W4jitO5853RpeuQP76Q7dF6DE8E+MX9mg1VfwBrHhGTkzrKf/MFfT512s8S8pQA+DtOFGDmaScj2MpNlbABcLCCGwyyLKqAsbWFj3w/i64WKkLNEPC4qvTduF9bVIOgID7siEDxXzXQhbS1Y7dnOo0Tf22h7L0lmjjpWQKpU3gDW7tUdex2o3R1w6KsZT7aaxvzq+WTKRS8q/ekLZpll81SQ1vZ33djx1l9wdoP6LlMlX7rBo6hw5Hz9Q79ypcoCU5avo3Lml8+TeZUPugr+BaywyW0KdY2EEWG/zeoi2awTGJDPO9AkM7PGdPFY1oTwXjjwL2bYBwq8AWr+X3b2rg6DOodsdzmz12R0bpb7yMLB66FkDLHLGuiH91jExAnyFSaMLLPSSTnXkAvtji9Ng/60u+PqwWXq9vgakHYc6xlSLG/W58Eel6xPUTiqDARZHA9+D4WYTiJzxMXLrYrkBEbbrAIM/JZOklqJPDATr7XGVabtGXGeqJnX+J6xB22bxZZNUJJ6svLdjP2rXMjb1X6BUvhKHBdDMOdIo/pQK506lA6QiXzlu6Dy5d9mQuxB970+u9fqnDi71zwBwN5JueuhrsZ1Rx2/Dkfsj208jeOUO4Fp0rMIp1qmTuveFRaHPd0cljrFah253aP5Wn+gbp830DIbcGVd49+8R9hbhT9AxSWcJkYOZNciWEEMb9/THR84M1sBDpyap+gRGsMY2AhBtscYUY7VmUqdPYLijlnl6Kyap4ReCBp6yt4b6z3ORfA2cI0UPfm385c4d4DoHSNOwJ3ktcuOyIS+Gvkomaamx1oMVaJCcxLcNR+6OOt8dbbOwv/zVGcZou0YAA15oQ4OL0dGAVzHfyU4FyWkG1Dp0u0JDwz+CM7Mw8ARCey8VMAC4WKadceRg6cYNSx1PoeVm2825JuwtwieRYKq52BxniplsCL8QaH2k/Yf/aSG4Jq0aImeI3nqKsFNWPwDo+GsD622EaLsGpuPMa1g6qUs9iBNsWrnV4zztWhGi/vPcVj554P11+V3jALkg7Glei7Rd990h/CofSfWVZMgJDwYGaNLd/TYceRzqfJE5eZrim7FdJFbQ1Tl2IoS9rzDga7d4SQ7dDnFmj3+8DCJ7nNX5ojBDMjDFLFlusTDwku1B6hz/7L00275gK8I1YX8dXsV4Cmk/qIr5Pxv7dLlIli0583D0OsKo3zL0S3xTQe9rUbFU3n7U8RRYf+JzDUzH6vFa/aROx0qEsOVJW9voDKp0owAAIABJREFU9aG5y6xOy5PSFkH9l3Fv+criL3fuXOUAuYvzpO26bxdxWz7tx9K2XoVvjuDKe/cb8ttw5Mb4Zn57jb+Be/WELETFnDEff51Dt0scDgfxezxhwBDeFTGQjJwuQltogEDur91lfVoXQ2FrENBsEcpXbU0qE03YYfEaBIz3K6m8/MW2VfjtGTl531DcE6j/vPyhrQnNDo//V8l3vKfZIizUk9DWpGdDYWtxeRWpjj+Oz5buy0XrGXIYQxiSrvL5ysvTJKxWltEnlA25HSdtvLIf84SR/i6OhYW2natHUp9wLhx5HHm9J/16hV1Tqc8kXKrG4nNZfxHXnUq7qVAvulYtDoeDUA6Hg/j4+PjltMGHqWwwadtBuifx8/OD3+vi/SmT3zcVbCaiE+cbqH/q//X0zz6e3I7XrOPk3txG7+yLbsHPz88Fh3tL0bGiEsi9iBwsXQMTVrBuQv0TQggBEDlLwKO9eQv+e3YGCKlEnWMnnp0J8jSo/xdAx0pwqCWEPBd1vsPq2ZloCVd6/AkhhBBCCCHvgHI4HOhTI4QQQgghpOX8B4CHbV6Erh98ovyUn/J3V37SfljHuwn1/jrc4HAvIYQQQggh5B2g4U8IIYQQQkgHoOFPCCGEEEJIB6DhTwghhBBCSAeg4f8rfJiKCf/Z2SCEEEIIIaQhBcPfh6koUOQ/8zHmrW8+Lq1yIjjDIZwof9U35fKgsU8IeQyRM8Sw2CG1KL1XosuyE0K6RYnHX4MdCgiR/K0e89VGffW4tJrimwpG8LKyEA/+XLRvZpOOoQMOS4SQPOUOi/ak15TTfD3fmUTOIo9xilI9+co9V1L/kvsn6uYY+rpcqrNCXal1xtbpva7ONa2Pb06zrT6+KRWe3MHGW14cZ1haUJF0PeuA4/CmOcw9X/S4lIf1YQ4dOKas9GyVolH4Qn7jWxGcYQ9WEMDqpRXFx8Y14NVMRr4r5L4NPsxlH6EQECKEDQuzllZCQgi5Na/oTCIyPszRPnM0hjZgzU6N+sjBcAR4qQPOG8DqZQafbypQNhN4Rkn8HENfkl/pTF9JTlgB4RmAMSlxxtbpvazOfSZ1qWF9bAElhn9i/Moza30FbxAXXuTMsJ7+w1xNn3exxr9EGR4GaUFFDmbraVL4Ah5G0swugIsFhBDYZRFl1IUNLKz7YXzdcDFSluiHBQXWpu3C+locK447MuFDxXwXwtaS1Y7dHGr0jb22x7Jy25Mk910qiI7Vbo64dFSMp9otI395fLNkIpeUf/WEsk3bsqomqentvGeibY5N6r9ImXzlDoumzpHnplelrzpnUnErauqAKstX0ZlUFZY8DX8D11hktoQ6x8IIsN7mFRNt1whkA0+fwMAe30d7sGqC1+0x9JW5XmcRnKULY3JDvTesj23gzFaf3bEQ9JWHgdVDzxpgkTPWDem3jokR4CtMGmtgoZd0tCMX2KctFRrsv9WemPqwWXq9vgakilLHmGpxZ3Au/NGLr09wMuGUCQZYHGeYHgxXnkBIcicV5Cusi+waImzXAQZ/SiZJLUWfGAjW2+Mq03aNuM5UTer8T1iDJ27Lugtlk1QkHrC8Z2I/apchQ/0XKJWvxGEBNHOOPDu9Wn1VOJOgY5Xz+A1gzRxEVfnKURWWPIvoe39yrdc/NdLUPwPA3Uj1qIe+dul4270x9P2p0Zn/CQs2aszIijh0rMIp1qlzu/eFRdL3NK2PbaD5W32ib5wWyxkMuWOv8O7fI+wtwp+gY1I7S7gfkTODNfDQqVVrfQIjWGMbAYi2WGOKsVozqdMnMNxRyzy9FZPU8AtB2z0T1H+ei+Rr4Bw5t03x3unVxl/uTAIKqz0j92zOZK4JS56IvkomlOlqTQ9WoKHfax5FJ8fQN6daZ4m3f1E2wT8fR7RdI4ABL7ShwcWobcvlDWho+EdwZhYGnkBo7wsF5WKZDhyRg6UbN0h1PIWWm6U355qwtwifRIKp5mJz3EmQyRZTLvetiZwheuspws71WDr+2sB6GyHaroHpOGvkpZO61KM3waaVWz3O0y5vFvWf57byqfPdGafIvdP7RfyRg5k1yPZ6hzYa++OuCUseRvgVlF7XV9JqjfBgYICm3V13x9D3pVZn/ieswEDpLp9zcfhmfE2soKtz7EQIez+q3CpbVR/fnTN7/OM9tPKsSZ0vTra8TDFLlk0sDLxke5A6xz97L83SL9iKcE3YX4dXMZ5C2iOqYv7Pxn6knMpWIjds+dzDbfBNBb2vRcXSdftRx1Ng/YnPNTAdq8dr9ZM6HSsRwpYnbW2j14fmLrM6fceJ5zOh/st4tHz3Tq8s/gqnSviFQOsjreb+p4XGw/I1YcldiNvyaT+WtvUqfHMEt/RQZ9mz3R5D35FzOvM3LjT7b63+m+s9RGrb/7Y+viWHw0H8Hk8YMIR3RQwkI6eL0BYaIJD7a3dZn9bFUNgaBDRbhPJVW5PKRBN2WLwGAeP9Siovf7FtFX57Rk7eNxT3BOo/L39oa0Kzw+P/VfId72m2CAv1JLQ16dlQ2FpcXmU8Mr3q+OP4bOm+rErPkMMYwpDqRj5feXmahNWqCobcjJM2XtmPecJIfxfHwkLbzulV6hO6OIa+KkW9/1pnoS20Uh3GY0VpfSnEUUw71+5bOK4WORwOQjkcDuLj4+OX0wYfprLBpG0H6Z7Ez88Pfq+L96dMft9UsJmITuzNpP6p/y7rP4ZjSpthHe8m99c7+42m/Pz8XHC4txQdKxY0uReRg6V7fi8faSnUPyGEkDNEzhLwaIs25b9nZ4CQStQ5duLZmSBPg/rvIDpWgsM3IaQ56nyH1bMz8UZc6fEnhBBCCCGEvAPK4XCgT40QQgghhJCW8x8AHrZ5Ebp+8InyU37K3135SfthHe8m1PvrcIPDvYQQQgghhJB3gIY/IYQQQgghHYCGPyGEEEIIIR2Ahj8hhBBCCCEdgIY/IYQQQgghHaBg+PswFQWK/Gf6D8mIbz4urXIiOMMhnCh/1Tfl8jDxzBwSQsgziJwhhsXO8YL7j84PIYSQcko8/hrsUECI5G/1mK8o6qvHpdUU31QwgpeVhXjwJ6F9M5t0DB1wmCOE/I5Tx8bjnS3lzpX2pEcuRh7jFKV6Mpd7rqDTBuNk5AyhKAqe6lskR1J9/Faf2aM1jtkknhOd065quNXHN6UCkjtTH6ZiwpGUKDfcnHKPpR+HN81h7vmiB6c8rA9z6MAxZUVnqxSNwhfyG9+K4Ax7sIIAVi+tDD42rgGvajJS1xHdBB/mso9QCAgRwoaFGUcwQsiNeEVnC+kSPszRPnM0hjZgzU7H0sjBcAR4qQPOG8DqpUZeg3EycjBbT2Ebj5GKnMPH53qa6EwgtAHrU7LRGto9dY5Z31SgbCbwTnROuwoAcDgcRIYnDEDg+KcJO0zuGBCaHYrQ1oSWXkyez/9OwoS20DRbHO8YEIYnhBChsDUIxD+O5OKtDJtPzzOkPIa20GAIr0H4Y9qeIZCGEaGwtUzeOA5NaHJ5HPMsyXlD8rrIky/3diLLn9a3lNDWjuUf2lqJTpJ6ddRXqtf34bQtGsKWZM01Gc+QZC3ce1Oof1n/VfKc1otif3zafxfiSvrGsr68rO8XJ8+ekr9flveyPHjC0GxhG/JzWT5eKT1yG3J13DNO7IBiuxci3/aTpyrH39N6mo3rmR1AHk2dbVOm85TqfifuB8+p85zOu2BXFTkcDuLMVp8d5mp8VV95GFg99KwBFulFAIAh/dYxMQJ8hUC0XSMILPQSr/jIBfbfUZbG32pPU33YLL1eXwOMRZxHdYyptsd3dD780YuvT1DrBAgGWBxnkx4MdxSvEPgbuNoUY7Uu8C2JsF0HGPx5WIJPR58YCNbb4yrTdo24ziTem9Rb4CHVySeswRO3Zd0FF9bXIvFyGXBHiZcrcjAseMr2o3Ztaei8/mvlcbHGv2O/NDh6SXWsjs8nntGZgwgq5rsQtpb07bs5TnuSqrC3yntFHgIL634Y69JwMVKW6Iep9/ez2ZmqR6dHbkL0vT+51utrJ9fUPwPA3Ui66aGvxXZGIcaTcTJyZlhP/2HenaHzLZB3ZIzgYVeqoBq7J/rGXttjedV51O7ZVSnN3+oTfeO0mZ7BkDtjUaHcO4S9RfgTdEyetFQYOTNYAw+dWpXXJzCCNbYRgGiLNeKJVuWkTp/AcEctO4BdMUkNvxCkE14AUOdYGAHW21ZZ/t3Wf6085c4WoLDFceRelGTTsFHF1s5mea+WpdKR8+D0yIuhr5JJWmrk9WAFGvq9/GMn42TkYHbiqCSvgDrfHW2zsL8s3Wt/1u6pcsw2pJN2VUJDwz+CM7Mw8ARCe49RrnRdLNPOOHKwdOMGqY6n0HKz9OZcE/YW4ZNIMNVcbI5HEzLZ0OtDS42SOxM5Q/TWU4Sdq506/trAehsh2q6B6TjzUpZO6lKP5QSbjr6BqV2ei67r/xfyJIbOcS90aOPUf3p9WHnQLneo3FYXj06PPI/wKyi9rq+k1SjhwcAAcndXOk6GXwiQTRhGLuCOag4Qk6egzheZkyfhcrvnMsdsd+2qmBLDPzngKi2fyDMjdb4ozKwMTDGLn+1ZGHjJ9iB1jn/2XpqlX7AV4Zqwvw6vYjyFdLhXxfyfjf1IqZANUjndZ5uFbyrofS0qlubbjzqeAutPfK6BabKv6vykTsdKhLDlSVvb6PWhucuszsmT0hZB/QPl8pQ7WxB+IdD6SKuB/2mh3Iwq4ZqwF+X9nrRN9+0mbsun/dj0zB5a3xz9f3tnj6Sq04Xxh6r/PuYN9AaWK8AV4CRGpmZNKMnNDM0mgVCymxqZDKxgWMHUBAOBs5J+A1C6+RLHb3l+VVaNwOnu05+nD6cd+GKiHeSsXCetleYgCAQggnO8/ScnEdp6WE64ga9s5FrZPU2O2YPZd9uuAlA83Hss7Q5YkHZobRG7+sHiBz2weAzlvpgd0FMOaktZONyZHfLSr5UPjz8CVYd7g7rvT364N6W77V+vT/Oh70CoMkIKpe72aVYe7j0s2/awbVNb6GXQ+7R+iLPwYws3zo+ch9IYr53H0kPXIpDltVDt8Eeskzzcezva/XCBPNCeqVw+ZPVna+dB9ccKOmhXFdlut9LYbrfy5eXll9uGELaxweTRD9LdCT8/P/h9Wzw+VfqHtoHNRHYiDo/tz/Y/3P6cc8nj0vU5rqucp905952Dn5+fIw73VmJhxUYglyLxsPQFJuxg3YTtTwghBEDiLYGA9uY5+O/WBSCklt4cH/LWhSA3g+1fgYWV5NJHCOkWvfkHVrcuxJNwosefEEIIIYQQ8ggY2+2WPjVCCCGEEHJ2Xl7+d+siEIX/APCwzZ3Q9YNP1J/6U//u6k+eH/ZxQm4PQ30IIYQQQgjpADT8CSGEEEII6QA0/AkhhBBCCOkANPwJIYQQQgjpADT8CSGEEEII6QAFwz+EbRgw1I8dXqUgoX29vKpJ4I1G8JJ9gfR6MAwYho1blpAQQm5B4o0w2k+Ox9+/dnlICuuJEFKkwuNvwo0lpMw+q+v8l0hrdb28WmGt8jqQEjIQgJhc999Fq5uPkQdO34SQ31FwbOAWzpZyGR4jv2uXu0MUHGy1mxTtuUJbNKyTiTeqlyO3JQQMA2icgrJndp9i+4W2fj+skRt5DVnUpdFURjX9ER7ONmsX6hPayoBSJ8EQtmHDUwaXOnC1QbevuVTetkfa80XPRLVsCHvkwbNVD3z+lqKVfKG86a0E3qgPJ4rg9KuM7ATe0oeYWEo66puAS0woIezlALGUkDKGCwczzlqEkDNxd84W0jFC2OPP3NEYu4AzK6+liYfRGAj2TrghnP7u7XvTOhnibT3N7knELuC88Z39PRDagLEBAtHwUIKs3QEpARkATj83zEMbGCO7l312s5k9Btw4ux4DcMqbhkNpNJXRXgJx9rwLYNawsbhLttutzAmkACT2H1O6cXZHQJpuLGPXlObuYva8/j2TiV1pmq7c3xGQIpBSyli6JiTSL3u0dGtl9fwCoZQxdqUJIYMW8vu8AyGxk5GxdM1cX41ASCjpHX2/JXpb6Oj1/pyo+u/6247YNfftFrtm3kf3/SjrV/u+u2vXx4H6U/+cOn0CKSCkq9RBcT4uz9+FtLK5qmour5r7ZenZMvr9qrJXlSGQwnSlK9Tn8nLcR371dVfug23Tb2pDma1LeZ75UpmuUUKYisxp7XZttD4eiJIdUBz3UupjP3sqtzMKNOlblTa5Fqj8pLZZ9b3YhYTQrwlAunH+d1CVblCWS9u+/GxtGi3LuCtnVdr3/DkQ6vOBeS+9aq0CDJ0++s4Qi91FAIBQvluYiAhfMZC8rxFFDvqZV3zsA5/fSZ7H33pPU7Nsnl9/YAJikZax94qp+Ynv5LB8sPNyWRM0bTiz0qTe/sUcqtbaG4WxfzCV00jwvo4w/NM7/OiTYE0EovX7/i3T+xppn0k8zBQvToBx+tYmfIMzVEfn6rphWWeG+ndb/2Z9fKzxb7/KDfdeUgsrbcUawpl5SNDD/COGa2Zz+4c+l6XUyZ6r7DVliBysB+lKHggfY2OJQbzz/r61O1N10fwq0qnrg0elX9OGiYdRwQv+OVbfKEfwsYCUEh/z3nnb7cok35+la/2BWbrW+zME/I1Sd30MzNTOKKRYWifVdXqMIKsz8gj0/gDw9dCbgYm03RPg0wSWSoiOYafPJN/ltPqDigwa0jiG9zUw/HO83C1p/6s+yTfKw/QAhW3SUYPuFNlzyO8I3+BEAhPVikg8zJxh/uoxdlGers5H4s3gDAN06q28NYGI1nhPACTvWGOK117Dps6aQPjj5zmATf2pf60+1c4W4DSHRFvZpCa0s13Z63WpdeTcWX6nOqaKz2ltGH8h2skAQG+OhYiwfq93ml3XCXUDrFW2idqF1vbhRCYGff2xqnWyN//Y2wDxYMmzco+ElYbZjBXD3ImQt3sELJQQHeEfOC9QxYlpJB7gDPFwtllLwz+BN3MwDCRi9xNjrWZ8LHeTY+Jh6acDsvc6hant0ttziuw55FXCjQ/T/at7D+MvROYAu/4XvjmIzpBXFYk3Qn89RfxoPetkLPx1gfV7guR9DUxfcy9l5aZu5/maYPMUv8BE/buu/9H6nOKQOEJWNaaqHSrnbYv7yw/ncyy1pPZt75WdUJcm/qpeSa2VGqsQQGAItUrarJO9+SJ3JpCHwFrp8fcCQN1QmDSEb8Rf7fJrSqNI4gH9NRCv2svcCxWGf3bAVTkYq+6ke/MFhK+82oTAFLP02b6DYZCFB/Xm+Od+Krv0Iw7AniL7a/keXqfQD/cmHpa+KIQ2IfVADBWPD0SLkKHjCW0D/a9Fzav556f3OgXWb3hbA9PX3v5a86bOwkrGcE0fm8e2/Kh/x/VPqdKn2tlykkPiIs6Ma7fFdfI7j2Oppg37A5j+Ml+v1HtVXNEJdW7SeizruhvrdYT2GL7yC3u162Ro679aFW7gFzYM5HEIbcAX2eHbHjA1kY/zBFj6yJzOgOkrh3mze9PXQoINabQpS/8LkB94TNus6UDpYdJDSo92iO5e0doidqWpHdp6zAOLx1Dui9nBusLBae1gXXbIS79WPjz+CFB/6r+jXp/ywVBV1UCoMkIKpe72aVYe7j0s2/awbVNb6GXQ1w/9EGfDjy3cKL9i3RX7YHE9rE+/uQ0PHe4tlvGUdrs2pTFeq2t6aFkEsrwWqpXVuE4+/oH/50E/XKr12eyzO7Trmtlh2hiFdi8cUi3c117CBSj0q/zePv0DadSWsViu7HPokPA9fYztditfXl5+tWlIf9Zyg8mjH6S7E35+fvD7tnh8qvQPbQObiXy4GLrfQP2p/+Hxzzn38eluG3Z9jesuRusnbQOYSFxsbFw6/Ueg/eHeSiysOjh5kSuRhVpNutrBqH+39SeEkA6ReACCyxnll07/UTjR40/OSde9IdSf+lP/7upPnh/28a7S3uNPLs+JHn9CCCGEEELII2Bst1t560IQQgghhJDn4+Xlf7cuAlH4DwBfvd0JXX8NSv2pP/Xvrv7k+WEfJ+T2MNSHEEIIIYSQDkDDnxBCCCGEkA5Aw58QQgghhJAOQMOfEEIIIYSQDkDD/1eEsA0b4a2LQQghhBBCSEsKhn8I2zBgqB/7OuZtaF8vr2oSeKMRvGRfIL0eDAMGjX1CyJVIvBFG+wmpWxzSvct1cwysJ0JIkQqPvwk3lpAy+6yu88+NrdX18mqFtcrrQErIQABict1/9axuPkYeOH0TQnQKDou7oVyu6zt3zlU391rHT0DBwVa7SdGeK7RF3TpJ5919EwKGATROCdkzu4/a7t5Iv2fY7eSa0h95LdMHENr6/UfqV+1CfUJbGVDqJJiGvHjeqHLgJsr1fMJP5W17pD1f9ExUy4awRx48Wx3E+VuKVvKF8qa3EnijPpwogtOvMrITeEsfYqKb/d81ep+HEPZygFhKSBnDhYMZVx5CyINyd84dcmNC2OPP3NEYu4AzKxtpiYfRGAj2TrghnP7OgG9YJ+/BeUcqCW3A2ACBaHgoQdbugJSADACnrxvYIsjuSUCu2svtsMeAG2fPxQCcFuln5R9DuSfxUP2qwvDPjF91Z22tEAzTAZV4M6yn/zDv7Z73sca/bHAFGO4GbuJhtp5mA1IiwFjZ2UXwsYCUEh95QjlNspGD9SBOrwsfY2OJQbybNN7SRmvM24fztdhPBP7YRoge5h8xXDN72/Exh1aq8A0OXPzVWlbRu27COgkLq305enidmudM/O4J7YqNXNaI9RvK5/HsUP9u61+mSr8qh0V758jh9IFm504xNHTnEKp2pOjOnTrZe6ubel1+75hqdpgVPdX52lXlNDtnPV6ZcANfLHJbojfHQkRYvxfcbu9rRKrBbk0g8InvBGi/TlY778htsFa6IV1F8g5EQjGoLUAAWbufX+4YNj4QHCj/XbPdbmVOIAVM6caygkAKQAJCBto19buUgYAUgZSxa0oA2sd0YyllLF2znEfsmtn9Jlk9v9g1JcT+2z7dtvL69+pypddTnWSlnK73KehtcagMz4emfyAkTFdmPSJvm9iV5v66Uu+BUPrCY0L9qb+KOifW61ect9J5eicXCEhAqbvCvLWnNn09vcY1oq7NqvRplD3wbPH+peummE5dH2ydfkOdxq40Uchr/z1dBxr7+ZH1eG3UPq6v3/m1UnkDUbA76tbDhnWyUC/k2qDyk46d6nsySG23QLmWtm/+t2rj7dM5IKd9YkhTSaMoU5l+DGmauhxEjQ53+mn/qz7JNz6P3VUUarrSu38J2XPI7wjf4EQCt3QUJN4MzjBAp96SWxOIaI33BEDyjjWmeO1l3p/IQT/zcI194PM7SZ/3x8/j6aX+3da/yFH6CSyy+a4/MIGdV7X3iqmZekqTore5Mf08PcDCRET4itNvmud77B+lUlvZUlmLXLpuiuWp64Mt0y8+p9Vp/IWo0QtuwtVfPZ/UBg+Btcre7u/eavThRCYGff2x+nUy8/YvCm/yyX1jpaFAYyWO3omwb/f5hxJqEwD+OAvTOSCnkrwDEYAgBkwAYyWOvzZ9AIiAhRLmI/wDZxXujJaGfwJv5mAYSMTuJ8aahj6Wu8kx8bD00wHZe53C9De/WoRPkT2HvEq48WG6fyvit6r1PjeJN0J/PUXcKasfACz8dYH1e4LkfQ1MX/NJu3JTZ2ElJaScYPMUoR7Uv9v6Fzmvfr35x+n1l3iYOcM89jp20Tog8QjZclmLXLpuKjiXY6klwz816Z/SBndI/BVVXrdWuitYYAi1ShrXyTtw3pHfYa30OHoBoHIoFMJ5WsmFQH8NxBKwesCHBNzPGgP+QLjQpOmswh1yIMY/jV9Ud9K9+QLCV2PmBaaYpc/2HQyDj8zDMcc/91PZpR8Re3iK7K/le3idQj/cm3hY+qpnRkXXG6567uE8hLaB/teifOagI/Rep8D6DW9rYPra219r3tRZWMkYrulj8+CWH/Xvtv7VXFq/qvRrnBzxFyJzgJ2/I3xzUG22VXCK7FFlPz/ncSzV1Gl/ANNf5uvVIafSRerxOqT1WNZ1N9brCO0xfCXm/9A6We+8I49EaAO+Gruv3QR8VG8KGuUK1Ow79fR7wNREPsckwNKvfqNwt9THlbehHOtOfo/WFrGrx5CVzlc8H+W+mMW0FmIz9TMcNec6HjDem/pTf5XGc0+Kfvt7pivjluegitSnn6bnKvfVqk3j2HcyQopCfHlernL89iHZtjH+l66bOl2KfbDtGbRDdZrGtKPiXnUZT6nHa1Ma47W6pucgRCDLa6FaWYfWycazG+R6lGP7UWg3Ny7E48fNcfSiIL+Pzz8gp8b7F8thui3Sr8ij9pzCnX6M7XYrX15efrltCGEbG0zkirvpM/Dz84Pft8XjU6V/aBvYTGQnzjdQf+p/f+Ofc/z56W6d3mcfJ5fHaP2kbQCTC/485qXTfwTaH+6txMKqg5MXuRJZqFVnYzOpf7f1J4SQDpF4AILLGeWXTv9RONHjT85J170h1J/6U//u6k+eH/bxrtLe408uz4kef0IIIYQQQsgjYGy3W3nrQhBCCCGEkOfj5eV/ty4CUfgPAF+93Qldfw1K/ak/9e+u/uT5YR/vJj8/W7b7nfDz88NQH0IIIYQQQroADX9CCCGEEEI6AA1/QgghhBBCOgANf0IIIYQQQjoADX9CCCGEEEI6QMHwD2EbBgz1Y4dXKUhoXy+vahJ4oxG8ZF8gvR4MA4Zh45YlJISQW5B4I4z2k+Px90kK65EQcmsqPP4m3FhCyuyzus4/N7ZW18urFdYqrwMpIQMBiMl1/9WzuvkYeeByQAj5HQXHBm7hbCmX4bbpPCpPpH/BwVa76dGe03VPvFHtPa6hd0zWNlVTUGg3OFwb5Fql39Tn2vbHB6ddqE9oK4NGnXRC2IYNTxl4akVpA3Jf+6m8bY+054uejmr5Hw4PAAAL3UlEQVTZEPbIg2erHSJ/S9FKvlDe9FYCb9SHE0Vw+lUTRAJv6UNMFLO/YSI6DyHs5QCxlJAyhgsHsyfthISQ63N3zhbSMULY48/c0Ri7gDMrr6WJh9EYCPZOuCGc/s4YDPG2nmbrpETsAs6bst5zDb1LQtuAsZkgENX3xggUx+tq73BtkmuXflWfe9v3pfp7z0WF4Z8Zv6pBa60QDNNBk3gzrKf/MO/tnvexxr+sgQIMdwM38TBTBmSAsbLziuBjASklPvKEcppkIwfrQZxeFz7GxhKDuNBIjXn7cL4Wey++P7YRoof5RwzXzN52fMyhlSp8gwMXf/drZKGDyA9UqXEaFlb7cvTwOjXPncFdE9oVG7msEes3lM8TlkX9u61/vT5NzpZiqObOIVHt2NCdLXWy5yp7VRnaO3L0tKt1+b2jqNmBVfQC5mtJlRPrUerxDgg38MUiXzt7cyxEhPV7we32vkakvm23JhD4xHcC6OskEH9FMAf93YOdXkPvmXqnQ4iNLxDUOCTaOivo1DjAdruVOYEUMKUbywoCKQAJCBlo19TvUgYCUgRSxq4pAWgf042llLF0zXIesWtm95tk9fxi15QQ+2/7dNvK69+ry5VeT3VSlJQwXVlZTSegt8WBMjwhmv5aHSttE7vSVOp+199kIJS+8JhQf+q/p1afdB429xNVw5xdV4cZ6pzbLHvg2eL92rIXy6DrEghIQGnnwtpSm05dn2idfkOdxq40Uchr/z2dlxv73V3X4/VR+7i+fufXSvUTiILdoa+H2npf2xbdWEPvlTrbJh+rGbErTdOUpmq/VTRaSa6GyudiV0m/MDaa7j0J2+1Wtv9Vn+Qbn8fuKkSgxclXevcvIXsO+R3hG5xIYHLDzWPizeAMA3RqA2tNIKI13hMAyTvWmOK1l3l/Igf9zJs19oHP7yR93h8/gac3g/pT/1p9BBb7+czCRET4itNvmud77B+VZVvZpM4z3qrs9br0Byaw8wD3XjE1U6/uofxq+0TL9IvPaXUafyFq9EqbcP/qE/O91uPDYq2yt/u7Nxp9OJGJnWO/N//Yr/PxYFkZy9/JNfRRiYZY7G23AMIfH47nP4LkfY0IAkHswoSPsZJ4071noqXhn8CbORgGErH7WagMH8vdpJV4WPrpgOy9TmH6m18twqfInkNeJdz4MN2/+qHe/gDmzii5MIk3Qn89Rdy5GcvCXxdYvydI3tfA9DUPv6rc1FlYSQkpJ9g8RagH9e+6/kfrk3iYOcM8Fjp20Tq44QhZ1dCqdqicty0O54fzOXpaMvxTk/4d1+MjEH9FldetlfJDGzKAwBBVTdCbL3KHQUZ319BnwMLkQDz/UYR22hfkClZvjg8Zw/3MNhZN956MAzH+abykulvuzReFHZjAFLP02b6DYZDFu/fm+Od+Krv0I2IdT5H9tXwPr1Poh3sTD0tf9QSp6aN8FuLMhLaB/teifOagI/Rep8D6DW9rYPra219r3tRZWMkYrulj8+ADlvp3W/+UKn2qnS2IvxCZA+winMM3B9VmVAWnyB5V9vNzHkdPTZ32BzD9ZT6/q/eqeOB6vDZpu5XrdjfW6wjtMfxdzH9o679MFW7gK5uCrq+hD0fvFVO1nx8abycTo2aveeDeg1MfV96Gcow/+T1aW2ixZs8dc7aj3BezGNrCeQr9DEfNuY4HDOak/tR/R70+6ZzrKvdVVdP47p2MkKIQXw7k9VmMpz4k2zY2vakt9DK0O7NVl19Rl2KfaHsm7FCdpjHmqLhXXcZHqsdrUxrjtXWbnlsQgSyvhXpQeDpHVK2RHVxD75Viu2tjRBuzstRutfObJqef4WhKv3ivdg4s3HsWttutNLbbrXx5efnltiGEbWwwUX5uifyen58f/L4tHp8q/UPbwGYiOxGbSf2p/+Hxzzn3/LBOr0XX17iucvl25xhuy8/PT9sY/zosrFjR5FJkoVa3PFh9U6h/t/UnhBBykMRbAgFt0bb8d+sCEFJLb44PeetC3BDq3239K7GwklzezgvrlJBHpjf/wOrWhXggTvT4E0IIIYQQQh4BY7vd0qdGCCGEEELIk/MfAB62uRO6fvCJ+lN/6t9d/cnzwz7eTdju98MZDvcSQgghhBBCHgEa/oQQQgghhHQAGv6EEEIIIYR0ABr+hBBCCCGEdAAa/mchhG3YCG9dDEIIIYQQQmooGP4hbMOAoX7s65izoX29vKpJ4I1G8JJ9gfR6MAwYNO4JIVci8UYY7Sek58uPEELI9anw+JtwYwkps8/qOv/R0FpdL69WWKu8DqSEDAQgJtf9l9Dq5mPkgUsyIUSn4LB4uvzIU1NwsNVuPLXnCv2vKY226ZPr0sK2SbwRDMNApT/4kGOWfaKRdqE+oa00jjrxpyEuXtZAxUpMlOu5Nz+Vt+2R9nzR21QtG8IeefBstaHztxSt5AvlTW8l8EZ9OFEEp1/VERN4Sx9iopj9auexN62q8ThC2MsBYikhZQwXDmYd7KCEEEKekRD2+DN3NMYu4MzKm8rEw2gMBHsn3BBOf2fkNaVRde+Nb+1vTgvbJvEwW0/hipokGh2zTe3ess89ORWGf2b8qjtra4VgmDZO4s2wnv7DvLd73sca/7IGCDDcVWLWcHHWMAHGys4tgo8FpJT4yBPKaZKNHKwHcXpd+BgbSwziQuM25u3D+VrsO4s/thGih/lHDNfM3nZ8zKGVKnyDAxd/93a/3nniwSf8XzZAPRZW+3L08Do1z57DfVO3Sdvd1nftN40SI+TiJPBGRe9WlcOivXPkvvIjnSPcwBeL3JbozbEQEdbvBbfb+xqR+rbdmkDgE99J+zTIPXHItkngzdaY/pvjT6v0KhyzdbC/ADgY6vOxryBrFWDo9NF3hlhoxrpQvluYiAhfcTZYIwf9bAEY+8Dnd5Ln8be+kZpl8/z6AxPYNWLvFVMznQwOyQe7kCJrgroNpVKatFMtlM1AuIFvTvGaXejNFy3SOYUE7+sIwz8Vm6SnpmqThswDpO/aP8cMPyBPTPgGZxgoXq4VrDqHRRvnyL3lRzpH8v1ZutYflB1cvT9DwN8o/aiPgZnZGY1pWFjFU6x3jsz+FxZydd1wXXKAsm1Tdi4foOSYrW/3tn3u2Wn/qz7JN8pVdgARaK9jKr37l5A9h/yO8A1OJNBmM3kpEm8GZxjgno5AXIeaTVr8hYi7dtIlrAmEP275AwMtnCM14ZmXyo+QX2Otsg3l7q1SH05kYtA/LJq8rxFBIIhdmPAx5qvhu6Jk2yQeZiXncmMKZccs2O6HaGn4J/BmDoaBROx+FirRx3K3cCQeln46IHuvU5jaLr09p8ieQ14l3Pgw3b+6l6A/gBmt8b5Xe3mBUJ9d2iP011PE3bP6j6Z7b0RId7CwkhJSTrA5wy+M9eYfB5wi582PkDbEX1HldWulxHPLAAJD1E33+zRCO1075QpWb44PGcP9HDMs9E6otG3iL0TIN3ljH/DHTYe+KxyzR7Z7XZ97Zg7E+KcHY9VdWW++gPDVShSYYpa9UnEwDLLwoN4c/9xPZZd+RCjGKbK/lu/hdQr9cG/iYemL8u6zN8c/F/t6mmEB9wJvi0LbQP9rUT5z0HX6A5j+Mm9TZcNJyHNjYSVjuKaPzVUMmGvnR7pC6qArz+PT1+bVLrTH8LOY/+PSiNFBG+8uqbVtCod2AwGIoD5io9IxWyJv99/2uadju93K3xNIASGDE1IgOVpbxK40AQnt89x1rffFYt8qfA+EVjfiCSrmtLH4+FB/Xf/YNaXpxvu/tblA6fD7e6Yr48I4iV1TeTaWrmnKLMkS186PdI/SGK+dxwMpdt+La2Fxsm9YCwKhr6EmO+NN+K1tEwi1PWPpmsr32JVmg1xtuz+h7XAM2+1WGtvtVr68vPxy2xDCNjaY8MDMWfj5+cHv2+Lxof7Un/p3V3/y/LCPd5PztDvtzXPw8/OD/05LwsJKsgkIIYQQQshlSLwlEHzQ6D8DJxr+hBBCCCGEXI7e/AOrWxfiSWj/c56EEEIIIYSQh+U/ADAM49blIIQQQgghhFyQ/wO9LpLp1F+ByQAAAABJRU5ErkJggg==)\n",
        "\n",
        "The experiments with lower # of parameters are \n",
        "\n",
        "![image.png](data:image/png;base64,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)\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "ld1WcEvoxPA0"
      }
    }
  ]
}
