{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LSTM_pamap2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LSTM models for Human Activity Recognition**\n",
        "\n",
        "Experiements on [Pamap2](https://archive.ics.uci.edu/ml/datasets/pamap2+physical+activity+monitoring) dataset using different combinations of \n",
        "*with/without* x *temporal and/or spatial attention* x *1 or 2 LSTM layer(s)*\n",
        "\n"
      ],
      "metadata": {
        "id": "-hUwKQBBY40r"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oSzZmQw9QLlv",
        "outputId": "8efa9816-aae4-4480-a3fa-fc2f0b6be171"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyyaml h5py \n",
        "!pip install -q tensorflow-addons\n",
        "!pip install keras\n",
        "!pip install pyts\n"
      ],
      "metadata": {
        "id": "UAFxJj4VR7Vf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "eabdb3c7-1696-478d-8be3-2bfa28967eb2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (3.13)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (3.1.0)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py) (1.5.2)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from h5py) (1.21.6)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 4.9 MB/s \n",
            "\u001b[?25hLooking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: keras in /usr/local/lib/python3.7/dist-packages (2.8.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pyts\n",
            "  Downloading pyts-0.12.0-py3-none-any.whl (2.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5 MB 4.9 MB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.22.1 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.0.2)\n",
            "Requirement already satisfied: scipy>=1.3.0 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.4.1)\n",
            "Requirement already satisfied: numba>=0.48.0 in /usr/local/lib/python3.7/dist-packages (from pyts) (0.51.2)\n",
            "Requirement already satisfied: joblib>=0.12 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.1.0)\n",
            "Requirement already satisfied: numpy>=1.17.5 in /usr/local/lib/python3.7/dist-packages (from pyts) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.48.0->pyts) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.22.1->pyts) (3.1.0)\n",
            "Installing collected packages: pyts\n",
            "Successfully installed pyts-0.12.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/Debarshi-Bhattacharya/Ensem_HAR/blob/9d7769f34258185c56feb7c34f6059e07469030f/Implementation_on_PAMAP2/datapreprocessing.ipynb\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import math\n",
        "import h5py\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "'''\n",
        "0: 'transient', 1: 'lying', 2: 'sitting', 3: 'standing', 4: 'walking', 5: 'running', 6: 'cycling', 7: 'Nordic_walking', 9: 'watching_TV', \n",
        "10: 'computer_work', 11: 'car driving', 12: 'ascending_stairs', 13: 'descending_stairs', 16: 'vacuum_cleaning', 17: 'ironing', \n",
        "18: 'folding_laundry', 19: 'house_cleaning', 20: 'playing_soccer', 24: 'rope_jumping'\n",
        "'''\n",
        "\n",
        "def read_files():\n",
        "    list_of_files = ['Protocol/subject101.dat',\n",
        "                     'Protocol/subject102.dat',\n",
        "                     'Protocol/subject103.dat',\n",
        "                     'Protocol/subject104.dat',\n",
        "                     'Protocol/subject105.dat',\n",
        "                     'Protocol/subject106.dat',\n",
        "                     'Protocol/subject107.dat',\n",
        "                     'Protocol/subject108.dat',\n",
        "                     'Protocol/subject109.dat']\n",
        "    \n",
        "    subjectID = [1,2,3,4,5,6,7,8,9]\n",
        "    \n",
        "    # there are 54 columns in the data files\n",
        "    colNames = [\"timestamp\", \"activityID\",\"heartrate\"] # 1, 2, 3\n",
        "    IMUhand = ['handTemperature', \n",
        "               'handAcc16_1', 'handAcc16_2', 'handAcc16_3', \n",
        "               'handAcc6_1', 'handAcc6_2', 'handAcc6_3', \n",
        "               'handGyro1', 'handGyro2', 'handGyro3', \n",
        "               'handMagne1', 'handMagne2', 'handMagne3',\n",
        "               'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4'] # 4-20\n",
        "    IMUchest = ['chestTemperature', \n",
        "               'chestAcc16_1', 'chestAcc16_2', 'chestAcc16_3', \n",
        "               'chestAcc6_1', 'chestAcc6_2', 'chestAcc6_3', \n",
        "               'chestGyro1', 'chestGyro2', 'chestGyro3', \n",
        "               'chestMagne1', 'chestMagne2', 'chestMagne3',\n",
        "               'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4'] # 21-37\n",
        "    IMUankle = ['ankleTemperature', \n",
        "               'ankleAcc16_1', 'ankleAcc16_2', 'ankleAcc16_3', \n",
        "               'ankleAcc6_1', 'ankleAcc6_2', 'ankleAcc6_3', \n",
        "               'ankleGyro1', 'ankleGyro2', 'ankleGyro3', \n",
        "               'ankleMagne1', 'ankleMagne2', 'ankleMagne3',\n",
        "               'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'] # 38-54\n",
        "    \n",
        "    columns = colNames + IMUhand + IMUchest + IMUankle\n",
        "    \n",
        "    dataCollection = pd.DataFrame()\n",
        "\n",
        "    for file in list_of_files:\n",
        "        print(file)\n",
        "        procData = pd.read_table(file, header=None, sep='\\s+')\n",
        "        procData.columns = columns\n",
        "        procData['subject_id'] = int(file[-5])\n",
        "        dataCollection = dataCollection.append(procData, ignore_index=True) \n",
        "        \n",
        "    dataCollection.reset_index(drop=True, inplace=True)\n",
        "    \n",
        "    return dataCollection\n",
        "\n",
        "data = read_files()\n",
        "data.head()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "5TfLqWMp7WWi",
        "outputId": "33ee15e7-a713-49f3-d953-95850af06b29"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Protocol/subject101.dat\n",
            "Protocol/subject102.dat\n",
            "Protocol/subject103.dat\n",
            "Protocol/subject104.dat\n",
            "Protocol/subject105.dat\n",
            "Protocol/subject106.dat\n",
            "Protocol/subject107.dat\n",
            "Protocol/subject108.dat\n",
            "Protocol/subject109.dat\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   timestamp  activityID  heartrate  handTemperature  handAcc16_1  \\\n",
              "0       8.38           0      104.0             30.0      2.37223   \n",
              "1       8.39           0        NaN             30.0      2.18837   \n",
              "2       8.40           0        NaN             30.0      2.37357   \n",
              "3       8.41           0        NaN             30.0      2.07473   \n",
              "4       8.42           0        NaN             30.0      2.22936   \n",
              "\n",
              "   handAcc16_2  handAcc16_3  handAcc6_1  handAcc6_2  handAcc6_3  ...  \\\n",
              "0      8.60074      3.51048     2.43954     8.76165     3.35465  ...   \n",
              "1      8.56560      3.66179     2.39494     8.55081     3.64207  ...   \n",
              "2      8.60107      3.54898     2.30514     8.53644     3.73280  ...   \n",
              "3      8.52853      3.66021     2.33528     8.53622     3.73277  ...   \n",
              "4      8.83122      3.70000     2.23055     8.59741     3.76295  ...   \n",
              "\n",
              "   ankleGyro2  ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  \\\n",
              "0    0.009250   -0.017580     -61.1888     -38.9599     -58.1438   \n",
              "1   -0.004638    0.000368     -59.8479     -38.8919     -58.5253   \n",
              "2    0.000148    0.022495     -60.7361     -39.4138     -58.3999   \n",
              "3   -0.020301    0.011275     -60.4091     -38.7635     -58.3956   \n",
              "4   -0.014303   -0.002823     -61.5199     -39.3879     -58.2694   \n",
              "\n",
              "   ankleOrientation1  ankleOrientation2  ankleOrientation3  ankleOrientation4  \\\n",
              "0                1.0                0.0                0.0                0.0   \n",
              "1                1.0                0.0                0.0                0.0   \n",
              "2                1.0                0.0                0.0                0.0   \n",
              "3                1.0                0.0                0.0                0.0   \n",
              "4                1.0                0.0                0.0                0.0   \n",
              "\n",
              "   subject_id  \n",
              "0           1  \n",
              "1           1  \n",
              "2           1  \n",
              "3           1  \n",
              "4           1  \n",
              "\n",
              "[5 rows x 55 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-09192e17-e1bb-4c45-b60c-89cfff09dd8b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>timestamp</th>\n",
              "      <th>activityID</th>\n",
              "      <th>heartrate</th>\n",
              "      <th>handTemperature</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>ankleOrientation1</th>\n",
              "      <th>ankleOrientation2</th>\n",
              "      <th>ankleOrientation3</th>\n",
              "      <th>ankleOrientation4</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>8.38</td>\n",
              "      <td>0</td>\n",
              "      <td>104.0</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.37223</td>\n",
              "      <td>8.60074</td>\n",
              "      <td>3.51048</td>\n",
              "      <td>2.43954</td>\n",
              "      <td>8.76165</td>\n",
              "      <td>3.35465</td>\n",
              "      <td>...</td>\n",
              "      <td>0.009250</td>\n",
              "      <td>-0.017580</td>\n",
              "      <td>-61.1888</td>\n",
              "      <td>-38.9599</td>\n",
              "      <td>-58.1438</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.39</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.18837</td>\n",
              "      <td>8.56560</td>\n",
              "      <td>3.66179</td>\n",
              "      <td>2.39494</td>\n",
              "      <td>8.55081</td>\n",
              "      <td>3.64207</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.004638</td>\n",
              "      <td>0.000368</td>\n",
              "      <td>-59.8479</td>\n",
              "      <td>-38.8919</td>\n",
              "      <td>-58.5253</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8.40</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.37357</td>\n",
              "      <td>8.60107</td>\n",
              "      <td>3.54898</td>\n",
              "      <td>2.30514</td>\n",
              "      <td>8.53644</td>\n",
              "      <td>3.73280</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000148</td>\n",
              "      <td>0.022495</td>\n",
              "      <td>-60.7361</td>\n",
              "      <td>-39.4138</td>\n",
              "      <td>-58.3999</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>8.41</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.07473</td>\n",
              "      <td>8.52853</td>\n",
              "      <td>3.66021</td>\n",
              "      <td>2.33528</td>\n",
              "      <td>8.53622</td>\n",
              "      <td>3.73277</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.020301</td>\n",
              "      <td>0.011275</td>\n",
              "      <td>-60.4091</td>\n",
              "      <td>-38.7635</td>\n",
              "      <td>-58.3956</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>8.42</td>\n",
              "      <td>0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>30.0</td>\n",
              "      <td>2.22936</td>\n",
              "      <td>8.83122</td>\n",
              "      <td>3.70000</td>\n",
              "      <td>2.23055</td>\n",
              "      <td>8.59741</td>\n",
              "      <td>3.76295</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.014303</td>\n",
              "      <td>-0.002823</td>\n",
              "      <td>-61.5199</td>\n",
              "      <td>-39.3879</td>\n",
              "      <td>-58.2694</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 55 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-09192e17-e1bb-4c45-b60c-89cfff09dd8b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-09192e17-e1bb-4c45-b60c-89cfff09dd8b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-09192e17-e1bb-4c45-b60c-89cfff09dd8b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def dataCleaning(dataCollection):\n",
        "    dataCollection = dataCollection.drop(['timestamp', 'handOrientation1', 'handOrientation2', 'handOrientation3', 'handOrientation4',\n",
        "                                         'chestOrientation1', 'chestOrientation2', 'chestOrientation3', 'chestOrientation4',\n",
        "                                         'ankleOrientation1', 'ankleOrientation2', 'ankleOrientation3', 'ankleOrientation4'],\n",
        "                                         axis = 1)  # removal of orientation columns as they are not needed\n",
        "    dataCollection = dataCollection.drop(dataCollection[dataCollection.activityID == 0].index) # removal of any row of activity 0 as it is transient activity which it is not used\n",
        "    dataCollection = dataCollection.apply(pd.to_numeric, errors = 'coerce') # removal of non numeric data in cells\n",
        "    dataCollection = dataCollection.drop(['heartrate'], axis = 1)\n",
        "    dataCollection = dataCollection.dropna()\n",
        "\n",
        "    dataCollection = dataCollection.drop(['handTemperature', 'chestTemperature', 'ankleTemperature'],\n",
        "                                         axis = 1)  # removal of temperature columns as they are not needed - sumeyye\n",
        "    print(\"data cleaned!\")\n",
        "    return dataCollection\n",
        "\n",
        "cleaned_data = dataCleaning(data)\n",
        "print(cleaned_data['activityID'].value_counts())\n",
        "cleaned_data.head(10)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 699
        },
        "id": "AD-d8IZUcjo4",
        "outputId": "d8d45602-8c2f-499a-adca-ad4d3e08060e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "data cleaned!\n",
            "17    237902\n",
            "4     229709\n",
            "1     192290\n",
            "3     188984\n",
            "2     184645\n",
            "7     184444\n",
            "16    174976\n",
            "6     163302\n",
            "12    117094\n",
            "13    104865\n",
            "5      95641\n",
            "24     47579\n",
            "Name: activityID, dtype: int64\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1      2.21530      8.27915      5.58753     2.24689   \n",
              "2929           1      2.29196      7.67288      5.74467     2.27373   \n",
              "2930           1      2.29090      7.14240      5.82342     2.26966   \n",
              "2931           1      2.21800      7.14365      5.89930     2.22177   \n",
              "2932           1      2.30106      7.25857      6.09259     2.20720   \n",
              "2933           1      2.07165      7.25965      6.01218     2.19238   \n",
              "2934           1      2.41148      7.59780      5.93915     2.23988   \n",
              "2935           1      2.32815      7.63431      5.70686     2.31663   \n",
              "2936           1      2.25096      7.78598      5.62821     2.28637   \n",
              "2937           1      2.14107      7.52262      5.78141     2.31538   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928     8.55387     5.77143  -0.004750   0.037579  -0.011145  ...   \n",
              "2929     8.14592     5.78739  -0.171710   0.025479  -0.009538  ...   \n",
              "2930     7.66268     5.78846  -0.238241   0.011214   0.000831  ...   \n",
              "2931     7.25535     5.88000  -0.192912   0.019053   0.013374  ...   \n",
              "2932     7.24042     5.95555  -0.069961  -0.018328   0.004582  ...   \n",
              "2933     7.21038     6.01604   0.063895   0.007175   0.024701  ...   \n",
              "2934     7.46679     6.03053   0.190837   0.003116   0.038762  ...   \n",
              "2935     7.64745     6.01495   0.200328  -0.009266   0.068567  ...   \n",
              "2936     7.70801     5.93935   0.204098  -0.068256   0.050000  ...   \n",
              "2937     7.72276     5.78828   0.171291  -0.055411   0.021576  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "2933      9.60177     -1.75239     0.311276    0.003793   -0.026906   \n",
              "2934      9.67694     -1.76748     0.326060    0.036814   -0.032277   \n",
              "2935      9.61685     -1.76749     0.326380   -0.010352   -0.016621   \n",
              "2936      9.61686     -1.72212     0.326234    0.039346    0.020393   \n",
              "2937      9.63189     -1.70699     0.326105    0.029874   -0.010763   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "2933    0.004125     -61.3257     -36.9744     -57.7501           1  \n",
              "2934   -0.006866     -61.5520     -36.9632     -57.9957           1  \n",
              "2935    0.006548     -61.5738     -36.1724     -59.3487           1  \n",
              "2936   -0.011880     -61.7741     -37.1744     -58.1199           1  \n",
              "2937    0.005133     -60.7680     -37.4206     -58.8735           1  \n",
              "\n",
              "[10 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-995d8752-721e-4d1a-85cb-104d97ec6c08\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>2.07165</td>\n",
              "      <td>7.25965</td>\n",
              "      <td>6.01218</td>\n",
              "      <td>2.19238</td>\n",
              "      <td>7.21038</td>\n",
              "      <td>6.01604</td>\n",
              "      <td>0.063895</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60177</td>\n",
              "      <td>-1.75239</td>\n",
              "      <td>0.311276</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-61.3257</td>\n",
              "      <td>-36.9744</td>\n",
              "      <td>-57.7501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>2.41148</td>\n",
              "      <td>7.59780</td>\n",
              "      <td>5.93915</td>\n",
              "      <td>2.23988</td>\n",
              "      <td>7.46679</td>\n",
              "      <td>6.03053</td>\n",
              "      <td>0.190837</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.038762</td>\n",
              "      <td>...</td>\n",
              "      <td>9.67694</td>\n",
              "      <td>-1.76748</td>\n",
              "      <td>0.326060</td>\n",
              "      <td>0.036814</td>\n",
              "      <td>-0.032277</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-61.5520</td>\n",
              "      <td>-36.9632</td>\n",
              "      <td>-57.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>2.32815</td>\n",
              "      <td>7.63431</td>\n",
              "      <td>5.70686</td>\n",
              "      <td>2.31663</td>\n",
              "      <td>7.64745</td>\n",
              "      <td>6.01495</td>\n",
              "      <td>0.200328</td>\n",
              "      <td>-0.009266</td>\n",
              "      <td>0.068567</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61685</td>\n",
              "      <td>-1.76749</td>\n",
              "      <td>0.326380</td>\n",
              "      <td>-0.010352</td>\n",
              "      <td>-0.016621</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>-61.5738</td>\n",
              "      <td>-36.1724</td>\n",
              "      <td>-59.3487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>2.25096</td>\n",
              "      <td>7.78598</td>\n",
              "      <td>5.62821</td>\n",
              "      <td>2.28637</td>\n",
              "      <td>7.70801</td>\n",
              "      <td>5.93935</td>\n",
              "      <td>0.204098</td>\n",
              "      <td>-0.068256</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61686</td>\n",
              "      <td>-1.72212</td>\n",
              "      <td>0.326234</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-61.7741</td>\n",
              "      <td>-37.1744</td>\n",
              "      <td>-58.1199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>2.14107</td>\n",
              "      <td>7.52262</td>\n",
              "      <td>5.78141</td>\n",
              "      <td>2.31538</td>\n",
              "      <td>7.72276</td>\n",
              "      <td>5.78828</td>\n",
              "      <td>0.171291</td>\n",
              "      <td>-0.055411</td>\n",
              "      <td>0.021576</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63189</td>\n",
              "      <td>-1.70699</td>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.029874</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>0.005133</td>\n",
              "      <td>-60.7680</td>\n",
              "      <td>-37.4206</td>\n",
              "      <td>-58.8735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-995d8752-721e-4d1a-85cb-104d97ec6c08')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-995d8752-721e-4d1a-85cb-104d97ec6c08 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-995d8752-721e-4d1a-85cb-104d97ec6c08');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def reset_label(dataCollection): \n",
        "    # Convert original labels {1, 2, 3, 4, 5, 6, 7, 12, 13, 16, 17, 24} to new labels. \n",
        "    mapping = {24:0,1:1,2:2,3:3,4:4,5:5,6:6,7:7,12:8,13:9,16:10,17:11} # old activity Id to new activity Id \n",
        "    for i in [24,12,13,16,17]:\n",
        "        dataCollection.loc[dataCollection.activityID == i, 'activityID'] = mapping[i]\n",
        "\n",
        "    return dataCollection\n",
        "data_reset = reset_label(cleaned_data)  \n",
        "data_reset.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "nBYOFqYrcu2i",
        "outputId": "830f52df-c9e5-424b-c40b-df091b143e30"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1      2.21530      8.27915      5.58753     2.24689   \n",
              "2929           1      2.29196      7.67288      5.74467     2.27373   \n",
              "2930           1      2.29090      7.14240      5.82342     2.26966   \n",
              "2931           1      2.21800      7.14365      5.89930     2.22177   \n",
              "2932           1      2.30106      7.25857      6.09259     2.20720   \n",
              "2933           1      2.07165      7.25965      6.01218     2.19238   \n",
              "2934           1      2.41148      7.59780      5.93915     2.23988   \n",
              "2935           1      2.32815      7.63431      5.70686     2.31663   \n",
              "2936           1      2.25096      7.78598      5.62821     2.28637   \n",
              "2937           1      2.14107      7.52262      5.78141     2.31538   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928     8.55387     5.77143  -0.004750   0.037579  -0.011145  ...   \n",
              "2929     8.14592     5.78739  -0.171710   0.025479  -0.009538  ...   \n",
              "2930     7.66268     5.78846  -0.238241   0.011214   0.000831  ...   \n",
              "2931     7.25535     5.88000  -0.192912   0.019053   0.013374  ...   \n",
              "2932     7.24042     5.95555  -0.069961  -0.018328   0.004582  ...   \n",
              "2933     7.21038     6.01604   0.063895   0.007175   0.024701  ...   \n",
              "2934     7.46679     6.03053   0.190837   0.003116   0.038762  ...   \n",
              "2935     7.64745     6.01495   0.200328  -0.009266   0.068567  ...   \n",
              "2936     7.70801     5.93935   0.204098  -0.068256   0.050000  ...   \n",
              "2937     7.72276     5.78828   0.171291  -0.055411   0.021576  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "2933      9.60177     -1.75239     0.311276    0.003793   -0.026906   \n",
              "2934      9.67694     -1.76748     0.326060    0.036814   -0.032277   \n",
              "2935      9.61685     -1.76749     0.326380   -0.010352   -0.016621   \n",
              "2936      9.61686     -1.72212     0.326234    0.039346    0.020393   \n",
              "2937      9.63189     -1.70699     0.326105    0.029874   -0.010763   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "2933    0.004125     -61.3257     -36.9744     -57.7501           1  \n",
              "2934   -0.006866     -61.5520     -36.9632     -57.9957           1  \n",
              "2935    0.006548     -61.5738     -36.1724     -59.3487           1  \n",
              "2936   -0.011880     -61.7741     -37.1744     -58.1199           1  \n",
              "2937    0.005133     -60.7680     -37.4206     -58.8735           1  \n",
              "\n",
              "[10 rows x 38 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c8cbf13e-b086-455f-980c-d9c03df49ea8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>2.07165</td>\n",
              "      <td>7.25965</td>\n",
              "      <td>6.01218</td>\n",
              "      <td>2.19238</td>\n",
              "      <td>7.21038</td>\n",
              "      <td>6.01604</td>\n",
              "      <td>0.063895</td>\n",
              "      <td>0.007175</td>\n",
              "      <td>0.024701</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60177</td>\n",
              "      <td>-1.75239</td>\n",
              "      <td>0.311276</td>\n",
              "      <td>0.003793</td>\n",
              "      <td>-0.026906</td>\n",
              "      <td>0.004125</td>\n",
              "      <td>-61.3257</td>\n",
              "      <td>-36.9744</td>\n",
              "      <td>-57.7501</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>2.41148</td>\n",
              "      <td>7.59780</td>\n",
              "      <td>5.93915</td>\n",
              "      <td>2.23988</td>\n",
              "      <td>7.46679</td>\n",
              "      <td>6.03053</td>\n",
              "      <td>0.190837</td>\n",
              "      <td>0.003116</td>\n",
              "      <td>0.038762</td>\n",
              "      <td>...</td>\n",
              "      <td>9.67694</td>\n",
              "      <td>-1.76748</td>\n",
              "      <td>0.326060</td>\n",
              "      <td>0.036814</td>\n",
              "      <td>-0.032277</td>\n",
              "      <td>-0.006866</td>\n",
              "      <td>-61.5520</td>\n",
              "      <td>-36.9632</td>\n",
              "      <td>-57.9957</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>2.32815</td>\n",
              "      <td>7.63431</td>\n",
              "      <td>5.70686</td>\n",
              "      <td>2.31663</td>\n",
              "      <td>7.64745</td>\n",
              "      <td>6.01495</td>\n",
              "      <td>0.200328</td>\n",
              "      <td>-0.009266</td>\n",
              "      <td>0.068567</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61685</td>\n",
              "      <td>-1.76749</td>\n",
              "      <td>0.326380</td>\n",
              "      <td>-0.010352</td>\n",
              "      <td>-0.016621</td>\n",
              "      <td>0.006548</td>\n",
              "      <td>-61.5738</td>\n",
              "      <td>-36.1724</td>\n",
              "      <td>-59.3487</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>2.25096</td>\n",
              "      <td>7.78598</td>\n",
              "      <td>5.62821</td>\n",
              "      <td>2.28637</td>\n",
              "      <td>7.70801</td>\n",
              "      <td>5.93935</td>\n",
              "      <td>0.204098</td>\n",
              "      <td>-0.068256</td>\n",
              "      <td>0.050000</td>\n",
              "      <td>...</td>\n",
              "      <td>9.61686</td>\n",
              "      <td>-1.72212</td>\n",
              "      <td>0.326234</td>\n",
              "      <td>0.039346</td>\n",
              "      <td>0.020393</td>\n",
              "      <td>-0.011880</td>\n",
              "      <td>-61.7741</td>\n",
              "      <td>-37.1744</td>\n",
              "      <td>-58.1199</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>2.14107</td>\n",
              "      <td>7.52262</td>\n",
              "      <td>5.78141</td>\n",
              "      <td>2.31538</td>\n",
              "      <td>7.72276</td>\n",
              "      <td>5.78828</td>\n",
              "      <td>0.171291</td>\n",
              "      <td>-0.055411</td>\n",
              "      <td>0.021576</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63189</td>\n",
              "      <td>-1.70699</td>\n",
              "      <td>0.326105</td>\n",
              "      <td>0.029874</td>\n",
              "      <td>-0.010763</td>\n",
              "      <td>0.005133</td>\n",
              "      <td>-60.7680</td>\n",
              "      <td>-37.4206</td>\n",
              "      <td>-58.8735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 38 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c8cbf13e-b086-455f-980c-d9c03df49ea8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c8cbf13e-b086-455f-980c-d9c03df49ea8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c8cbf13e-b086-455f-980c-d9c03df49ea8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X=data_reset.drop(['activityID'],axis=1)\n",
        "y=data_reset['activityID']\n",
        "\n",
        "X.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "NpsSGkYYdKF5",
        "outputId": "3a13c76f-2714-4e96-8076-059af5456cee"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  handAcc6_2  \\\n",
              "2928      2.21530      8.27915      5.58753     2.24689     8.55387   \n",
              "2929      2.29196      7.67288      5.74467     2.27373     8.14592   \n",
              "2930      2.29090      7.14240      5.82342     2.26966     7.66268   \n",
              "2931      2.21800      7.14365      5.89930     2.22177     7.25535   \n",
              "2932      2.30106      7.25857      6.09259     2.20720     7.24042   \n",
              "\n",
              "      handAcc6_3  handGyro1  handGyro2  handGyro3  handMagne1  ...  \\\n",
              "2928     5.77143  -0.004750   0.037579  -0.011145     8.93200  ...   \n",
              "2929     5.78739  -0.171710   0.025479  -0.009538     9.58300  ...   \n",
              "2930     5.78846  -0.238241   0.011214   0.000831     9.05516  ...   \n",
              "2931     5.88000  -0.192912   0.019053   0.013374     9.92698  ...   \n",
              "2932     5.95555  -0.069961  -0.018328   0.004582     9.15626  ...   \n",
              "\n",
              "      ankleAcc6_1  ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  \\\n",
              "2928      9.63162     -1.76757     0.265761    0.002908   -0.027714   \n",
              "2929      9.58649     -1.75247     0.250816    0.020882    0.000945   \n",
              "2930      9.60196     -1.73721     0.356632   -0.035392   -0.052422   \n",
              "2931      9.58674     -1.78264     0.311453   -0.032514   -0.018844   \n",
              "2932      9.64677     -1.75240     0.295902    0.001351   -0.048878   \n",
              "\n",
              "      ankleGyro3  ankleMagne1  ankleMagne2  ankleMagne3  subject_id  \n",
              "2928    0.001752     -61.1081     -36.8636     -58.3696           1  \n",
              "2929    0.006007     -60.8916     -36.3197     -58.3656           1  \n",
              "2930   -0.004882     -60.3407     -35.7842     -58.6119           1  \n",
              "2931    0.026950     -60.7646     -37.1028     -57.8799           1  \n",
              "2932   -0.006328     -60.2040     -37.1225     -57.8847           1  \n",
              "\n",
              "[5 rows x 37 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>handMagne1</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_1</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>2.21530</td>\n",
              "      <td>8.27915</td>\n",
              "      <td>5.58753</td>\n",
              "      <td>2.24689</td>\n",
              "      <td>8.55387</td>\n",
              "      <td>5.77143</td>\n",
              "      <td>-0.004750</td>\n",
              "      <td>0.037579</td>\n",
              "      <td>-0.011145</td>\n",
              "      <td>8.93200</td>\n",
              "      <td>...</td>\n",
              "      <td>9.63162</td>\n",
              "      <td>-1.76757</td>\n",
              "      <td>0.265761</td>\n",
              "      <td>0.002908</td>\n",
              "      <td>-0.027714</td>\n",
              "      <td>0.001752</td>\n",
              "      <td>-61.1081</td>\n",
              "      <td>-36.8636</td>\n",
              "      <td>-58.3696</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>2.29196</td>\n",
              "      <td>7.67288</td>\n",
              "      <td>5.74467</td>\n",
              "      <td>2.27373</td>\n",
              "      <td>8.14592</td>\n",
              "      <td>5.78739</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.025479</td>\n",
              "      <td>-0.009538</td>\n",
              "      <td>9.58300</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58649</td>\n",
              "      <td>-1.75247</td>\n",
              "      <td>0.250816</td>\n",
              "      <td>0.020882</td>\n",
              "      <td>0.000945</td>\n",
              "      <td>0.006007</td>\n",
              "      <td>-60.8916</td>\n",
              "      <td>-36.3197</td>\n",
              "      <td>-58.3656</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>2.29090</td>\n",
              "      <td>7.14240</td>\n",
              "      <td>5.82342</td>\n",
              "      <td>2.26966</td>\n",
              "      <td>7.66268</td>\n",
              "      <td>5.78846</td>\n",
              "      <td>-0.238241</td>\n",
              "      <td>0.011214</td>\n",
              "      <td>0.000831</td>\n",
              "      <td>9.05516</td>\n",
              "      <td>...</td>\n",
              "      <td>9.60196</td>\n",
              "      <td>-1.73721</td>\n",
              "      <td>0.356632</td>\n",
              "      <td>-0.035392</td>\n",
              "      <td>-0.052422</td>\n",
              "      <td>-0.004882</td>\n",
              "      <td>-60.3407</td>\n",
              "      <td>-35.7842</td>\n",
              "      <td>-58.6119</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>2.21800</td>\n",
              "      <td>7.14365</td>\n",
              "      <td>5.89930</td>\n",
              "      <td>2.22177</td>\n",
              "      <td>7.25535</td>\n",
              "      <td>5.88000</td>\n",
              "      <td>-0.192912</td>\n",
              "      <td>0.019053</td>\n",
              "      <td>0.013374</td>\n",
              "      <td>9.92698</td>\n",
              "      <td>...</td>\n",
              "      <td>9.58674</td>\n",
              "      <td>-1.78264</td>\n",
              "      <td>0.311453</td>\n",
              "      <td>-0.032514</td>\n",
              "      <td>-0.018844</td>\n",
              "      <td>0.026950</td>\n",
              "      <td>-60.7646</td>\n",
              "      <td>-37.1028</td>\n",
              "      <td>-57.8799</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>2.30106</td>\n",
              "      <td>7.25857</td>\n",
              "      <td>6.09259</td>\n",
              "      <td>2.20720</td>\n",
              "      <td>7.24042</td>\n",
              "      <td>5.95555</td>\n",
              "      <td>-0.069961</td>\n",
              "      <td>-0.018328</td>\n",
              "      <td>0.004582</td>\n",
              "      <td>9.15626</td>\n",
              "      <td>...</td>\n",
              "      <td>9.64677</td>\n",
              "      <td>-1.75240</td>\n",
              "      <td>0.295902</td>\n",
              "      <td>0.001351</td>\n",
              "      <td>-0.048878</td>\n",
              "      <td>-0.006328</td>\n",
              "      <td>-60.2040</td>\n",
              "      <td>-37.1225</td>\n",
              "      <td>-57.8847</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 37 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-4251e3d0-eafc-4ba4-8926-0f9a07f457ee');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_subID=X['subject_id']\n",
        "\n",
        "def scale(df): # minmax scale\n",
        "    features=df.columns[0:X.shape[1]]\n",
        "    scaler = MinMaxScaler(feature_range=(-1,1))\n",
        "    df[features]=scaler.fit_transform(df[features])\n",
        "    return df\n",
        "\n",
        "data_scaled =scale(X)\n",
        "data_scaled.shape\n",
        "X_scaled=pd.concat([pd.DataFrame(y,columns = ['activityID']),pd.DataFrame(data_scaled)],axis=1)\n",
        "X_scaled=pd.concat([pd.DataFrame(X_scaled),pd.DataFrame(X_subID,columns = ['subject_id'])],axis=1)\n",
        "\n",
        "X_scaled.head(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "E9rs2zewb8Pi",
        "outputId": "c60b7b47-d498-4dc1-f618-3c5e69fe160d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "      activityID  handAcc16_1  handAcc16_2  handAcc16_3  handAcc6_1  \\\n",
              "2928           1     0.417516    -0.133999    -0.174116    0.113009   \n",
              "2929           1     0.418253    -0.138662    -0.172903    0.113480   \n",
              "2930           1     0.418242    -0.142743    -0.172296    0.113408   \n",
              "2931           1     0.417542    -0.142733    -0.171710    0.112568   \n",
              "2932           1     0.418340    -0.141849    -0.170219    0.112313   \n",
              "2933           1     0.416137    -0.141841    -0.170839    0.112053   \n",
              "2934           1     0.419401    -0.139240    -0.171403    0.112886   \n",
              "2935           1     0.418600    -0.138959    -0.173195    0.114232   \n",
              "2936           1     0.417859    -0.137792    -0.173802    0.113701   \n",
              "2937           1     0.416803    -0.139818    -0.172620    0.114210   \n",
              "\n",
              "      handAcc6_2  handAcc6_3  handGyro1  handGyro2  handGyro3  ...  \\\n",
              "2928    0.134484    0.093285   0.031349  -0.125912  -0.003356  ...   \n",
              "2929    0.127909    0.093543   0.025227  -0.126503  -0.003244  ...   \n",
              "2930    0.120122    0.093560   0.022788  -0.127200  -0.002519  ...   \n",
              "2931    0.113557    0.095039   0.024450  -0.126817  -0.001641  ...   \n",
              "2932    0.113316    0.096259   0.028958  -0.128644  -0.002256  ...   \n",
              "2933    0.112832    0.097235   0.033865  -0.127398  -0.000850  ...   \n",
              "2934    0.116965    0.097469   0.038519  -0.127596   0.000134  ...   \n",
              "2935    0.119876    0.097218   0.038867  -0.128201   0.002218  ...   \n",
              "2936    0.120852    0.095997   0.039005  -0.131084   0.000920  ...   \n",
              "2937    0.121090    0.093558   0.037803  -0.130456  -0.001068  ...   \n",
              "\n",
              "      ankleAcc6_2  ankleAcc6_3  ankleGyro1  ankleGyro2  ankleGyro3  \\\n",
              "2928    -0.029670     0.015502    0.186908    0.141361   -0.082024   \n",
              "2929    -0.029426     0.015259    0.187797    0.143168   -0.081745   \n",
              "2930    -0.029180     0.016977    0.185013    0.139803   -0.082458   \n",
              "2931    -0.029913     0.016243    0.185156    0.141920   -0.080374   \n",
              "2932    -0.029425     0.015991    0.186831    0.140026   -0.082553   \n",
              "2933    -0.029425     0.016241    0.186951    0.141412   -0.081868   \n",
              "2934    -0.029668     0.016480    0.188585    0.141073   -0.082588   \n",
              "2935    -0.029669     0.016486    0.186252    0.142060   -0.081710   \n",
              "2936    -0.028937     0.016483    0.188710    0.144395   -0.082916   \n",
              "2937    -0.028692     0.016481    0.188241    0.142430   -0.081802   \n",
              "\n",
              "      ankleMagne1  ankleMagne2  ankleMagne3  subject_id  subject_id  \n",
              "2928    -0.154691    -0.129512    -0.644683        -1.0           1  \n",
              "2929    -0.153053    -0.124827    -0.644651        -1.0           1  \n",
              "2930    -0.148886    -0.120213    -0.646624        -1.0           1  \n",
              "2931    -0.152093    -0.131573    -0.640759        -1.0           1  \n",
              "2932    -0.147852    -0.131743    -0.640798        -1.0           1  \n",
              "2933    -0.156337    -0.130467    -0.639719        -1.0           1  \n",
              "2934    -0.158048    -0.130370    -0.641687        -1.0           1  \n",
              "2935    -0.158213    -0.123558    -0.652528        -1.0           1  \n",
              "2936    -0.159728    -0.132190    -0.642682        -1.0           1  \n",
              "2937    -0.152118    -0.134311    -0.648720        -1.0           1  \n",
              "\n",
              "[10 rows x 39 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-b815e042-a857-4dcd-adc2-d7441be0438a\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>activityID</th>\n",
              "      <th>handAcc16_1</th>\n",
              "      <th>handAcc16_2</th>\n",
              "      <th>handAcc16_3</th>\n",
              "      <th>handAcc6_1</th>\n",
              "      <th>handAcc6_2</th>\n",
              "      <th>handAcc6_3</th>\n",
              "      <th>handGyro1</th>\n",
              "      <th>handGyro2</th>\n",
              "      <th>handGyro3</th>\n",
              "      <th>...</th>\n",
              "      <th>ankleAcc6_2</th>\n",
              "      <th>ankleAcc6_3</th>\n",
              "      <th>ankleGyro1</th>\n",
              "      <th>ankleGyro2</th>\n",
              "      <th>ankleGyro3</th>\n",
              "      <th>ankleMagne1</th>\n",
              "      <th>ankleMagne2</th>\n",
              "      <th>ankleMagne3</th>\n",
              "      <th>subject_id</th>\n",
              "      <th>subject_id</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2928</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417516</td>\n",
              "      <td>-0.133999</td>\n",
              "      <td>-0.174116</td>\n",
              "      <td>0.113009</td>\n",
              "      <td>0.134484</td>\n",
              "      <td>0.093285</td>\n",
              "      <td>0.031349</td>\n",
              "      <td>-0.125912</td>\n",
              "      <td>-0.003356</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029670</td>\n",
              "      <td>0.015502</td>\n",
              "      <td>0.186908</td>\n",
              "      <td>0.141361</td>\n",
              "      <td>-0.082024</td>\n",
              "      <td>-0.154691</td>\n",
              "      <td>-0.129512</td>\n",
              "      <td>-0.644683</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2929</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418253</td>\n",
              "      <td>-0.138662</td>\n",
              "      <td>-0.172903</td>\n",
              "      <td>0.113480</td>\n",
              "      <td>0.127909</td>\n",
              "      <td>0.093543</td>\n",
              "      <td>0.025227</td>\n",
              "      <td>-0.126503</td>\n",
              "      <td>-0.003244</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029426</td>\n",
              "      <td>0.015259</td>\n",
              "      <td>0.187797</td>\n",
              "      <td>0.143168</td>\n",
              "      <td>-0.081745</td>\n",
              "      <td>-0.153053</td>\n",
              "      <td>-0.124827</td>\n",
              "      <td>-0.644651</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2930</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418242</td>\n",
              "      <td>-0.142743</td>\n",
              "      <td>-0.172296</td>\n",
              "      <td>0.113408</td>\n",
              "      <td>0.120122</td>\n",
              "      <td>0.093560</td>\n",
              "      <td>0.022788</td>\n",
              "      <td>-0.127200</td>\n",
              "      <td>-0.002519</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029180</td>\n",
              "      <td>0.016977</td>\n",
              "      <td>0.185013</td>\n",
              "      <td>0.139803</td>\n",
              "      <td>-0.082458</td>\n",
              "      <td>-0.148886</td>\n",
              "      <td>-0.120213</td>\n",
              "      <td>-0.646624</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2931</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417542</td>\n",
              "      <td>-0.142733</td>\n",
              "      <td>-0.171710</td>\n",
              "      <td>0.112568</td>\n",
              "      <td>0.113557</td>\n",
              "      <td>0.095039</td>\n",
              "      <td>0.024450</td>\n",
              "      <td>-0.126817</td>\n",
              "      <td>-0.001641</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029913</td>\n",
              "      <td>0.016243</td>\n",
              "      <td>0.185156</td>\n",
              "      <td>0.141920</td>\n",
              "      <td>-0.080374</td>\n",
              "      <td>-0.152093</td>\n",
              "      <td>-0.131573</td>\n",
              "      <td>-0.640759</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2932</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418340</td>\n",
              "      <td>-0.141849</td>\n",
              "      <td>-0.170219</td>\n",
              "      <td>0.112313</td>\n",
              "      <td>0.113316</td>\n",
              "      <td>0.096259</td>\n",
              "      <td>0.028958</td>\n",
              "      <td>-0.128644</td>\n",
              "      <td>-0.002256</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029425</td>\n",
              "      <td>0.015991</td>\n",
              "      <td>0.186831</td>\n",
              "      <td>0.140026</td>\n",
              "      <td>-0.082553</td>\n",
              "      <td>-0.147852</td>\n",
              "      <td>-0.131743</td>\n",
              "      <td>-0.640798</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2933</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416137</td>\n",
              "      <td>-0.141841</td>\n",
              "      <td>-0.170839</td>\n",
              "      <td>0.112053</td>\n",
              "      <td>0.112832</td>\n",
              "      <td>0.097235</td>\n",
              "      <td>0.033865</td>\n",
              "      <td>-0.127398</td>\n",
              "      <td>-0.000850</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029425</td>\n",
              "      <td>0.016241</td>\n",
              "      <td>0.186951</td>\n",
              "      <td>0.141412</td>\n",
              "      <td>-0.081868</td>\n",
              "      <td>-0.156337</td>\n",
              "      <td>-0.130467</td>\n",
              "      <td>-0.639719</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2934</th>\n",
              "      <td>1</td>\n",
              "      <td>0.419401</td>\n",
              "      <td>-0.139240</td>\n",
              "      <td>-0.171403</td>\n",
              "      <td>0.112886</td>\n",
              "      <td>0.116965</td>\n",
              "      <td>0.097469</td>\n",
              "      <td>0.038519</td>\n",
              "      <td>-0.127596</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029668</td>\n",
              "      <td>0.016480</td>\n",
              "      <td>0.188585</td>\n",
              "      <td>0.141073</td>\n",
              "      <td>-0.082588</td>\n",
              "      <td>-0.158048</td>\n",
              "      <td>-0.130370</td>\n",
              "      <td>-0.641687</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2935</th>\n",
              "      <td>1</td>\n",
              "      <td>0.418600</td>\n",
              "      <td>-0.138959</td>\n",
              "      <td>-0.173195</td>\n",
              "      <td>0.114232</td>\n",
              "      <td>0.119876</td>\n",
              "      <td>0.097218</td>\n",
              "      <td>0.038867</td>\n",
              "      <td>-0.128201</td>\n",
              "      <td>0.002218</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.029669</td>\n",
              "      <td>0.016486</td>\n",
              "      <td>0.186252</td>\n",
              "      <td>0.142060</td>\n",
              "      <td>-0.081710</td>\n",
              "      <td>-0.158213</td>\n",
              "      <td>-0.123558</td>\n",
              "      <td>-0.652528</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2936</th>\n",
              "      <td>1</td>\n",
              "      <td>0.417859</td>\n",
              "      <td>-0.137792</td>\n",
              "      <td>-0.173802</td>\n",
              "      <td>0.113701</td>\n",
              "      <td>0.120852</td>\n",
              "      <td>0.095997</td>\n",
              "      <td>0.039005</td>\n",
              "      <td>-0.131084</td>\n",
              "      <td>0.000920</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028937</td>\n",
              "      <td>0.016483</td>\n",
              "      <td>0.188710</td>\n",
              "      <td>0.144395</td>\n",
              "      <td>-0.082916</td>\n",
              "      <td>-0.159728</td>\n",
              "      <td>-0.132190</td>\n",
              "      <td>-0.642682</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2937</th>\n",
              "      <td>1</td>\n",
              "      <td>0.416803</td>\n",
              "      <td>-0.139818</td>\n",
              "      <td>-0.172620</td>\n",
              "      <td>0.114210</td>\n",
              "      <td>0.121090</td>\n",
              "      <td>0.093558</td>\n",
              "      <td>0.037803</td>\n",
              "      <td>-0.130456</td>\n",
              "      <td>-0.001068</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.028692</td>\n",
              "      <td>0.016481</td>\n",
              "      <td>0.188241</td>\n",
              "      <td>0.142430</td>\n",
              "      <td>-0.081802</td>\n",
              "      <td>-0.152118</td>\n",
              "      <td>-0.134311</td>\n",
              "      <td>-0.648720</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 39 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b815e042-a857-4dcd-adc2-d7441be0438a')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b815e042-a857-4dcd-adc2-d7441be0438a button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b815e042-a857-4dcd-adc2-d7441be0438a');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "SLIDING_WINDOW_LENGTH = 100\n",
        "\n",
        "def segment_signal(data, window_size): # data is numpy array\n",
        "    n = len(data)\n",
        "    X, y = [], []\n",
        "    start, end = 0, 0\n",
        "    while start + window_size - 1 < n:\n",
        "        end = start + window_size-1\n",
        "        # if the frame contains the same activity and from the same object\n",
        "        X.append(data[start:(end+1),1:-1])\n",
        "        y.append(data[start][0])\n",
        "        start += window_size #without overlap (for 50% overlap use window_size//2)\n",
        "    print(np.asarray(X).shape, np.asarray(y).shape)\n",
        "    return {'inputs' : np.asarray(X), 'labels': np.asarray(y,dtype=int)}\n",
        "\n",
        "data_segmented=segment_signal(X_scaled.to_numpy(),SLIDING_WINDOW_LENGTH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "sJOSDb87g3KU",
        "outputId": "139ffe00-08b1-49da-97b4-dcd2a24f39c2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37) (19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def save_data(data,file_name): # save the data in h5 format\n",
        "    f = h5py.File(file_name,'w')\n",
        "    for key in data:\n",
        "        f.create_dataset(key,data = data[key])       \n",
        "    f.close()   \n",
        "\n",
        "file_name = 'pamap_scaled_segmented_100.h5'\n",
        "\n",
        "save_data(data_segmented, file_name)\n",
        "print(\"File is saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "f3dWmwoLhWEH",
        "outputId": "91c9ca52-e2a3-4d1a-e48f-eb2ea51ce885"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File is saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.models import Model\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "path = \"pamap_scaled_segmented_100.h5\"\n",
        "\n",
        "f = h5py.File(path, 'r')\n",
        "\n",
        "data_x = np.array(f[\"inputs\"][:]) \n",
        "data_y = np.array(f[\"labels\"][:])\n",
        "\n",
        "print(data_x.shape)\n",
        "print(data_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "y7xOGP679WKn",
        "outputId": "8d9b8d2e-b82f-4bbe-effe-208030760677"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37)\n",
            "(19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://stackoverflow.com/questions/53731141/cifar10-randomize-train-and-test-set\n",
        "def shuffle_train_data(X_train, Y_train): \n",
        "    \"\"\"called after each epoch\"\"\" \n",
        "    perm = np.random.permutation(len(Y_train)) \n",
        "    Xtr_shuf = X_train[perm] \n",
        "    Ytr_shuf = Y_train[perm] \n",
        "    return Xtr_shuf, Ytr_shuf \n",
        "X_shuffled, y_shuffled = shuffle_train_data(data_x, data_y) \n",
        "print(X_shuffled.shape) \n",
        "print(y_shuffled.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G7Kk09rA3Ems",
        "outputId": "7673f5de-0b17-42b5-9cdf-88246c1cd6f4"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(19214, 100, 37)\n",
            "(19214,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#https://stackoverflow.com/questions/53731141/cifar10-randomize-train-and-test-set\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_shuffled, y_shuffled, test_size=0.33, random_state=1234)\n",
        "# Check shape\n",
        "print(X_train.shape) \n",
        "print(y_train.shape)\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "67z8auk8inFT",
        "outputId": "10d47e02-02f2-43ef-b404-db99c1b0f36e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(12873, 100, 37)\n",
            "(12873,)\n",
            "(6341, 100, 37)\n",
            "(6341,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, LSTM, Flatten, Input\n",
        "from keras import optimizers, losses, metrics, initializers\n",
        "\n",
        "from collections import Counter\n",
        "NUM_CLASSES = len(Counter(y_shuffled).keys()) # Hardcoded number of classes in the gesture recognition problem\n",
        "BATCH_SIZE = 50 # Batch Size\n",
        "NUM_UNITS_LSTM = 16 # Number of unit in the long short-term recurrent layers\n",
        "NB_SENSOR_CHANNELS = data_x.shape[2]\n",
        "SLIDING_WINDOW_LENGTH = data_x.shape[1]"
      ],
      "metadata": {
        "id": "7iIHarpB2u13"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model._name=\"Experiement1_1LSTM_without_Attention\"\n",
        "model.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2)\n",
        "model.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "id": "0NVDN3J_S9Fj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "de073bfb-146f-43a0-b122-42d6bb7484ce"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement1_1LSTM_without_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_2 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " flatten_2 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 22,668\n",
            "Trainable params: 22,668\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 11s 13ms/step - loss: 1.4186 - sparse_categorical_accuracy: 0.5538 - val_loss: 1.1230 - val_sparse_categorical_accuracy: 0.6342\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9893 - sparse_categorical_accuracy: 0.7056 - val_loss: 0.8632 - val_sparse_categorical_accuracy: 0.7417\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.8170 - sparse_categorical_accuracy: 0.7614 - val_loss: 0.7470 - val_sparse_categorical_accuracy: 0.8000\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.7059 - sparse_categorical_accuracy: 0.7953 - val_loss: 0.7048 - val_sparse_categorical_accuracy: 0.7938\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.6335 - sparse_categorical_accuracy: 0.8139 - val_loss: 0.6686 - val_sparse_categorical_accuracy: 0.8190\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5822 - sparse_categorical_accuracy: 0.8344 - val_loss: 0.6048 - val_sparse_categorical_accuracy: 0.8350\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.5284 - sparse_categorical_accuracy: 0.8459 - val_loss: 0.5608 - val_sparse_categorical_accuracy: 0.8388\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 12ms/step - loss: 0.5013 - sparse_categorical_accuracy: 0.8546 - val_loss: 0.5580 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4767 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.5591 - val_sparse_categorical_accuracy: 0.8384\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4495 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.5095 - val_sparse_categorical_accuracy: 0.8551\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4420 - sparse_categorical_accuracy: 0.8746 - val_loss: 0.5107 - val_sparse_categorical_accuracy: 0.8501\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4212 - sparse_categorical_accuracy: 0.8770 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8614\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4066 - sparse_categorical_accuracy: 0.8836 - val_loss: 0.4674 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3881 - sparse_categorical_accuracy: 0.8865 - val_loss: 0.4859 - val_sparse_categorical_accuracy: 0.8652\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3755 - sparse_categorical_accuracy: 0.8908 - val_loss: 0.4393 - val_sparse_categorical_accuracy: 0.8847\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3719 - sparse_categorical_accuracy: 0.8921 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.8691\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3542 - sparse_categorical_accuracy: 0.8957 - val_loss: 0.4574 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3392 - sparse_categorical_accuracy: 0.9000 - val_loss: 0.4553 - val_sparse_categorical_accuracy: 0.8757\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9010 - val_loss: 0.4194 - val_sparse_categorical_accuracy: 0.8800\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3308 - sparse_categorical_accuracy: 0.9040 - val_loss: 0.4056 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3269 - sparse_categorical_accuracy: 0.9055 - val_loss: 0.4101 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3135 - sparse_categorical_accuracy: 0.9064 - val_loss: 0.3856 - val_sparse_categorical_accuracy: 0.8928\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3088 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.4173 - val_sparse_categorical_accuracy: 0.8878\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.3057 - sparse_categorical_accuracy: 0.9091 - val_loss: 0.4159 - val_sparse_categorical_accuracy: 0.8870\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2966 - sparse_categorical_accuracy: 0.9113 - val_loss: 0.3756 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2948 - sparse_categorical_accuracy: 0.9116 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2876 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.3782 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2818 - sparse_categorical_accuracy: 0.9120 - val_loss: 0.3634 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2722 - sparse_categorical_accuracy: 0.9181 - val_loss: 0.3747 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 8ms/step - loss: 0.2731 - sparse_categorical_accuracy: 0.9180 - val_loss: 0.3801 - val_sparse_categorical_accuracy: 0.8936\n",
            "199/199 [==============================] - 1s 4ms/step - loss: 0.3725 - sparse_categorical_accuracy: 0.8959\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.37246766686439514, 0.8959154486656189]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/ManzhuYu/Code-SpatioTemporalAttention-LSTM-main/blob/main/modelbase.py\n",
        "\n",
        "model_2 = Sequential()\n",
        "model_2._name=\"Experiement2_1LSTM_with_temporal_Attention\"\n",
        "model_2.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_2.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_2.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_2.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_2.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_2.summary()\n",
        "\n",
        "model_2.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_2.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_2.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "9BzwM2gLTSR5",
        "outputId": "0fa58fad-6892-46be-b3b9-763ba75ce82b"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement2_1LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_3 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_3 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,950,668\n",
            "Trainable params: 1,950,668\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 4s 13ms/step - loss: 1.3131 - sparse_categorical_accuracy: 0.5967 - val_loss: 1.0788 - val_sparse_categorical_accuracy: 0.6781\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.9614 - sparse_categorical_accuracy: 0.7121 - val_loss: 0.8401 - val_sparse_categorical_accuracy: 0.7425\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.8328 - sparse_categorical_accuracy: 0.7567 - val_loss: 0.7572 - val_sparse_categorical_accuracy: 0.7922\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.7555 - sparse_categorical_accuracy: 0.7802 - val_loss: 0.6876 - val_sparse_categorical_accuracy: 0.8035\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6970 - sparse_categorical_accuracy: 0.7979 - val_loss: 0.6230 - val_sparse_categorical_accuracy: 0.8346\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6389 - sparse_categorical_accuracy: 0.8160 - val_loss: 0.8146 - val_sparse_categorical_accuracy: 0.7794\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.8212 - val_loss: 0.6371 - val_sparse_categorical_accuracy: 0.8082\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.5718 - sparse_categorical_accuracy: 0.8322 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.8381\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5464 - sparse_categorical_accuracy: 0.8411 - val_loss: 0.6169 - val_sparse_categorical_accuracy: 0.8315\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.5203 - sparse_categorical_accuracy: 0.8496 - val_loss: 0.5373 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 12ms/step - loss: 0.4984 - sparse_categorical_accuracy: 0.8532 - val_loss: 0.4582 - val_sparse_categorical_accuracy: 0.8668\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4821 - sparse_categorical_accuracy: 0.8592 - val_loss: 0.5083 - val_sparse_categorical_accuracy: 0.8520\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4590 - sparse_categorical_accuracy: 0.8629 - val_loss: 0.5593 - val_sparse_categorical_accuracy: 0.8252\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4502 - sparse_categorical_accuracy: 0.8709 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4393 - sparse_categorical_accuracy: 0.8687 - val_loss: 0.5128 - val_sparse_categorical_accuracy: 0.8524\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4235 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.4952 - val_sparse_categorical_accuracy: 0.8462\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4159 - sparse_categorical_accuracy: 0.8765 - val_loss: 0.4308 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4084 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4517 - val_sparse_categorical_accuracy: 0.8699\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3973 - sparse_categorical_accuracy: 0.8826 - val_loss: 0.5063 - val_sparse_categorical_accuracy: 0.8606\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3856 - sparse_categorical_accuracy: 0.8838 - val_loss: 0.4429 - val_sparse_categorical_accuracy: 0.8699\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3753 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.4608 - val_sparse_categorical_accuracy: 0.8548\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3737 - sparse_categorical_accuracy: 0.8889 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.8722\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3688 - sparse_categorical_accuracy: 0.8930 - val_loss: 0.3747 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3585 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.4554 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3438 - sparse_categorical_accuracy: 0.8963 - val_loss: 0.4133 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3431 - sparse_categorical_accuracy: 0.8980 - val_loss: 0.3944 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3409 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.3768 - val_sparse_categorical_accuracy: 0.8909\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3346 - sparse_categorical_accuracy: 0.8981 - val_loss: 0.3503 - val_sparse_categorical_accuracy: 0.8986\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3303 - sparse_categorical_accuracy: 0.9003 - val_loss: 0.3718 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3267 - sparse_categorical_accuracy: 0.8999 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.9068\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.3592 - sparse_categorical_accuracy: 0.9065\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3592246472835541, 0.9064816236495972]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = Sequential()\n",
        "model_3._name=\"Experiement3_1LSTM_with_spatial_Attention\"\n",
        "model_3.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_3.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_3.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_3.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_3.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_3.summary()\n",
        "\n",
        "model_3.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_3.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_3.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "z_oal8obkntv",
        "outputId": "c4289891-bb57-4439-e4a0-b71adc79d211"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement3_1LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_5 (Dense)             (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_4 (LSTM)               (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 30,500\n",
            "Trainable params: 30,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 4s 11ms/step - loss: 1.2637 - sparse_categorical_accuracy: 0.6079 - val_loss: 0.9515 - val_sparse_categorical_accuracy: 0.7052\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.7749 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.6936 - val_sparse_categorical_accuracy: 0.7984\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.5914 - sparse_categorical_accuracy: 0.8385 - val_loss: 0.5706 - val_sparse_categorical_accuracy: 0.8532\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4934 - sparse_categorical_accuracy: 0.8622 - val_loss: 0.5075 - val_sparse_categorical_accuracy: 0.8652\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.4359 - sparse_categorical_accuracy: 0.8796 - val_loss: 0.4649 - val_sparse_categorical_accuracy: 0.8777\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3939 - sparse_categorical_accuracy: 0.8892 - val_loss: 0.4400 - val_sparse_categorical_accuracy: 0.8804\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3572 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3973 - val_sparse_categorical_accuracy: 0.8901\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3356 - sparse_categorical_accuracy: 0.9058 - val_loss: 0.3848 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.3126 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3837 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2918 - sparse_categorical_accuracy: 0.9198 - val_loss: 0.3664 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2776 - sparse_categorical_accuracy: 0.9185 - val_loss: 0.3686 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2637 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.3282 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2508 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.3630 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2389 - sparse_categorical_accuracy: 0.9318 - val_loss: 0.3295 - val_sparse_categorical_accuracy: 0.9115\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2284 - sparse_categorical_accuracy: 0.9337 - val_loss: 0.3493 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2222 - sparse_categorical_accuracy: 0.9350 - val_loss: 0.3378 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2112 - sparse_categorical_accuracy: 0.9379 - val_loss: 0.3196 - val_sparse_categorical_accuracy: 0.9157\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.2044 - sparse_categorical_accuracy: 0.9411 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1968 - sparse_categorical_accuracy: 0.9412 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9165\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1905 - sparse_categorical_accuracy: 0.9420 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1867 - sparse_categorical_accuracy: 0.9466 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9254\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1764 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.3106 - val_sparse_categorical_accuracy: 0.9161\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1730 - sparse_categorical_accuracy: 0.9484 - val_loss: 0.3147 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1667 - sparse_categorical_accuracy: 0.9490 - val_loss: 0.3098 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.1650 - sparse_categorical_accuracy: 0.9513 - val_loss: 0.3602 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1556 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.3261 - val_sparse_categorical_accuracy: 0.9184\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1599 - sparse_categorical_accuracy: 0.9510 - val_loss: 0.2758 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1505 - sparse_categorical_accuracy: 0.9552 - val_loss: 0.2918 - val_sparse_categorical_accuracy: 0.9278\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1486 - sparse_categorical_accuracy: 0.9558 - val_loss: 0.3208 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 9ms/step - loss: 0.1473 - sparse_categorical_accuracy: 0.9571 - val_loss: 0.2979 - val_sparse_categorical_accuracy: 0.9235\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.2828 - sparse_categorical_accuracy: 0.9308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28276267647743225, 0.9307680130004883]"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = Sequential()\n",
        "model_4._name=\"Experiement4_1LSTM_with_spatial_temporal_Attention\"\n",
        "model_4.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_4.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_4.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_4.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_4.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_4.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_4.summary()\n",
        "\n",
        "model_4.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_4.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_4.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "bFDquPOITjLt",
        "outputId": "748df464-c216-4dcb-8cce-0845acf26f69"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement4_1LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_7 (Dense)             (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_5 (LSTM)               (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,958,500\n",
            "Trainable params: 1,958,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 5s 14ms/step - loss: 1.3689 - sparse_categorical_accuracy: 0.5742 - val_loss: 1.0746 - val_sparse_categorical_accuracy: 0.6633\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.8954 - sparse_categorical_accuracy: 0.7327 - val_loss: 0.7793 - val_sparse_categorical_accuracy: 0.7786\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.7279 - sparse_categorical_accuracy: 0.7902 - val_loss: 0.7661 - val_sparse_categorical_accuracy: 0.7988\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.6199 - sparse_categorical_accuracy: 0.8188 - val_loss: 0.6260 - val_sparse_categorical_accuracy: 0.8183\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.5498 - sparse_categorical_accuracy: 0.8397 - val_loss: 0.6316 - val_sparse_categorical_accuracy: 0.8264\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.4950 - sparse_categorical_accuracy: 0.8558 - val_loss: 0.4861 - val_sparse_categorical_accuracy: 0.8672\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.4500 - sparse_categorical_accuracy: 0.8727 - val_loss: 0.4827 - val_sparse_categorical_accuracy: 0.8707\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.4076 - sparse_categorical_accuracy: 0.8815 - val_loss: 0.4632 - val_sparse_categorical_accuracy: 0.8583\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3876 - sparse_categorical_accuracy: 0.8864 - val_loss: 0.5524 - val_sparse_categorical_accuracy: 0.8439\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3623 - sparse_categorical_accuracy: 0.8956 - val_loss: 0.4231 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3350 - sparse_categorical_accuracy: 0.9024 - val_loss: 0.3884 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 2s 10ms/step - loss: 0.3243 - sparse_categorical_accuracy: 0.9027 - val_loss: 0.3367 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.3134 - sparse_categorical_accuracy: 0.9086 - val_loss: 0.3785 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2888 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8897\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2809 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3414 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3500 - val_sparse_categorical_accuracy: 0.9017\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2593 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3490 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2567 - sparse_categorical_accuracy: 0.9226 - val_loss: 0.3549 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2466 - sparse_categorical_accuracy: 0.9229 - val_loss: 0.3746 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2447 - sparse_categorical_accuracy: 0.9256 - val_loss: 0.3569 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.3670 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2281 - sparse_categorical_accuracy: 0.9286 - val_loss: 0.3717 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2219 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.3676 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2125 - sparse_categorical_accuracy: 0.9344 - val_loss: 0.3619 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2197 - sparse_categorical_accuracy: 0.9366 - val_loss: 0.3167 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2097 - sparse_categorical_accuracy: 0.9339 - val_loss: 0.3546 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.2102 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1992 - sparse_categorical_accuracy: 0.9357 - val_loss: 0.3025 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1987 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.3120 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 2s 11ms/step - loss: 0.1936 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.3318 - val_sparse_categorical_accuracy: 0.9080\n",
            "199/199 [==============================] - 1s 5ms/step - loss: 0.3406 - sparse_categorical_accuracy: 0.9159\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3405660092830658, 0.9159438610076904]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5a = Sequential()\n",
        "model_5a._name=\"Experiement5a_2LSTM_with_temporal_Attention\"\n",
        "model_5a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5a.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5a.summary()\n",
        "\n",
        "model_5a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "e3gMzO2LTs-0",
        "outputId": "41c09263-119c-4a5d-a79c-99333aff2315"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5a_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_6 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " lstm_7 (LSTM)               (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " flatten_6 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 153,356\n",
            "Trainable params: 153,356\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.2954 - sparse_categorical_accuracy: 0.5812 - val_loss: 0.9202 - val_sparse_categorical_accuracy: 0.7157\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.7909 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.7619 - val_sparse_categorical_accuracy: 0.7817\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.6198 - sparse_categorical_accuracy: 0.8175 - val_loss: 0.6218 - val_sparse_categorical_accuracy: 0.8144\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5161 - sparse_categorical_accuracy: 0.8456 - val_loss: 0.6163 - val_sparse_categorical_accuracy: 0.8124\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4650 - sparse_categorical_accuracy: 0.8625 - val_loss: 0.4570 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4220 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.4269 - val_sparse_categorical_accuracy: 0.8691\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3835 - sparse_categorical_accuracy: 0.8837 - val_loss: 0.4915 - val_sparse_categorical_accuracy: 0.8466\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3535 - sparse_categorical_accuracy: 0.8923 - val_loss: 0.3863 - val_sparse_categorical_accuracy: 0.8866\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3407 - sparse_categorical_accuracy: 0.8991 - val_loss: 0.3738 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3171 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.3601 - val_sparse_categorical_accuracy: 0.8944\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3103 - sparse_categorical_accuracy: 0.9074 - val_loss: 0.3900 - val_sparse_categorical_accuracy: 0.8882\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2954 - sparse_categorical_accuracy: 0.9090 - val_loss: 0.3553 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2827 - sparse_categorical_accuracy: 0.9148 - val_loss: 0.3416 - val_sparse_categorical_accuracy: 0.8998\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2771 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3411 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2686 - sparse_categorical_accuracy: 0.9192 - val_loss: 0.2949 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2588 - sparse_categorical_accuracy: 0.9215 - val_loss: 0.3307 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2489 - sparse_categorical_accuracy: 0.9216 - val_loss: 0.3142 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2413 - sparse_categorical_accuracy: 0.9222 - val_loss: 0.3322 - val_sparse_categorical_accuracy: 0.9056\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2402 - sparse_categorical_accuracy: 0.9252 - val_loss: 0.2989 - val_sparse_categorical_accuracy: 0.9130\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2282 - sparse_categorical_accuracy: 0.9285 - val_loss: 0.2879 - val_sparse_categorical_accuracy: 0.9134\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2296 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.2854 - val_sparse_categorical_accuracy: 0.9130\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2267 - sparse_categorical_accuracy: 0.9283 - val_loss: 0.3016 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2179 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.2887 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2096 - sparse_categorical_accuracy: 0.9368 - val_loss: 0.3290 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.2670 - val_sparse_categorical_accuracy: 0.9177\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2015 - sparse_categorical_accuracy: 0.9369 - val_loss: 0.2909 - val_sparse_categorical_accuracy: 0.9212\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2052 - sparse_categorical_accuracy: 0.9355 - val_loss: 0.2885 - val_sparse_categorical_accuracy: 0.9177\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9385 - val_loss: 0.3191 - val_sparse_categorical_accuracy: 0.9064\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 4s 19ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9386 - val_loss: 0.3790 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 4s 18ms/step - loss: 0.1906 - sparse_categorical_accuracy: 0.9406 - val_loss: 0.2698 - val_sparse_categorical_accuracy: 0.9200\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2684 - sparse_categorical_accuracy: 0.9308\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.26835426688194275, 0.9307680130004883]"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5b = Sequential()\n",
        "model_5b._name=\"Experiement5b_2LSTM_with_temporal_Attention\"\n",
        "model_5b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5b.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5b.summary()\n",
        "\n",
        "model_5b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "U-KradvolxlI",
        "outputId": "4b7f5056-974e-4b0b-d168-380e99d40aad"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5b_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_8 (LSTM)               (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_9 (LSTM)               (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_12 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_7 (Flatten)         (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_13 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,952,780\n",
            "Trainable params: 1,952,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.4785 - sparse_categorical_accuracy: 0.5314 - val_loss: 1.0519 - val_sparse_categorical_accuracy: 0.6777\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 1.0453 - sparse_categorical_accuracy: 0.6800 - val_loss: 0.9701 - val_sparse_categorical_accuracy: 0.7029\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8752 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.8002 - val_sparse_categorical_accuracy: 0.7526\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.7646 - sparse_categorical_accuracy: 0.7710 - val_loss: 0.6411 - val_sparse_categorical_accuracy: 0.8132\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.8010 - val_loss: 0.6016 - val_sparse_categorical_accuracy: 0.8404\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6184 - sparse_categorical_accuracy: 0.8221 - val_loss: 0.5831 - val_sparse_categorical_accuracy: 0.8280\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5787 - sparse_categorical_accuracy: 0.8282 - val_loss: 0.5602 - val_sparse_categorical_accuracy: 0.8373\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5440 - sparse_categorical_accuracy: 0.8382 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5076 - sparse_categorical_accuracy: 0.8534 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4915 - sparse_categorical_accuracy: 0.8554 - val_loss: 0.5227 - val_sparse_categorical_accuracy: 0.8485\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4737 - sparse_categorical_accuracy: 0.8612 - val_loss: 0.4970 - val_sparse_categorical_accuracy: 0.8602\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4569 - sparse_categorical_accuracy: 0.8619 - val_loss: 0.5135 - val_sparse_categorical_accuracy: 0.8548\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4380 - sparse_categorical_accuracy: 0.8708 - val_loss: 0.5278 - val_sparse_categorical_accuracy: 0.8485\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4291 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.8773\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4169 - sparse_categorical_accuracy: 0.8751 - val_loss: 0.4683 - val_sparse_categorical_accuracy: 0.8641\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4093 - sparse_categorical_accuracy: 0.8757 - val_loss: 0.4611 - val_sparse_categorical_accuracy: 0.8769\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3927 - sparse_categorical_accuracy: 0.8856 - val_loss: 0.4262 - val_sparse_categorical_accuracy: 0.8835\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3830 - sparse_categorical_accuracy: 0.8879 - val_loss: 0.4304 - val_sparse_categorical_accuracy: 0.8812\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3729 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.3883 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3679 - sparse_categorical_accuracy: 0.8909 - val_loss: 0.4378 - val_sparse_categorical_accuracy: 0.8773\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3601 - sparse_categorical_accuracy: 0.8912 - val_loss: 0.4742 - val_sparse_categorical_accuracy: 0.8695\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3544 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3832 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3487 - sparse_categorical_accuracy: 0.8953 - val_loss: 0.3626 - val_sparse_categorical_accuracy: 0.8948\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3451 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.4241 - val_sparse_categorical_accuracy: 0.8656\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3378 - sparse_categorical_accuracy: 0.8943 - val_loss: 0.4371 - val_sparse_categorical_accuracy: 0.8656\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3258 - sparse_categorical_accuracy: 0.9020 - val_loss: 0.3894 - val_sparse_categorical_accuracy: 0.8878\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3255 - sparse_categorical_accuracy: 0.9019 - val_loss: 0.4463 - val_sparse_categorical_accuracy: 0.8761\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3066 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3640 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3122 - sparse_categorical_accuracy: 0.9033 - val_loss: 0.4014 - val_sparse_categorical_accuracy: 0.8885\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3032 - sparse_categorical_accuracy: 0.9095 - val_loss: 0.3701 - val_sparse_categorical_accuracy: 0.8975\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3612 - sparse_categorical_accuracy: 0.9029\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.36121025681495667, 0.9028544425964355]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_5c = Sequential()\n",
        "model_5c._name=\"Experiement5c_2LSTM_with_temporal_Attention\"\n",
        "model_5c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_5c.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_5c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_5c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_5c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_5c.summary()\n",
        "\n",
        "model_5c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_5c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_5c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "rkhgkmJLmoei",
        "outputId": "b9b6b378-4f30-45e1-9869-389f73de5fc2"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement5c_2LSTM_with_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_14 (Dense)            (None, 100, 1600)         60800     \n",
            "                                                                 \n",
            " lstm_10 (LSTM)              (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " lstm_11 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_8 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_15 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 185,612\n",
            "Trainable params: 185,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 20ms/step - loss: 1.2001 - sparse_categorical_accuracy: 0.6137 - val_loss: 0.7411 - val_sparse_categorical_accuracy: 0.7744\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6075 - sparse_categorical_accuracy: 0.8238 - val_loss: 0.5414 - val_sparse_categorical_accuracy: 0.8482\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4529 - sparse_categorical_accuracy: 0.8715 - val_loss: 0.4984 - val_sparse_categorical_accuracy: 0.8633\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3783 - sparse_categorical_accuracy: 0.8922 - val_loss: 0.3838 - val_sparse_categorical_accuracy: 0.8905\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3332 - sparse_categorical_accuracy: 0.9015 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2980 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3511 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2799 - sparse_categorical_accuracy: 0.9164 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2569 - sparse_categorical_accuracy: 0.9233 - val_loss: 0.2895 - val_sparse_categorical_accuracy: 0.9192\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2395 - sparse_categorical_accuracy: 0.9276 - val_loss: 0.2659 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2288 - sparse_categorical_accuracy: 0.9320 - val_loss: 0.2611 - val_sparse_categorical_accuracy: 0.9243\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2173 - sparse_categorical_accuracy: 0.9334 - val_loss: 0.2760 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2053 - sparse_categorical_accuracy: 0.9378 - val_loss: 0.5238 - val_sparse_categorical_accuracy: 0.8707\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1969 - sparse_categorical_accuracy: 0.9397 - val_loss: 0.2473 - val_sparse_categorical_accuracy: 0.9309\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1915 - sparse_categorical_accuracy: 0.9416 - val_loss: 0.2352 - val_sparse_categorical_accuracy: 0.9332\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1810 - sparse_categorical_accuracy: 0.9457 - val_loss: 0.2710 - val_sparse_categorical_accuracy: 0.9134\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1702 - sparse_categorical_accuracy: 0.9480 - val_loss: 0.2465 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1659 - sparse_categorical_accuracy: 0.9476 - val_loss: 0.2559 - val_sparse_categorical_accuracy: 0.9278\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1628 - sparse_categorical_accuracy: 0.9483 - val_loss: 0.2238 - val_sparse_categorical_accuracy: 0.9301\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1555 - sparse_categorical_accuracy: 0.9512 - val_loss: 0.2644 - val_sparse_categorical_accuracy: 0.9258\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1557 - sparse_categorical_accuracy: 0.9519 - val_loss: 0.2587 - val_sparse_categorical_accuracy: 0.9274\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1437 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.2423 - val_sparse_categorical_accuracy: 0.9344\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1430 - sparse_categorical_accuracy: 0.9527 - val_loss: 0.2132 - val_sparse_categorical_accuracy: 0.9433\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1361 - sparse_categorical_accuracy: 0.9567 - val_loss: 0.2321 - val_sparse_categorical_accuracy: 0.9402\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1308 - sparse_categorical_accuracy: 0.9589 - val_loss: 0.2850 - val_sparse_categorical_accuracy: 0.9196\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1274 - sparse_categorical_accuracy: 0.9591 - val_loss: 0.2416 - val_sparse_categorical_accuracy: 0.9340\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1263 - sparse_categorical_accuracy: 0.9591 - val_loss: 0.2679 - val_sparse_categorical_accuracy: 0.9305\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1245 - sparse_categorical_accuracy: 0.9601 - val_loss: 0.2495 - val_sparse_categorical_accuracy: 0.9313\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1267 - sparse_categorical_accuracy: 0.9594 - val_loss: 0.2270 - val_sparse_categorical_accuracy: 0.9390\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1223 - sparse_categorical_accuracy: 0.9628 - val_loss: 0.3091 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1238 - sparse_categorical_accuracy: 0.9616 - val_loss: 0.2737 - val_sparse_categorical_accuracy: 0.9184\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2969 - sparse_categorical_accuracy: 0.9111\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.29685431718826294, 0.9110550284385681]"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6a = Sequential()\n",
        "model_6a._name=\"Experiement6a_2LSTM_with_spatial_Attention\"\n",
        "model_6a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6a.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6a.summary()\n",
        "\n",
        "model_6a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "oUrucUvKVIVv",
        "outputId": "4b418f07-be12-4216-a067-4fe09d783742"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6a_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_12 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_16 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " lstm_13 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " flatten_9 (Flatten)         (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_17 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 31,856\n",
            "Trainable params: 31,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 18ms/step - loss: 1.4279 - sparse_categorical_accuracy: 0.5421 - val_loss: 1.0280 - val_sparse_categorical_accuracy: 0.6668\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.9410 - sparse_categorical_accuracy: 0.7057 - val_loss: 0.8029 - val_sparse_categorical_accuracy: 0.7363\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.7538 - sparse_categorical_accuracy: 0.7695 - val_loss: 0.6370 - val_sparse_categorical_accuracy: 0.8019\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6397 - sparse_categorical_accuracy: 0.8087 - val_loss: 0.5499 - val_sparse_categorical_accuracy: 0.8435\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5562 - sparse_categorical_accuracy: 0.8371 - val_loss: 0.4927 - val_sparse_categorical_accuracy: 0.8559\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5008 - sparse_categorical_accuracy: 0.8515 - val_loss: 0.4932 - val_sparse_categorical_accuracy: 0.8540\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4681 - sparse_categorical_accuracy: 0.8616 - val_loss: 0.4448 - val_sparse_categorical_accuracy: 0.8753\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4305 - sparse_categorical_accuracy: 0.8731 - val_loss: 0.4411 - val_sparse_categorical_accuracy: 0.8683\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4128 - sparse_categorical_accuracy: 0.8777 - val_loss: 0.4443 - val_sparse_categorical_accuracy: 0.8583\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3839 - sparse_categorical_accuracy: 0.8858 - val_loss: 0.4673 - val_sparse_categorical_accuracy: 0.8683\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3691 - sparse_categorical_accuracy: 0.8902 - val_loss: 0.3781 - val_sparse_categorical_accuracy: 0.8889\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3584 - sparse_categorical_accuracy: 0.8918 - val_loss: 0.3589 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3353 - sparse_categorical_accuracy: 0.8949 - val_loss: 0.3585 - val_sparse_categorical_accuracy: 0.8909\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3261 - sparse_categorical_accuracy: 0.9014 - val_loss: 0.3314 - val_sparse_categorical_accuracy: 0.9083\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3129 - sparse_categorical_accuracy: 0.9052 - val_loss: 0.3699 - val_sparse_categorical_accuracy: 0.8917\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3114 - sparse_categorical_accuracy: 0.9062 - val_loss: 0.3656 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2956 - sparse_categorical_accuracy: 0.9107 - val_loss: 0.3494 - val_sparse_categorical_accuracy: 0.8967\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2861 - sparse_categorical_accuracy: 0.9111 - val_loss: 0.3394 - val_sparse_categorical_accuracy: 0.8990\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2759 - sparse_categorical_accuracy: 0.9169 - val_loss: 0.3127 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2745 - sparse_categorical_accuracy: 0.9161 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.9010\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2685 - sparse_categorical_accuracy: 0.9188 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2652 - sparse_categorical_accuracy: 0.9158 - val_loss: 0.3067 - val_sparse_categorical_accuracy: 0.9083\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2548 - sparse_categorical_accuracy: 0.9209 - val_loss: 0.3075 - val_sparse_categorical_accuracy: 0.9052\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2495 - sparse_categorical_accuracy: 0.9245 - val_loss: 0.2947 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2488 - sparse_categorical_accuracy: 0.9212 - val_loss: 0.3027 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2415 - sparse_categorical_accuracy: 0.9247 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2440 - sparse_categorical_accuracy: 0.9228 - val_loss: 0.3254 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2313 - sparse_categorical_accuracy: 0.9290 - val_loss: 0.3220 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2257 - sparse_categorical_accuracy: 0.9308 - val_loss: 0.3053 - val_sparse_categorical_accuracy: 0.9107\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2249 - sparse_categorical_accuracy: 0.9312 - val_loss: 0.2995 - val_sparse_categorical_accuracy: 0.9126\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3148 - sparse_categorical_accuracy: 0.9191\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3147575259208679, 0.9190979599952698]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6b = Sequential()\n",
        "model_6b._name=\"Experiement6b_2LSTM_with_spatial_Attention\"\n",
        "model_6b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6b.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6b.summary()\n",
        "\n",
        "model_6b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "X8LFx9armQNL",
        "outputId": "85ad3836-0590-44b9-9f18-558a016b475e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6b_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_14 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_15 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_18 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " flatten_10 (Flatten)        (None, 10000)             0         \n",
            "                                                                 \n",
            " dense_19 (Dense)            (None, 12)                120012    \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 127,280\n",
            "Trainable params: 127,280\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 17ms/step - loss: 1.4094 - sparse_categorical_accuracy: 0.5563 - val_loss: 1.0765 - val_sparse_categorical_accuracy: 0.6649\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.9758 - sparse_categorical_accuracy: 0.7008 - val_loss: 0.7963 - val_sparse_categorical_accuracy: 0.7573\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.8050 - sparse_categorical_accuracy: 0.7515 - val_loss: 0.8007 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6942 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.7284 - val_sparse_categorical_accuracy: 0.7794\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.6308 - sparse_categorical_accuracy: 0.8048 - val_loss: 0.6184 - val_sparse_categorical_accuracy: 0.8206\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5895 - sparse_categorical_accuracy: 0.8181 - val_loss: 0.6628 - val_sparse_categorical_accuracy: 0.8155\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5381 - sparse_categorical_accuracy: 0.8333 - val_loss: 0.4733 - val_sparse_categorical_accuracy: 0.8633\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5047 - sparse_categorical_accuracy: 0.8453 - val_loss: 0.5065 - val_sparse_categorical_accuracy: 0.8513\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4759 - sparse_categorical_accuracy: 0.8528 - val_loss: 0.5268 - val_sparse_categorical_accuracy: 0.8536\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4490 - sparse_categorical_accuracy: 0.8650 - val_loss: 0.4923 - val_sparse_categorical_accuracy: 0.8660\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4412 - sparse_categorical_accuracy: 0.8670 - val_loss: 0.4593 - val_sparse_categorical_accuracy: 0.8637\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4174 - sparse_categorical_accuracy: 0.8724 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8664\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4125 - sparse_categorical_accuracy: 0.8750 - val_loss: 0.4161 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3937 - sparse_categorical_accuracy: 0.8788 - val_loss: 0.4050 - val_sparse_categorical_accuracy: 0.8730\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3728 - sparse_categorical_accuracy: 0.8842 - val_loss: 0.4454 - val_sparse_categorical_accuracy: 0.8734\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3672 - sparse_categorical_accuracy: 0.8860 - val_loss: 0.3577 - val_sparse_categorical_accuracy: 0.9017\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3550 - sparse_categorical_accuracy: 0.8920 - val_loss: 0.3595 - val_sparse_categorical_accuracy: 0.9045\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3473 - sparse_categorical_accuracy: 0.8919 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.8994\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3369 - sparse_categorical_accuracy: 0.8969 - val_loss: 0.4083 - val_sparse_categorical_accuracy: 0.8800\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3337 - sparse_categorical_accuracy: 0.8958 - val_loss: 0.3646 - val_sparse_categorical_accuracy: 0.8963\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3210 - sparse_categorical_accuracy: 0.8992 - val_loss: 0.3846 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3123 - sparse_categorical_accuracy: 0.9050 - val_loss: 0.3784 - val_sparse_categorical_accuracy: 0.9014\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3071 - sparse_categorical_accuracy: 0.9032 - val_loss: 0.3412 - val_sparse_categorical_accuracy: 0.8983\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2985 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3774 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3061 - sparse_categorical_accuracy: 0.9053 - val_loss: 0.3237 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2902 - sparse_categorical_accuracy: 0.9103 - val_loss: 0.4052 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2866 - sparse_categorical_accuracy: 0.9104 - val_loss: 0.3492 - val_sparse_categorical_accuracy: 0.9064\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2752 - sparse_categorical_accuracy: 0.9150 - val_loss: 0.3021 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2749 - sparse_categorical_accuracy: 0.9119 - val_loss: 0.3350 - val_sparse_categorical_accuracy: 0.9068\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2666 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.9157\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3046 - sparse_categorical_accuracy: 0.9123\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.304587721824646, 0.9123166799545288]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_6c = Sequential()\n",
        "model_6c._name=\"Experiement6c_2LSTM_with_spatial_Attention\"\n",
        "model_6c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_6c.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_6c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_6c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_6c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_6c.summary()\n",
        "\n",
        "model_6c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_6c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_6c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "VzqGdGRHmxDg",
        "outputId": "8b3b32d6-bcd6-4db5-8e73-ca2be415e8c0"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement6c_2LSTM_with_spatial_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_20 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_16 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " lstm_17 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_11 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_21 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 32,612\n",
            "Trainable params: 32,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 17ms/step - loss: 1.3470 - sparse_categorical_accuracy: 0.5654 - val_loss: 0.9629 - val_sparse_categorical_accuracy: 0.7014\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8659 - sparse_categorical_accuracy: 0.7326 - val_loss: 0.7576 - val_sparse_categorical_accuracy: 0.7740\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 4s 17ms/step - loss: 0.6831 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.6295 - val_sparse_categorical_accuracy: 0.8144\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5785 - sparse_categorical_accuracy: 0.8393 - val_loss: 0.5307 - val_sparse_categorical_accuracy: 0.8509\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.5028 - sparse_categorical_accuracy: 0.8609 - val_loss: 0.4906 - val_sparse_categorical_accuracy: 0.8664\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.4375 - sparse_categorical_accuracy: 0.8754 - val_loss: 0.4287 - val_sparse_categorical_accuracy: 0.8722\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3952 - sparse_categorical_accuracy: 0.8885 - val_loss: 0.4524 - val_sparse_categorical_accuracy: 0.8645\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3723 - sparse_categorical_accuracy: 0.8928 - val_loss: 0.3703 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3428 - sparse_categorical_accuracy: 0.8993 - val_loss: 0.3566 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3253 - sparse_categorical_accuracy: 0.9041 - val_loss: 0.3751 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.3050 - sparse_categorical_accuracy: 0.9137 - val_loss: 0.3561 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2959 - sparse_categorical_accuracy: 0.9122 - val_loss: 0.3767 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2843 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3302 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2740 - sparse_categorical_accuracy: 0.9165 - val_loss: 0.3271 - val_sparse_categorical_accuracy: 0.9068\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2620 - sparse_categorical_accuracy: 0.9194 - val_loss: 0.3134 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2544 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3072 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2456 - sparse_categorical_accuracy: 0.9243 - val_loss: 0.3221 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2380 - sparse_categorical_accuracy: 0.9280 - val_loss: 0.3283 - val_sparse_categorical_accuracy: 0.9045\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2270 - sparse_categorical_accuracy: 0.9313 - val_loss: 0.2994 - val_sparse_categorical_accuracy: 0.9150\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2274 - sparse_categorical_accuracy: 0.9299 - val_loss: 0.3040 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2143 - sparse_categorical_accuracy: 0.9341 - val_loss: 0.3511 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2111 - sparse_categorical_accuracy: 0.9343 - val_loss: 0.2707 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2046 - sparse_categorical_accuracy: 0.9377 - val_loss: 0.2757 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.2018 - sparse_categorical_accuracy: 0.9364 - val_loss: 0.3296 - val_sparse_categorical_accuracy: 0.9076\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1940 - sparse_categorical_accuracy: 0.9373 - val_loss: 0.3060 - val_sparse_categorical_accuracy: 0.9126\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1897 - sparse_categorical_accuracy: 0.9398 - val_loss: 0.2743 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1903 - sparse_categorical_accuracy: 0.9402 - val_loss: 0.2777 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1857 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.2885 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 14ms/step - loss: 0.1792 - sparse_categorical_accuracy: 0.9446 - val_loss: 0.2869 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.1806 - sparse_categorical_accuracy: 0.9425 - val_loss: 0.2754 - val_sparse_categorical_accuracy: 0.9239\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3044 - sparse_categorical_accuracy: 0.9241\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3044148087501526, 0.924144446849823]"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7a = Sequential()\n",
        "model_7a._name=\"Experiement7a_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7a.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7a.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7a.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7a.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "\n",
        "model_7a.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7a.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7a.summary()\n",
        "\n",
        "model_7a.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7a.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7a.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "d0mKCC0ulmYf",
        "outputId": "c96ad5b8-be3d-4c88-cb60-ded7df3dec16"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7a_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_22 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_18 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " lstm_19 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " dense_23 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_12 (Flatten)        (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_24 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,960,612\n",
            "Trainable params: 1,960,612\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 19ms/step - loss: 1.4059 - sparse_categorical_accuracy: 0.5514 - val_loss: 1.3026 - val_sparse_categorical_accuracy: 0.6027\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.9017 - sparse_categorical_accuracy: 0.7304 - val_loss: 0.7770 - val_sparse_categorical_accuracy: 0.7658\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.7303 - sparse_categorical_accuracy: 0.7850 - val_loss: 0.5881 - val_sparse_categorical_accuracy: 0.8392\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.6226 - sparse_categorical_accuracy: 0.8184 - val_loss: 0.5775 - val_sparse_categorical_accuracy: 0.8482\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5407 - sparse_categorical_accuracy: 0.8418 - val_loss: 0.5272 - val_sparse_categorical_accuracy: 0.8497\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4929 - sparse_categorical_accuracy: 0.8560 - val_loss: 0.4604 - val_sparse_categorical_accuracy: 0.8695\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4510 - sparse_categorical_accuracy: 0.8685 - val_loss: 0.5798 - val_sparse_categorical_accuracy: 0.8291\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4209 - sparse_categorical_accuracy: 0.8762 - val_loss: 0.4870 - val_sparse_categorical_accuracy: 0.8680\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3938 - sparse_categorical_accuracy: 0.8855 - val_loss: 0.3994 - val_sparse_categorical_accuracy: 0.8858\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3771 - sparse_categorical_accuracy: 0.8871 - val_loss: 0.3681 - val_sparse_categorical_accuracy: 0.9021\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3554 - sparse_categorical_accuracy: 0.8947 - val_loss: 0.4686 - val_sparse_categorical_accuracy: 0.8555\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3353 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.4074 - val_sparse_categorical_accuracy: 0.8932\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3221 - sparse_categorical_accuracy: 0.9036 - val_loss: 0.3431 - val_sparse_categorical_accuracy: 0.9025\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3145 - sparse_categorical_accuracy: 0.9077 - val_loss: 0.3216 - val_sparse_categorical_accuracy: 0.9111\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2961 - sparse_categorical_accuracy: 0.9098 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3079 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2727 - sparse_categorical_accuracy: 0.9186 - val_loss: 0.3182 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2623 - sparse_categorical_accuracy: 0.9195 - val_loss: 0.3168 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2612 - sparse_categorical_accuracy: 0.9207 - val_loss: 0.3263 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2529 - sparse_categorical_accuracy: 0.9213 - val_loss: 0.2894 - val_sparse_categorical_accuracy: 0.9270\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2467 - sparse_categorical_accuracy: 0.9264 - val_loss: 0.3044 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2399 - sparse_categorical_accuracy: 0.9260 - val_loss: 0.3320 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2268 - sparse_categorical_accuracy: 0.9288 - val_loss: 0.2994 - val_sparse_categorical_accuracy: 0.9235\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2283 - sparse_categorical_accuracy: 0.9317 - val_loss: 0.2741 - val_sparse_categorical_accuracy: 0.9250\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2277 - sparse_categorical_accuracy: 0.9298 - val_loss: 0.3128 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2200 - sparse_categorical_accuracy: 0.9311 - val_loss: 0.3092 - val_sparse_categorical_accuracy: 0.9204\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2141 - sparse_categorical_accuracy: 0.9352 - val_loss: 0.3326 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2165 - sparse_categorical_accuracy: 0.9346 - val_loss: 0.2750 - val_sparse_categorical_accuracy: 0.9328\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1974 - sparse_categorical_accuracy: 0.9367 - val_loss: 0.2873 - val_sparse_categorical_accuracy: 0.9266\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.1999 - sparse_categorical_accuracy: 0.9367 - val_loss: 0.2782 - val_sparse_categorical_accuracy: 0.9254\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.9230\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.30051830410957336, 0.923040509223938]"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7b = Sequential()\n",
        "model_7b._name=\"Experiement7b_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7b.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7b.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7b.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7b.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "\n",
        "model_7b.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7b.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7b.summary()\n",
        "\n",
        "model_7b.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7b.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7b.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "A-aARh2Qm56G",
        "outputId": "d93799fb-04a0-42d2-fd2a-fd80a520d358"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7b_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_20 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " dense_25 (Dense)            (None, 100, 100)          1700      \n",
            "                                                                 \n",
            " lstm_21 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_26 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " flatten_13 (Flatten)        (None, 160000)            0         \n",
            "                                                                 \n",
            " dense_27 (Dense)            (None, 12)                1920012   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1,959,856\n",
            "Trainable params: 1,959,856\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 7s 19ms/step - loss: 1.5433 - sparse_categorical_accuracy: 0.5096 - val_loss: 1.1725 - val_sparse_categorical_accuracy: 0.6151\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.0232 - sparse_categorical_accuracy: 0.6791 - val_loss: 0.8839 - val_sparse_categorical_accuracy: 0.7258\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.8172 - sparse_categorical_accuracy: 0.7530 - val_loss: 0.7188 - val_sparse_categorical_accuracy: 0.7969\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6994 - sparse_categorical_accuracy: 0.7836 - val_loss: 0.5954 - val_sparse_categorical_accuracy: 0.8353\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.6372 - sparse_categorical_accuracy: 0.8071 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.8384\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5805 - sparse_categorical_accuracy: 0.8260 - val_loss: 0.5879 - val_sparse_categorical_accuracy: 0.8334\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5402 - sparse_categorical_accuracy: 0.8407 - val_loss: 0.4692 - val_sparse_categorical_accuracy: 0.8703\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.5014 - sparse_categorical_accuracy: 0.8486 - val_loss: 0.5269 - val_sparse_categorical_accuracy: 0.8377\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4772 - sparse_categorical_accuracy: 0.8584 - val_loss: 0.4714 - val_sparse_categorical_accuracy: 0.8594\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4563 - sparse_categorical_accuracy: 0.8646 - val_loss: 0.4307 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4345 - sparse_categorical_accuracy: 0.8679 - val_loss: 0.3920 - val_sparse_categorical_accuracy: 0.8757\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.4102 - sparse_categorical_accuracy: 0.8708 - val_loss: 0.4211 - val_sparse_categorical_accuracy: 0.8850\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3963 - sparse_categorical_accuracy: 0.8802 - val_loss: 0.4054 - val_sparse_categorical_accuracy: 0.8765\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3807 - sparse_categorical_accuracy: 0.8822 - val_loss: 0.3866 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8899 - val_loss: 0.3901 - val_sparse_categorical_accuracy: 0.8936\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3525 - sparse_categorical_accuracy: 0.8961 - val_loss: 0.3657 - val_sparse_categorical_accuracy: 0.8843\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3415 - sparse_categorical_accuracy: 0.8975 - val_loss: 0.3906 - val_sparse_categorical_accuracy: 0.8874\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3339 - sparse_categorical_accuracy: 0.8985 - val_loss: 0.3687 - val_sparse_categorical_accuracy: 0.8948\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3281 - sparse_categorical_accuracy: 0.8983 - val_loss: 0.3229 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3128 - sparse_categorical_accuracy: 0.9048 - val_loss: 0.3474 - val_sparse_categorical_accuracy: 0.9014\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.3155 - sparse_categorical_accuracy: 0.9047 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2975 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3488 - val_sparse_categorical_accuracy: 0.8971\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2977 - sparse_categorical_accuracy: 0.9075 - val_loss: 0.2899 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2960 - sparse_categorical_accuracy: 0.9093 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2838 - sparse_categorical_accuracy: 0.9130 - val_loss: 0.3135 - val_sparse_categorical_accuracy: 0.9115\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2829 - sparse_categorical_accuracy: 0.9144 - val_loss: 0.3440 - val_sparse_categorical_accuracy: 0.9087\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2747 - sparse_categorical_accuracy: 0.9144 - val_loss: 0.3346 - val_sparse_categorical_accuracy: 0.9099\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2697 - sparse_categorical_accuracy: 0.9163 - val_loss: 0.3287 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2677 - sparse_categorical_accuracy: 0.9166 - val_loss: 0.3078 - val_sparse_categorical_accuracy: 0.9146\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2632 - sparse_categorical_accuracy: 0.9184 - val_loss: 0.2734 - val_sparse_categorical_accuracy: 0.9219\n",
            "199/199 [==============================] - 2s 8ms/step - loss: 0.2863 - sparse_categorical_accuracy: 0.9268\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.2862699627876282, 0.9268254041671753]"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7c = Sequential()\n",
        "model_7c._name=\"Experiement7c_2LSTM_with_spatial_temporal_Attention\"\n",
        "model_7c.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7c.add(Dense(SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # spatial module\n",
        "model_7c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7c.add(Dense(NUM_UNITS_LSTM*SLIDING_WINDOW_LENGTH, input_shape=(SLIDING_WINDOW_LENGTH,), activation=None)) # temporal module\n",
        "model_7c.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7c.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7c.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7c.summary()\n",
        "\n",
        "model_7c.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7c.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7c.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "NdWJNBDynDOa",
        "outputId": "62092723-5711-459c-a754-50d839808b80"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7c_2LSTM_with_spatial_temporal_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dense_31 (Dense)            (None, 100, 100)          3800      \n",
            "                                                                 \n",
            " lstm_24 (LSTM)              (None, 100, 16)           7488      \n",
            "                                                                 \n",
            " dense_32 (Dense)            (None, 100, 1600)         27200     \n",
            "                                                                 \n",
            " lstm_25 (LSTM)              (None, 100, 16)           103488    \n",
            "                                                                 \n",
            " flatten_15 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_33 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 161,188\n",
            "Trainable params: 161,188\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 8s 21ms/step - loss: 1.3187 - sparse_categorical_accuracy: 0.5608 - val_loss: 0.8294 - val_sparse_categorical_accuracy: 0.7313\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.7471 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.6162 - val_sparse_categorical_accuracy: 0.8136\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.5494 - sparse_categorical_accuracy: 0.8388 - val_loss: 0.5155 - val_sparse_categorical_accuracy: 0.8458\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.4440 - sparse_categorical_accuracy: 0.8756 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.8672\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3933 - sparse_categorical_accuracy: 0.8852 - val_loss: 0.4170 - val_sparse_categorical_accuracy: 0.8742\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.3454 - sparse_categorical_accuracy: 0.9018 - val_loss: 0.3712 - val_sparse_categorical_accuracy: 0.8893\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.9025 - val_loss: 0.3389 - val_sparse_categorical_accuracy: 0.9037\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.2936 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.4057 - val_sparse_categorical_accuracy: 0.8936\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2787 - sparse_categorical_accuracy: 0.9176 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8920\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2598 - sparse_categorical_accuracy: 0.9208 - val_loss: 0.3204 - val_sparse_categorical_accuracy: 0.9103\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2405 - sparse_categorical_accuracy: 0.9294 - val_loss: 0.3270 - val_sparse_categorical_accuracy: 0.9142\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2335 - sparse_categorical_accuracy: 0.9305 - val_loss: 0.3473 - val_sparse_categorical_accuracy: 0.9033\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2191 - sparse_categorical_accuracy: 0.9322 - val_loss: 0.3429 - val_sparse_categorical_accuracy: 0.9095\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2085 - sparse_categorical_accuracy: 0.9351 - val_loss: 0.3125 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1966 - sparse_categorical_accuracy: 0.9401 - val_loss: 0.3080 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1913 - sparse_categorical_accuracy: 0.9413 - val_loss: 0.3336 - val_sparse_categorical_accuracy: 0.9080\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1811 - sparse_categorical_accuracy: 0.9454 - val_loss: 0.3059 - val_sparse_categorical_accuracy: 0.9169\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1725 - sparse_categorical_accuracy: 0.9451 - val_loss: 0.3003 - val_sparse_categorical_accuracy: 0.9153\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 17ms/step - loss: 0.1700 - sparse_categorical_accuracy: 0.9464 - val_loss: 0.3107 - val_sparse_categorical_accuracy: 0.9122\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1609 - sparse_categorical_accuracy: 0.9495 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9216\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1546 - sparse_categorical_accuracy: 0.9535 - val_loss: 0.2883 - val_sparse_categorical_accuracy: 0.9247\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1498 - sparse_categorical_accuracy: 0.9529 - val_loss: 0.2871 - val_sparse_categorical_accuracy: 0.9293\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1455 - sparse_categorical_accuracy: 0.9543 - val_loss: 0.2970 - val_sparse_categorical_accuracy: 0.9208\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 4s 17ms/step - loss: 0.1418 - sparse_categorical_accuracy: 0.9563 - val_loss: 0.2959 - val_sparse_categorical_accuracy: 0.9289\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1385 - sparse_categorical_accuracy: 0.9580 - val_loss: 0.3310 - val_sparse_categorical_accuracy: 0.9173\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1380 - sparse_categorical_accuracy: 0.9576 - val_loss: 0.4237 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1323 - sparse_categorical_accuracy: 0.9581 - val_loss: 0.3115 - val_sparse_categorical_accuracy: 0.9223\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1255 - sparse_categorical_accuracy: 0.9619 - val_loss: 0.2874 - val_sparse_categorical_accuracy: 0.9293\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1260 - sparse_categorical_accuracy: 0.9615 - val_loss: 0.3121 - val_sparse_categorical_accuracy: 0.9200\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.1302 - sparse_categorical_accuracy: 0.9593 - val_loss: 0.2997 - val_sparse_categorical_accuracy: 0.9274\n",
            "199/199 [==============================] - 1s 7ms/step - loss: 0.2899 - sparse_categorical_accuracy: 0.9317\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.28986841440200806, 0.9317142367362976]"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_7d = Sequential()\n",
        "model_7d._name=\"Experiement7d_2LSTM_without_Attention\"\n",
        "model_7d.add(Input(shape=(SLIDING_WINDOW_LENGTH, NB_SENSOR_CHANNELS)))\n",
        "\n",
        "#intializing weights\n",
        "initializer = initializers.Orthogonal()\n",
        "model_7d.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7d.add(LSTM(NUM_UNITS_LSTM, dropout=0.5,return_sequences=True,kernel_initializer = initializer))\n",
        "model_7d.add(Flatten())\n",
        "\n",
        "#Applying a dense layer of softmax.\n",
        "model_7d.add(Dense(NUM_CLASSES))\n",
        "\n",
        "model_7d.summary()\n",
        "\n",
        "model_7d.compile(\n",
        "    optimizer=keras.optimizers.RMSprop(),\n",
        "    loss=losses.SparseCategoricalCrossentropy(from_logits=True),\n",
        "    metrics=[metrics.SparseCategoricalAccuracy()],\n",
        ")\n",
        "\n",
        "model_7d.fit(X_train, y_train, batch_size=BATCH_SIZE, epochs=30,validation_split=0.2) \n",
        "model_7d.evaluate(X_test, y_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "PWR8jJ0_ynSW",
        "outputId": "9f58e07a-ca46-498f-efdf-c09b30739a2a"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"Experiement7d_2LSTM_without_Attention\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm_26 (LSTM)              (None, 100, 16)           3456      \n",
            "                                                                 \n",
            " lstm_27 (LSTM)              (None, 100, 16)           2112      \n",
            "                                                                 \n",
            " flatten_16 (Flatten)        (None, 1600)              0         \n",
            "                                                                 \n",
            " dense_34 (Dense)            (None, 12)                19212     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24,780\n",
            "Trainable params: 24,780\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/30\n",
            "206/206 [==============================] - 6s 17ms/step - loss: 1.4992 - sparse_categorical_accuracy: 0.5020 - val_loss: 1.2027 - val_sparse_categorical_accuracy: 0.5977\n",
            "Epoch 2/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 1.0291 - sparse_categorical_accuracy: 0.6687 - val_loss: 0.8551 - val_sparse_categorical_accuracy: 0.7250\n",
            "Epoch 3/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.7987 - sparse_categorical_accuracy: 0.7577 - val_loss: 0.6738 - val_sparse_categorical_accuracy: 0.8093\n",
            "Epoch 4/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.8010 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.8206\n",
            "Epoch 5/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.6088 - sparse_categorical_accuracy: 0.8234 - val_loss: 0.5623 - val_sparse_categorical_accuracy: 0.8361\n",
            "Epoch 6/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5637 - sparse_categorical_accuracy: 0.8381 - val_loss: 0.5036 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 7/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.5199 - sparse_categorical_accuracy: 0.8492 - val_loss: 0.5059 - val_sparse_categorical_accuracy: 0.8563\n",
            "Epoch 8/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4953 - sparse_categorical_accuracy: 0.8556 - val_loss: 0.5016 - val_sparse_categorical_accuracy: 0.8575\n",
            "Epoch 9/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4710 - sparse_categorical_accuracy: 0.8631 - val_loss: 0.4836 - val_sparse_categorical_accuracy: 0.8579\n",
            "Epoch 10/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4469 - sparse_categorical_accuracy: 0.8674 - val_loss: 0.4403 - val_sparse_categorical_accuracy: 0.8715\n",
            "Epoch 11/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4285 - sparse_categorical_accuracy: 0.8730 - val_loss: 0.4510 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 12/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.4100 - sparse_categorical_accuracy: 0.8803 - val_loss: 0.4412 - val_sparse_categorical_accuracy: 0.8750\n",
            "Epoch 13/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3899 - sparse_categorical_accuracy: 0.8863 - val_loss: 0.4041 - val_sparse_categorical_accuracy: 0.8827\n",
            "Epoch 14/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3822 - sparse_categorical_accuracy: 0.8870 - val_loss: 0.4064 - val_sparse_categorical_accuracy: 0.8870\n",
            "Epoch 15/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3680 - sparse_categorical_accuracy: 0.8876 - val_loss: 0.4004 - val_sparse_categorical_accuracy: 0.8913\n",
            "Epoch 16/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3599 - sparse_categorical_accuracy: 0.8932 - val_loss: 0.3878 - val_sparse_categorical_accuracy: 0.8928\n",
            "Epoch 17/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3472 - sparse_categorical_accuracy: 0.8965 - val_loss: 0.3639 - val_sparse_categorical_accuracy: 0.9041\n",
            "Epoch 18/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3372 - sparse_categorical_accuracy: 0.9004 - val_loss: 0.3745 - val_sparse_categorical_accuracy: 0.8951\n",
            "Epoch 19/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3240 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3725 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 20/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3208 - sparse_categorical_accuracy: 0.9030 - val_loss: 0.3791 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 21/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3132 - sparse_categorical_accuracy: 0.9035 - val_loss: 0.3725 - val_sparse_categorical_accuracy: 0.8975\n",
            "Epoch 22/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3144 - sparse_categorical_accuracy: 0.9031 - val_loss: 0.3804 - val_sparse_categorical_accuracy: 0.8940\n",
            "Epoch 23/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3045 - sparse_categorical_accuracy: 0.9069 - val_loss: 0.3512 - val_sparse_categorical_accuracy: 0.9006\n",
            "Epoch 24/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.3005 - sparse_categorical_accuracy: 0.9112 - val_loss: 0.3353 - val_sparse_categorical_accuracy: 0.9072\n",
            "Epoch 25/30\n",
            "206/206 [==============================] - 3s 15ms/step - loss: 0.2976 - sparse_categorical_accuracy: 0.9084 - val_loss: 0.3453 - val_sparse_categorical_accuracy: 0.9049\n",
            "Epoch 26/30\n",
            "206/206 [==============================] - 3s 16ms/step - loss: 0.2898 - sparse_categorical_accuracy: 0.9106 - val_loss: 0.3731 - val_sparse_categorical_accuracy: 0.8924\n",
            "Epoch 27/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2814 - sparse_categorical_accuracy: 0.9151 - val_loss: 0.3535 - val_sparse_categorical_accuracy: 0.9091\n",
            "Epoch 28/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2758 - sparse_categorical_accuracy: 0.9142 - val_loss: 0.3432 - val_sparse_categorical_accuracy: 0.9002\n",
            "Epoch 29/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2713 - sparse_categorical_accuracy: 0.9141 - val_loss: 0.3667 - val_sparse_categorical_accuracy: 0.8959\n",
            "Epoch 30/30\n",
            "206/206 [==============================] - 3s 13ms/step - loss: 0.2656 - sparse_categorical_accuracy: 0.9168 - val_loss: 0.3438 - val_sparse_categorical_accuracy: 0.9021\n",
            "199/199 [==============================] - 1s 6ms/step - loss: 0.3340 - sparse_categorical_accuracy: 0.9079\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33398544788360596, 0.9079009890556335]"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Summary:\n",
        "\n",
        "![image.png](data:image/png;base64,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)"
      ],
      "metadata": {
        "id": "ld1WcEvoxPA0"
      }
    }
  ]
}
